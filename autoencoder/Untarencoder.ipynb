{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c348f69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Conv2D,Conv2DTranspose,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766d9d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgpath = 'D:\\\\abnormal\\\\category\\\\data\\\\normal'\n",
    "imgfiles = os.listdir(imgpath)\n",
    "x_train = np.empty((326,512,512,3))\n",
    "i = 0\n",
    "\n",
    "for file in imgfiles:\n",
    "    img = cv2.imread(imgpath+'/'+file)[:,:,::-1]\n",
    "    img = cv2.resize(img,(512,512))\n",
    "    x_train[i] = img\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf129c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 43.,  43.,  43.],\n",
       "         [ 26.,  26.,  26.],\n",
       "         [ 13.,  13.,  13.],\n",
       "         ...,\n",
       "         [ 32.,  32.,  32.],\n",
       "         [ 32.,  32.,  32.],\n",
       "         [ 35.,  35.,  35.]],\n",
       "\n",
       "        [[ 41.,  41.,  41.],\n",
       "         [ 26.,  26.,  26.],\n",
       "         [ 18.,  18.,  18.],\n",
       "         ...,\n",
       "         [ 31.,  31.,  31.],\n",
       "         [ 33.,  33.,  33.],\n",
       "         [ 31.,  31.,  31.]],\n",
       "\n",
       "        [[ 39.,  39.,  39.],\n",
       "         [ 28.,  28.,  28.],\n",
       "         [ 14.,  14.,  14.],\n",
       "         ...,\n",
       "         [ 29.,  29.,  29.],\n",
       "         [ 33.,  33.,  33.],\n",
       "         [ 31.,  31.,  31.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [ 70.,  70.,  70.],\n",
       "         [ 72.,  72.,  72.],\n",
       "         [ 71.,  71.,  71.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [ 92.,  92.,  92.],\n",
       "         [ 85.,  85.,  85.],\n",
       "         [ 90.,  90.,  90.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[ 63.,  63.,  63.],\n",
       "         [ 46.,  46.,  46.],\n",
       "         [ 32.,  32.,  32.],\n",
       "         ...,\n",
       "         [ 61.,  61.,  61.],\n",
       "         [ 63.,  63.,  63.],\n",
       "         [ 69.,  69.,  69.]],\n",
       "\n",
       "        [[ 61.,  61.,  61.],\n",
       "         [ 46.,  46.,  46.],\n",
       "         [ 33.,  33.,  33.],\n",
       "         ...,\n",
       "         [ 61.,  61.,  61.],\n",
       "         [ 67.,  67.,  67.],\n",
       "         [ 66.,  66.,  66.]],\n",
       "\n",
       "        [[ 58.,  58.,  58.],\n",
       "         [ 45.,  45.,  45.],\n",
       "         [ 30.,  30.,  30.],\n",
       "         ...,\n",
       "         [ 62.,  62.,  62.],\n",
       "         [ 64.,  64.,  64.],\n",
       "         [ 63.,  63.,  63.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [107., 107., 107.],\n",
       "         [111., 111., 111.],\n",
       "         [124., 124., 124.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [129., 129., 129.],\n",
       "         [129., 129., 129.],\n",
       "         [138., 138., 138.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[101., 101., 101.],\n",
       "         [ 87.,  87.,  87.],\n",
       "         [ 80.,  80.,  80.],\n",
       "         ...,\n",
       "         [109., 109., 109.],\n",
       "         [ 29.,  29.,  29.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[102., 102., 102.],\n",
       "         [ 87.,  87.,  87.],\n",
       "         [ 80.,  80.,  80.],\n",
       "         ...,\n",
       "         [112., 112., 112.],\n",
       "         [ 28.,  28.,  28.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[ 96.,  96.,  96.],\n",
       "         [ 86.,  86.,  86.],\n",
       "         [ 73.,  73.,  73.],\n",
       "         ...,\n",
       "         [111., 111., 111.],\n",
       "         [ 28.,  28.,  28.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [213., 213., 213.],\n",
       "         [218., 218., 218.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [228., 228., 228.],\n",
       "         [228., 228., 228.],\n",
       "         [  0.,   0.,   0.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[125., 125., 125.],\n",
       "         [103., 103., 103.],\n",
       "         [ 82.,  82.,  82.],\n",
       "         ...,\n",
       "         [ 92.,  92.,  92.],\n",
       "         [ 98.,  98.,  98.],\n",
       "         [102., 102., 102.]],\n",
       "\n",
       "        [[125., 125., 125.],\n",
       "         [101., 101., 101.],\n",
       "         [ 79.,  79.,  79.],\n",
       "         ...,\n",
       "         [ 93.,  93.,  93.],\n",
       "         [ 99.,  99.,  99.],\n",
       "         [105., 105., 105.]],\n",
       "\n",
       "        [[120., 120., 120.],\n",
       "         [102., 102., 102.],\n",
       "         [ 84.,  84.,  84.],\n",
       "         ...,\n",
       "         [ 95.,  95.,  95.],\n",
       "         [ 94.,  94.,  94.],\n",
       "         [ 93.,  93.,  93.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [154., 154., 154.],\n",
       "         [152., 152., 152.],\n",
       "         [154., 154., 154.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [174., 174., 174.],\n",
       "         [172., 172., 172.],\n",
       "         [171., 171., 171.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[ 91.,  91.,  91.],\n",
       "         [ 76.,  76.,  76.],\n",
       "         [ 57.,  57.,  57.],\n",
       "         ...,\n",
       "         [ 75.,  75.,  75.],\n",
       "         [ 77.,  77.,  77.],\n",
       "         [ 82.,  82.,  82.]],\n",
       "\n",
       "        [[ 89.,  89.,  89.],\n",
       "         [ 73.,  73.,  73.],\n",
       "         [ 61.,  61.,  61.],\n",
       "         ...,\n",
       "         [ 76.,  76.,  76.],\n",
       "         [ 76.,  76.,  76.],\n",
       "         [ 78.,  78.,  78.]],\n",
       "\n",
       "        [[ 92.,  92.,  92.],\n",
       "         [ 71.,  71.,  71.],\n",
       "         [ 54.,  54.,  54.],\n",
       "         ...,\n",
       "         [ 73.,  73.,  73.],\n",
       "         [ 81.,  81.,  81.],\n",
       "         [ 77.,  77.,  77.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [118., 118., 118.],\n",
       "         [120., 120., 120.],\n",
       "         [120., 120., 120.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [139., 139., 139.],\n",
       "         [142., 142., 142.],\n",
       "         [140., 140., 140.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]],\n",
       "\n",
       "\n",
       "       [[[ 53.,  53.,  53.],\n",
       "         [ 38.,  38.,  38.],\n",
       "         [ 22.,  22.,  22.],\n",
       "         ...,\n",
       "         [ 53.,  53.,  53.],\n",
       "         [ 49.,  49.,  49.],\n",
       "         [ 56.,  56.,  56.]],\n",
       "\n",
       "        [[ 51.,  51.,  51.],\n",
       "         [ 40.,  40.,  40.],\n",
       "         [ 24.,  24.,  24.],\n",
       "         ...,\n",
       "         [ 48.,  48.,  48.],\n",
       "         [ 54.,  54.,  54.],\n",
       "         [ 54.,  54.,  54.]],\n",
       "\n",
       "        [[ 52.,  52.,  52.],\n",
       "         [ 39.,  39.,  39.],\n",
       "         [ 23.,  23.,  23.],\n",
       "         ...,\n",
       "         [ 52.,  52.,  52.],\n",
       "         [ 50.,  50.,  50.],\n",
       "         [ 50.,  50.,  50.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [ 79.,  79.,  79.],\n",
       "         [ 86.,  86.,  86.],\n",
       "         [ 86.,  86.,  86.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [105., 105., 105.],\n",
       "         [109., 109., 109.],\n",
       "         [108., 108., 108.]],\n",
       "\n",
       "        [[  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.]]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9c2edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326, 512, 512, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37af2c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "791ac35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "x_train /= 255\n",
    "x_valid = x_train[300:]\n",
    "x_train = x_train[:300]\n",
    "x_valid = x_valid.astype('float32')\n",
    "x_train = x_train.astype('float32')\n",
    "print('{}\\n{}'.format(len(x_train),len(x_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea7d115d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.16862746, 0.16862746, 0.16862746],\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.05098039, 0.05098039, 0.05098039],\n",
       "         ...,\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         [0.13725491, 0.13725491, 0.13725491]],\n",
       "\n",
       "        [[0.16078432, 0.16078432, 0.16078432],\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.07058824, 0.07058824, 0.07058824],\n",
       "         ...,\n",
       "         [0.12156863, 0.12156863, 0.12156863],\n",
       "         [0.12941177, 0.12941177, 0.12941177],\n",
       "         [0.12156863, 0.12156863, 0.12156863]],\n",
       "\n",
       "        [[0.15294118, 0.15294118, 0.15294118],\n",
       "         [0.10980392, 0.10980392, 0.10980392],\n",
       "         [0.05490196, 0.05490196, 0.05490196],\n",
       "         ...,\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         [0.12941177, 0.12941177, 0.12941177],\n",
       "         [0.12156863, 0.12156863, 0.12156863]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.27450982, 0.27450982, 0.27450982],\n",
       "         [0.28235295, 0.28235295, 0.28235295],\n",
       "         [0.2784314 , 0.2784314 , 0.2784314 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.36078432, 0.36078432, 0.36078432],\n",
       "         [0.33333334, 0.33333334, 0.33333334],\n",
       "         [0.3529412 , 0.3529412 , 0.3529412 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.24705882, 0.24705882, 0.24705882],\n",
       "         [0.18039216, 0.18039216, 0.18039216],\n",
       "         [0.1254902 , 0.1254902 , 0.1254902 ],\n",
       "         ...,\n",
       "         [0.23921569, 0.23921569, 0.23921569],\n",
       "         [0.24705882, 0.24705882, 0.24705882],\n",
       "         [0.27058825, 0.27058825, 0.27058825]],\n",
       "\n",
       "        [[0.23921569, 0.23921569, 0.23921569],\n",
       "         [0.18039216, 0.18039216, 0.18039216],\n",
       "         [0.12941177, 0.12941177, 0.12941177],\n",
       "         ...,\n",
       "         [0.23921569, 0.23921569, 0.23921569],\n",
       "         [0.2627451 , 0.2627451 , 0.2627451 ],\n",
       "         [0.25882354, 0.25882354, 0.25882354]],\n",
       "\n",
       "        [[0.22745098, 0.22745098, 0.22745098],\n",
       "         [0.1764706 , 0.1764706 , 0.1764706 ],\n",
       "         [0.11764706, 0.11764706, 0.11764706],\n",
       "         ...,\n",
       "         [0.24313726, 0.24313726, 0.24313726],\n",
       "         [0.2509804 , 0.2509804 , 0.2509804 ],\n",
       "         [0.24705882, 0.24705882, 0.24705882]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.41960785, 0.41960785, 0.41960785],\n",
       "         [0.43529412, 0.43529412, 0.43529412],\n",
       "         [0.4862745 , 0.4862745 , 0.4862745 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.5058824 , 0.5058824 , 0.5058824 ],\n",
       "         [0.5058824 , 0.5058824 , 0.5058824 ],\n",
       "         [0.5411765 , 0.5411765 , 0.5411765 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.39607844, 0.39607844, 0.39607844],\n",
       "         [0.34117648, 0.34117648, 0.34117648],\n",
       "         [0.3137255 , 0.3137255 , 0.3137255 ],\n",
       "         ...,\n",
       "         [0.42745098, 0.42745098, 0.42745098],\n",
       "         [0.11372549, 0.11372549, 0.11372549],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.4       , 0.4       , 0.4       ],\n",
       "         [0.34117648, 0.34117648, 0.34117648],\n",
       "         [0.3137255 , 0.3137255 , 0.3137255 ],\n",
       "         ...,\n",
       "         [0.4392157 , 0.4392157 , 0.4392157 ],\n",
       "         [0.10980392, 0.10980392, 0.10980392],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.3764706 , 0.3764706 , 0.3764706 ],\n",
       "         [0.3372549 , 0.3372549 , 0.3372549 ],\n",
       "         [0.28627452, 0.28627452, 0.28627452],\n",
       "         ...,\n",
       "         [0.43529412, 0.43529412, 0.43529412],\n",
       "         [0.10980392, 0.10980392, 0.10980392],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.8352941 , 0.8352941 , 0.8352941 ],\n",
       "         [0.85490197, 0.85490197, 0.85490197],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.89411765, 0.89411765, 0.89411765],\n",
       "         [0.89411765, 0.89411765, 0.89411765],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.14901961, 0.14901961, 0.14901961],\n",
       "         [0.09411765, 0.09411765, 0.09411765],\n",
       "         [0.04313726, 0.04313726, 0.04313726],\n",
       "         ...,\n",
       "         [0.15686275, 0.15686275, 0.15686275],\n",
       "         [0.16862746, 0.16862746, 0.16862746],\n",
       "         [0.1764706 , 0.1764706 , 0.1764706 ]],\n",
       "\n",
       "        [[0.14901961, 0.14901961, 0.14901961],\n",
       "         [0.09019608, 0.09019608, 0.09019608],\n",
       "         [0.04705882, 0.04705882, 0.04705882],\n",
       "         ...,\n",
       "         [0.16078432, 0.16078432, 0.16078432],\n",
       "         [0.16862746, 0.16862746, 0.16862746],\n",
       "         [0.16862746, 0.16862746, 0.16862746]],\n",
       "\n",
       "        [[0.14901961, 0.14901961, 0.14901961],\n",
       "         [0.10196079, 0.10196079, 0.10196079],\n",
       "         [0.05490196, 0.05490196, 0.05490196],\n",
       "         ...,\n",
       "         [0.16862746, 0.16862746, 0.16862746],\n",
       "         [0.16470589, 0.16470589, 0.16470589],\n",
       "         [0.16470589, 0.16470589, 0.16470589]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.38039216, 0.38039216, 0.38039216],\n",
       "         [0.3882353 , 0.3882353 , 0.3882353 ],\n",
       "         [0.39215687, 0.39215687, 0.39215687]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.45882353, 0.45882353, 0.45882353],\n",
       "         [0.48235294, 0.48235294, 0.48235294],\n",
       "         [0.4745098 , 0.4745098 , 0.4745098 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.2901961 , 0.2901961 , 0.2901961 ],\n",
       "         [0.2901961 , 0.2901961 , 0.2901961 ],\n",
       "         [0.28627452, 0.28627452, 0.28627452],\n",
       "         ...,\n",
       "         [0.27058825, 0.27058825, 0.27058825],\n",
       "         [0.25882354, 0.25882354, 0.25882354],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.3019608 , 0.3019608 , 0.3019608 ],\n",
       "         [0.29411766, 0.29411766, 0.29411766],\n",
       "         [0.2901961 , 0.2901961 , 0.2901961 ],\n",
       "         ...,\n",
       "         [0.2784314 , 0.2784314 , 0.2784314 ],\n",
       "         [0.2627451 , 0.2627451 , 0.2627451 ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.27058825, 0.27058825, 0.27058825],\n",
       "         [0.28235295, 0.28235295, 0.28235295],\n",
       "         [0.28235295, 0.28235295, 0.28235295],\n",
       "         ...,\n",
       "         [0.27450982, 0.27450982, 0.27450982],\n",
       "         [0.26666668, 0.26666668, 0.26666668],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.5529412 , 0.5529412 , 0.5529412 ],\n",
       "         [0.45490196, 0.45490196, 0.45490196],\n",
       "         [0.39607844, 0.39607844, 0.39607844],\n",
       "         ...,\n",
       "         [0.45490196, 0.45490196, 0.45490196],\n",
       "         [0.4392157 , 0.4392157 , 0.4392157 ],\n",
       "         [0.47843137, 0.47843137, 0.47843137]],\n",
       "\n",
       "        [[0.54901963, 0.54901963, 0.54901963],\n",
       "         [0.45882353, 0.45882353, 0.45882353],\n",
       "         [0.38039216, 0.38039216, 0.38039216],\n",
       "         ...,\n",
       "         [0.4392157 , 0.4392157 , 0.4392157 ],\n",
       "         [0.45882353, 0.45882353, 0.45882353],\n",
       "         [0.45882353, 0.45882353, 0.45882353]],\n",
       "\n",
       "        [[0.5411765 , 0.5411765 , 0.5411765 ],\n",
       "         [0.46666667, 0.46666667, 0.46666667],\n",
       "         [0.38431373, 0.38431373, 0.38431373],\n",
       "         ...,\n",
       "         [0.43529412, 0.43529412, 0.43529412],\n",
       "         [0.45882353, 0.45882353, 0.45882353],\n",
       "         [0.4392157 , 0.4392157 , 0.4392157 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.6666667 , 0.6666667 , 0.6666667 ],\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ],\n",
       "         [0.6666667 , 0.6666667 , 0.6666667 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.7294118 , 0.7294118 , 0.7294118 ],\n",
       "         [0.7137255 , 0.7137255 , 0.7137255 ],\n",
       "         [0.7294118 , 0.7294118 , 0.7294118 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79f4b005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.47843137, 0.47843137, 0.47843137],\n",
       "         [0.4       , 0.4       , 0.4       ],\n",
       "         [0.32941177, 0.32941177, 0.32941177],\n",
       "         ...,\n",
       "         [0.4       , 0.4       , 0.4       ],\n",
       "         [0.4       , 0.4       , 0.4       ],\n",
       "         [0.43529412, 0.43529412, 0.43529412]],\n",
       "\n",
       "        [[0.46666667, 0.46666667, 0.46666667],\n",
       "         [0.4       , 0.4       , 0.4       ],\n",
       "         [0.3372549 , 0.3372549 , 0.3372549 ],\n",
       "         ...,\n",
       "         [0.41568628, 0.41568628, 0.41568628],\n",
       "         [0.41568628, 0.41568628, 0.41568628],\n",
       "         [0.41960785, 0.41960785, 0.41960785]],\n",
       "\n",
       "        [[0.47058824, 0.47058824, 0.47058824],\n",
       "         [0.4       , 0.4       , 0.4       ],\n",
       "         [0.32156864, 0.32156864, 0.32156864],\n",
       "         ...,\n",
       "         [0.4       , 0.4       , 0.4       ],\n",
       "         [0.41568628, 0.41568628, 0.41568628],\n",
       "         [0.4       , 0.4       , 0.4       ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.81960785, 0.81960785, 0.81960785],\n",
       "         [0.81960785, 0.81960785, 0.81960785],\n",
       "         [0.81960785, 0.81960785, 0.81960785]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.85882354, 0.85882354, 0.85882354],\n",
       "         [0.84705883, 0.84705883, 0.84705883],\n",
       "         [0.8392157 , 0.8392157 , 0.8392157 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.24313726, 0.24313726, 0.24313726],\n",
       "         [0.25882354, 0.25882354, 0.25882354],\n",
       "         [0.24313726, 0.24313726, 0.24313726],\n",
       "         ...,\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314]],\n",
       "\n",
       "        [[0.23137255, 0.23137255, 0.23137255],\n",
       "         [0.22745098, 0.22745098, 0.22745098],\n",
       "         [0.2509804 , 0.2509804 , 0.2509804 ],\n",
       "         ...,\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314]],\n",
       "\n",
       "        [[0.21568628, 0.21568628, 0.21568628],\n",
       "         [0.21568628, 0.21568628, 0.21568628],\n",
       "         [0.22745098, 0.22745098, 0.22745098],\n",
       "         ...,\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         ...,\n",
       "         [0.9137255 , 0.9137255 , 0.9137255 ],\n",
       "         [0.9019608 , 0.9019608 , 0.9019608 ],\n",
       "         [0.00784314, 0.00784314, 0.00784314]],\n",
       "\n",
       "        [[0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         ...,\n",
       "         [0.92156863, 0.92156863, 0.92156863],\n",
       "         [0.9254902 , 0.9254902 , 0.9254902 ],\n",
       "         [0.00784314, 0.00784314, 0.00784314]],\n",
       "\n",
       "        [[0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         ...,\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314],\n",
       "         [0.00784314, 0.00784314, 0.00784314]]],\n",
       "\n",
       "\n",
       "       [[[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.89411765, 0.89411765, 0.89411765],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.91764706, 0.91764706, 0.91764706],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.49019608, 0.49019608, 0.49019608],\n",
       "         [0.40392157, 0.40392157, 0.40392157],\n",
       "         [0.32156864, 0.32156864, 0.32156864],\n",
       "         ...,\n",
       "         [0.36078432, 0.36078432, 0.36078432],\n",
       "         [0.38431373, 0.38431373, 0.38431373],\n",
       "         [0.4       , 0.4       , 0.4       ]],\n",
       "\n",
       "        [[0.49019608, 0.49019608, 0.49019608],\n",
       "         [0.39607844, 0.39607844, 0.39607844],\n",
       "         [0.30980393, 0.30980393, 0.30980393],\n",
       "         ...,\n",
       "         [0.3647059 , 0.3647059 , 0.3647059 ],\n",
       "         [0.3882353 , 0.3882353 , 0.3882353 ],\n",
       "         [0.4117647 , 0.4117647 , 0.4117647 ]],\n",
       "\n",
       "        [[0.47058824, 0.47058824, 0.47058824],\n",
       "         [0.4       , 0.4       , 0.4       ],\n",
       "         [0.32941177, 0.32941177, 0.32941177],\n",
       "         ...,\n",
       "         [0.37254903, 0.37254903, 0.37254903],\n",
       "         [0.36862746, 0.36862746, 0.36862746],\n",
       "         [0.3647059 , 0.3647059 , 0.3647059 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.6039216 , 0.6039216 , 0.6039216 ],\n",
       "         [0.59607846, 0.59607846, 0.59607846],\n",
       "         [0.6039216 , 0.6039216 , 0.6039216 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.68235296, 0.68235296, 0.68235296],\n",
       "         [0.6745098 , 0.6745098 , 0.6745098 ],\n",
       "         [0.67058825, 0.67058825, 0.67058825]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.35686275, 0.35686275, 0.35686275],\n",
       "         [0.29803923, 0.29803923, 0.29803923],\n",
       "         [0.22352941, 0.22352941, 0.22352941],\n",
       "         ...,\n",
       "         [0.29411766, 0.29411766, 0.29411766],\n",
       "         [0.3019608 , 0.3019608 , 0.3019608 ],\n",
       "         [0.32156864, 0.32156864, 0.32156864]],\n",
       "\n",
       "        [[0.34901962, 0.34901962, 0.34901962],\n",
       "         [0.28627452, 0.28627452, 0.28627452],\n",
       "         [0.23921569, 0.23921569, 0.23921569],\n",
       "         ...,\n",
       "         [0.29803923, 0.29803923, 0.29803923],\n",
       "         [0.29803923, 0.29803923, 0.29803923],\n",
       "         [0.30588236, 0.30588236, 0.30588236]],\n",
       "\n",
       "        [[0.36078432, 0.36078432, 0.36078432],\n",
       "         [0.2784314 , 0.2784314 , 0.2784314 ],\n",
       "         [0.21176471, 0.21176471, 0.21176471],\n",
       "         ...,\n",
       "         [0.28627452, 0.28627452, 0.28627452],\n",
       "         [0.31764707, 0.31764707, 0.31764707],\n",
       "         [0.3019608 , 0.3019608 , 0.3019608 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.4627451 , 0.4627451 , 0.4627451 ],\n",
       "         [0.47058824, 0.47058824, 0.47058824],\n",
       "         [0.47058824, 0.47058824, 0.47058824]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.54509807, 0.54509807, 0.54509807],\n",
       "         [0.5568628 , 0.5568628 , 0.5568628 ],\n",
       "         [0.54901963, 0.54901963, 0.54901963]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]],\n",
       "\n",
       "\n",
       "       [[[0.20784314, 0.20784314, 0.20784314],\n",
       "         [0.14901961, 0.14901961, 0.14901961],\n",
       "         [0.08627451, 0.08627451, 0.08627451],\n",
       "         ...,\n",
       "         [0.20784314, 0.20784314, 0.20784314],\n",
       "         [0.19215687, 0.19215687, 0.19215687],\n",
       "         [0.21960784, 0.21960784, 0.21960784]],\n",
       "\n",
       "        [[0.2       , 0.2       , 0.2       ],\n",
       "         [0.15686275, 0.15686275, 0.15686275],\n",
       "         [0.09411765, 0.09411765, 0.09411765],\n",
       "         ...,\n",
       "         [0.1882353 , 0.1882353 , 0.1882353 ],\n",
       "         [0.21176471, 0.21176471, 0.21176471],\n",
       "         [0.21176471, 0.21176471, 0.21176471]],\n",
       "\n",
       "        [[0.20392157, 0.20392157, 0.20392157],\n",
       "         [0.15294118, 0.15294118, 0.15294118],\n",
       "         [0.09019608, 0.09019608, 0.09019608],\n",
       "         ...,\n",
       "         [0.20392157, 0.20392157, 0.20392157],\n",
       "         [0.19607843, 0.19607843, 0.19607843],\n",
       "         [0.19607843, 0.19607843, 0.19607843]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.30980393, 0.30980393, 0.30980393],\n",
       "         [0.3372549 , 0.3372549 , 0.3372549 ],\n",
       "         [0.3372549 , 0.3372549 , 0.3372549 ]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.4117647 , 0.4117647 , 0.4117647 ],\n",
       "         [0.42745098, 0.42745098, 0.42745098],\n",
       "         [0.42352942, 0.42352942, 0.42352942]],\n",
       "\n",
       "        [[0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         ...,\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ],\n",
       "         [0.        , 0.        , 0.        ]]]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c893ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import layers\n",
    "\n",
    "# class AnomalyDetector(Model):\n",
    "#   def __init__(self):\n",
    "#     super(AnomalyDetector, self).__init__()\n",
    "#     self.encoder = tf.keras.Sequential([\n",
    "#         layers.Input(shape=(512, 512, 3)),\n",
    "#         layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2),\n",
    "#         layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "#         layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
    "#         layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "#         layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
    "\n",
    "#     self.decoder = tf.keras.Sequential([\n",
    "#         layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "#         layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "#         layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "#         layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "#         layers.Conv2DTranspose(128, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "#         layers.Conv2D(3, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
    "\n",
    "#   def call(self, x):\n",
    "#       encoded = self.encoder(x)\n",
    "#       decoded = self.decoder(encoded)\n",
    "#       return decoded\n",
    "\n",
    "# autoencoder = AnomalyDetector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c201ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "class TarAutoEncoder(Model):\n",
    "    def __init__(self):\n",
    "        super(TarAutoEncoder, self).__init__()\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            layers.Input(shape=(2048, 2048, 3)),\n",
    "            layers.Conv2D(512, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(256, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),\n",
    "            layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "            layers.MaxPooling2D((2, 2), padding='same'),  \n",
    "            layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2),\n",
    "#             layers.MaxPooling2D((2, 2), padding='same'),  \n",
    "#             layers.Conv2D(4, (3, 3), activation='relu', padding='same', strides=2),\n",
    "#             layers.MaxPooling2D((2, 2), padding='same'),  \n",
    "#             layers.Conv2D(2, (3, 3), activation='relu', padding='same', strides=2)\n",
    "        ])\n",
    "        \n",
    "        self.decoder = tf.keras.Sequential([\n",
    "#             layers.Conv2DTranspose(2, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "#             layers.UpSampling2D((2, 2)),\n",
    "#             layers.Conv2DTranspose(4, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "#             layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.UpSampling2D((2, 2)),\n",
    "            layers.Conv2DTranspose(128, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "            layers.Conv2D(3, kernel_size=(3, 3), activation='sigmoid', padding='same')\n",
    "        ])\n",
    "\n",
    "        \n",
    "    def call(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "autoencoder = TarAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8389f74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 32)        18464     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 16)          4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1, 1, 8)           1160      \n",
      "=================================================================\n",
      "Total params: 101,624\n",
      "Trainable params: 101,624\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64aad094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "10/10 [==============================] - 12s 629ms/step - loss: 0.2485 - val_loss: 0.2285\n",
      "Epoch 2/200\n",
      "10/10 [==============================] - 4s 379ms/step - loss: 0.2119 - val_loss: 0.1923\n",
      "Epoch 3/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1950 - val_loss: 0.1851\n",
      "Epoch 4/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1867 - val_loss: 0.1765\n",
      "Epoch 5/200\n",
      "10/10 [==============================] - 4s 368ms/step - loss: 0.1773 - val_loss: 0.1662\n",
      "Epoch 6/200\n",
      "10/10 [==============================] - 4s 369ms/step - loss: 0.1692 - val_loss: 0.1584\n",
      "Epoch 7/200\n",
      "10/10 [==============================] - 4s 368ms/step - loss: 0.1633 - val_loss: 0.1525\n",
      "Epoch 8/200\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.1578 - val_loss: 0.1473\n",
      "Epoch 9/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1544 - val_loss: 0.1473\n",
      "Epoch 10/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1515 - val_loss: 0.1433\n",
      "Epoch 11/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1488 - val_loss: 0.1401\n",
      "Epoch 12/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1465 - val_loss: 0.1376\n",
      "Epoch 13/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1440 - val_loss: 0.1343\n",
      "Epoch 14/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1407 - val_loss: 0.1339\n",
      "Epoch 15/200\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 0.1379 - val_loss: 0.1277\n",
      "Epoch 16/200\n",
      "10/10 [==============================] - 4s 371ms/step - loss: 0.1355 - val_loss: 0.1271\n",
      "Epoch 17/200\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.1328 - val_loss: 0.1245\n",
      "Epoch 18/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1310 - val_loss: 0.1253\n",
      "Epoch 19/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1298 - val_loss: 0.1238\n",
      "Epoch 20/200\n",
      "10/10 [==============================] - 4s 372ms/step - loss: 0.1289 - val_loss: 0.1234\n",
      "Epoch 21/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1283 - val_loss: 0.1228\n",
      "Epoch 22/200\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.1280 - val_loss: 0.1224\n",
      "Epoch 23/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1271 - val_loss: 0.1220\n",
      "Epoch 24/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1268 - val_loss: 0.1229\n",
      "Epoch 25/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1262 - val_loss: 0.1213\n",
      "Epoch 26/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1248 - val_loss: 0.1206\n",
      "Epoch 27/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1238 - val_loss: 0.1197\n",
      "Epoch 28/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1232 - val_loss: 0.1190\n",
      "Epoch 29/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1218 - val_loss: 0.1172\n",
      "Epoch 30/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1201 - val_loss: 0.1161\n",
      "Epoch 31/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1189 - val_loss: 0.1149\n",
      "Epoch 32/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1178 - val_loss: 0.1134\n",
      "Epoch 33/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1168 - val_loss: 0.1149\n",
      "Epoch 34/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1171 - val_loss: 0.1126\n",
      "Epoch 35/200\n",
      "10/10 [==============================] - 4s 383ms/step - loss: 0.1158 - val_loss: 0.1120\n",
      "Epoch 36/200\n",
      "10/10 [==============================] - 4s 379ms/step - loss: 0.1156 - val_loss: 0.1136\n",
      "Epoch 37/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1153 - val_loss: 0.1110\n",
      "Epoch 38/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1140 - val_loss: 0.1103\n",
      "Epoch 39/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1138 - val_loss: 0.1109\n",
      "Epoch 40/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1137 - val_loss: 0.1104\n",
      "Epoch 41/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1132 - val_loss: 0.1096\n",
      "Epoch 42/200\n",
      "10/10 [==============================] - 4s 380ms/step - loss: 0.1128 - val_loss: 0.1090\n",
      "Epoch 43/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1124 - val_loss: 0.1089\n",
      "Epoch 44/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1134 - val_loss: 0.1140\n",
      "Epoch 45/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1144 - val_loss: 0.1090\n",
      "Epoch 46/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1124 - val_loss: 0.1089\n",
      "Epoch 47/200\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.1116 - val_loss: 0.1083\n",
      "Epoch 48/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1115 - val_loss: 0.1084\n",
      "Epoch 49/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1119 - val_loss: 0.1088\n",
      "Epoch 50/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1119 - val_loss: 0.1079\n",
      "Epoch 51/200\n",
      "10/10 [==============================] - 4s 379ms/step - loss: 0.1115 - val_loss: 0.1084\n",
      "Epoch 52/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1111 - val_loss: 0.1078\n",
      "Epoch 53/200\n",
      "10/10 [==============================] - 4s 379ms/step - loss: 0.1110 - val_loss: 0.1075\n",
      "Epoch 54/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1110 - val_loss: 0.1073\n",
      "Epoch 55/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1104 - val_loss: 0.1072\n",
      "Epoch 56/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1105 - val_loss: 0.1072\n",
      "Epoch 57/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1105 - val_loss: 0.1070\n",
      "Epoch 58/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1107 - val_loss: 0.1080\n",
      "Epoch 59/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1107 - val_loss: 0.1072\n",
      "Epoch 60/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1101 - val_loss: 0.1070\n",
      "Epoch 61/200\n",
      "10/10 [==============================] - 4s 382ms/step - loss: 0.1099 - val_loss: 0.1068\n",
      "Epoch 62/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1097 - val_loss: 0.1074\n",
      "Epoch 63/200\n",
      "10/10 [==============================] - 4s 379ms/step - loss: 0.1101 - val_loss: 0.1069\n",
      "Epoch 64/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1101 - val_loss: 0.1070\n",
      "Epoch 65/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1102 - val_loss: 0.1065\n",
      "Epoch 66/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1098 - val_loss: 0.1064\n",
      "Epoch 67/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1094 - val_loss: 0.1063\n",
      "Epoch 68/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1103 - val_loss: 0.1061\n",
      "Epoch 69/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1101 - val_loss: 0.1069\n",
      "Epoch 70/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1096 - val_loss: 0.1077\n",
      "Epoch 71/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1096 - val_loss: 0.1081\n",
      "Epoch 72/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1096 - val_loss: 0.1073\n",
      "Epoch 73/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1092 - val_loss: 0.1064\n",
      "Epoch 74/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1092 - val_loss: 0.1067\n",
      "Epoch 75/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1091 - val_loss: 0.1073\n",
      "Epoch 76/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1091 - val_loss: 0.1065\n",
      "Epoch 77/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1090 - val_loss: 0.1062\n",
      "Epoch 78/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1087 - val_loss: 0.1060\n",
      "Epoch 79/200\n",
      "10/10 [==============================] - 4s 379ms/step - loss: 0.1087 - val_loss: 0.1065\n",
      "Epoch 80/200\n",
      "10/10 [==============================] - 4s 382ms/step - loss: 0.1087 - val_loss: 0.1059\n",
      "Epoch 81/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1091 - val_loss: 0.1062\n",
      "Epoch 82/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1089 - val_loss: 0.1062\n",
      "Epoch 83/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1084 - val_loss: 0.1052\n",
      "Epoch 84/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1083 - val_loss: 0.1059\n",
      "Epoch 85/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1087 - val_loss: 0.1066\n",
      "Epoch 86/200\n",
      "10/10 [==============================] - 4s 381ms/step - loss: 0.1085 - val_loss: 0.1056\n",
      "Epoch 87/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1086 - val_loss: 0.1060\n",
      "Epoch 88/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1082 - val_loss: 0.1051\n",
      "Epoch 89/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1079 - val_loss: 0.1051\n",
      "Epoch 90/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1082 - val_loss: 0.1058\n",
      "Epoch 91/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1082 - val_loss: 0.1057\n",
      "Epoch 92/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1086 - val_loss: 0.1056\n",
      "Epoch 93/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1085 - val_loss: 0.1055\n",
      "Epoch 94/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1080 - val_loss: 0.1049\n",
      "Epoch 95/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1080 - val_loss: 0.1069\n",
      "Epoch 96/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1084 - val_loss: 0.1060\n",
      "Epoch 97/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1087 - val_loss: 0.1054\n",
      "Epoch 98/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1086 - val_loss: 0.1051\n",
      "Epoch 99/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1079 - val_loss: 0.1050\n",
      "Epoch 100/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1075 - val_loss: 0.1049\n",
      "Epoch 101/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1076 - val_loss: 0.1051\n",
      "Epoch 102/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1079 - val_loss: 0.1047\n",
      "Epoch 103/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1075 - val_loss: 0.1049\n",
      "Epoch 104/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1074 - val_loss: 0.1048\n",
      "Epoch 105/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1074 - val_loss: 0.1050\n",
      "Epoch 106/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1084 - val_loss: 0.1057\n",
      "Epoch 107/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1079 - val_loss: 0.1055\n",
      "Epoch 108/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1075 - val_loss: 0.1053\n",
      "Epoch 109/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1077 - val_loss: 0.1045\n",
      "Epoch 110/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1074 - val_loss: 0.1044\n",
      "Epoch 111/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1071 - val_loss: 0.1047\n",
      "Epoch 112/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1071 - val_loss: 0.1044\n",
      "Epoch 113/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1072 - val_loss: 0.1043\n",
      "Epoch 114/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1068 - val_loss: 0.1042\n",
      "Epoch 115/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1068 - val_loss: 0.1045\n",
      "Epoch 116/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1068 - val_loss: 0.1050\n",
      "Epoch 117/200\n",
      "10/10 [==============================] - 4s 383ms/step - loss: 0.1072 - val_loss: 0.1059\n",
      "Epoch 118/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1071 - val_loss: 0.1043\n",
      "Epoch 119/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1073 - val_loss: 0.1041\n",
      "Epoch 120/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1068 - val_loss: 0.1054\n",
      "Epoch 121/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1069 - val_loss: 0.1054\n",
      "Epoch 122/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1071 - val_loss: 0.1042\n",
      "Epoch 123/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1070 - val_loss: 0.1051\n",
      "Epoch 124/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1070 - val_loss: 0.1047\n",
      "Epoch 125/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1067 - val_loss: 0.1041\n",
      "Epoch 126/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1072 - val_loss: 0.1064\n",
      "Epoch 127/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1079 - val_loss: 0.1042\n",
      "Epoch 128/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1068 - val_loss: 0.1041\n",
      "Epoch 129/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1071 - val_loss: 0.1041\n",
      "Epoch 130/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1065 - val_loss: 0.1039\n",
      "Epoch 131/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1065 - val_loss: 0.1037\n",
      "Epoch 132/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1064 - val_loss: 0.1037\n",
      "Epoch 133/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1063 - val_loss: 0.1045\n",
      "Epoch 134/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1066 - val_loss: 0.1042\n",
      "Epoch 135/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1069 - val_loss: 0.1052\n",
      "Epoch 136/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1066 - val_loss: 0.1037\n",
      "Epoch 137/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1063 - val_loss: 0.1051\n",
      "Epoch 138/200\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.1072 - val_loss: 0.1036\n",
      "Epoch 139/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1062 - val_loss: 0.1038\n",
      "Epoch 140/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1060 - val_loss: 0.1035\n",
      "Epoch 141/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1060 - val_loss: 0.1039\n",
      "Epoch 142/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1060 - val_loss: 0.1036\n",
      "Epoch 143/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1058 - val_loss: 0.1037\n",
      "Epoch 144/200\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.1057 - val_loss: 0.1038\n",
      "Epoch 145/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1063 - val_loss: 0.1061\n",
      "Epoch 146/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1069 - val_loss: 0.1044\n",
      "Epoch 147/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1062 - val_loss: 0.1043\n",
      "Epoch 148/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1061 - val_loss: 0.1034\n",
      "Epoch 149/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1057 - val_loss: 0.1035\n",
      "Epoch 150/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1057 - val_loss: 0.1041\n",
      "Epoch 151/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1061 - val_loss: 0.1034\n",
      "Epoch 152/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1057 - val_loss: 0.1034\n",
      "Epoch 153/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1059 - val_loss: 0.1033\n",
      "Epoch 154/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1054 - val_loss: 0.1030\n",
      "Epoch 155/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1053 - val_loss: 0.1031\n",
      "Epoch 156/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1056 - val_loss: 0.1034\n",
      "Epoch 157/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1055 - val_loss: 0.1029\n",
      "Epoch 158/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1053 - val_loss: 0.1036\n",
      "Epoch 159/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1053 - val_loss: 0.1029\n",
      "Epoch 160/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1051 - val_loss: 0.1031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1052 - val_loss: 0.1033\n",
      "Epoch 162/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1052 - val_loss: 0.1038\n",
      "Epoch 163/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1060 - val_loss: 0.1054\n",
      "Epoch 164/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1065 - val_loss: 0.1056\n",
      "Epoch 165/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1065 - val_loss: 0.1051\n",
      "Epoch 166/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1059 - val_loss: 0.1034\n",
      "Epoch 167/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1053 - val_loss: 0.1030\n",
      "Epoch 168/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1051 - val_loss: 0.1034\n",
      "Epoch 169/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1049 - val_loss: 0.1030\n",
      "Epoch 170/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1050 - val_loss: 0.1034\n",
      "Epoch 171/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1054 - val_loss: 0.1032\n",
      "Epoch 172/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1048 - val_loss: 0.1027\n",
      "Epoch 173/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1048 - val_loss: 0.1026\n",
      "Epoch 174/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1050 - val_loss: 0.1039\n",
      "Epoch 175/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1050 - val_loss: 0.1030\n",
      "Epoch 176/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1050 - val_loss: 0.1029\n",
      "Epoch 177/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1049 - val_loss: 0.1032\n",
      "Epoch 178/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1048 - val_loss: 0.1032\n",
      "Epoch 179/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1053 - val_loss: 0.1029\n",
      "Epoch 180/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1046 - val_loss: 0.1031\n",
      "Epoch 181/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1045 - val_loss: 0.1040\n",
      "Epoch 182/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1048 - val_loss: 0.1027\n",
      "Epoch 183/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1047 - val_loss: 0.1025\n",
      "Epoch 184/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1044 - val_loss: 0.1027\n",
      "Epoch 185/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1044 - val_loss: 0.1024\n",
      "Epoch 186/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1043 - val_loss: 0.1025\n",
      "Epoch 187/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1043 - val_loss: 0.1026\n",
      "Epoch 188/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1042 - val_loss: 0.1033\n",
      "Epoch 189/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1043 - val_loss: 0.1030\n",
      "Epoch 190/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1044 - val_loss: 0.1028\n",
      "Epoch 191/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1042 - val_loss: 0.1028\n",
      "Epoch 192/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1041 - val_loss: 0.1026\n",
      "Epoch 193/200\n",
      "10/10 [==============================] - 4s 376ms/step - loss: 0.1040 - val_loss: 0.1026\n",
      "Epoch 194/200\n",
      "10/10 [==============================] - 4s 378ms/step - loss: 0.1045 - val_loss: 0.1048\n",
      "Epoch 195/200\n",
      "10/10 [==============================] - 4s 377ms/step - loss: 0.1047 - val_loss: 0.1031\n",
      "Epoch 196/200\n",
      "10/10 [==============================] - 4s 373ms/step - loss: 0.1046 - val_loss: 0.1027\n",
      "Epoch 197/200\n",
      "10/10 [==============================] - 4s 375ms/step - loss: 0.1046 - val_loss: 0.1031\n",
      "Epoch 198/200\n",
      "10/10 [==============================] - 4s 374ms/step - loss: 0.1042 - val_loss: 0.1024\n",
      "Epoch 199/200\n",
      "10/10 [==============================] - 4s 359ms/step - loss: 0.1042 - val_loss: 0.1027\n",
      "Epoch 200/200\n",
      "10/10 [==============================] - 4s 361ms/step - loss: 0.1045 - val_loss: 0.1026\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mae')\n",
    "history = autoencoder.fit(x_train, x_train,\n",
    "                          epochs=200,\n",
    "                          shuffle=True,\n",
    "                          validation_data=(x_valid, x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0464aa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.savefig('autoencoder_loss.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9256365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2512f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, threshold):\n",
    "  reconstructions = model(data)\n",
    "  loss = tf.keras.losses.mae(reconstructions, data)\n",
    "  return tf.math.less(loss, threshold)\n",
    "\n",
    "def print_stats(predictions, labels):\n",
    "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
    "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
    "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a347b3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold:  0.22236362\n"
     ]
    }
   ],
   "source": [
    "reconstructions = autoencoder.predict(x_train)\n",
    "train_loss = tf.keras.losses.mae(reconstructions, x_train)\n",
    "threshold = np.mean(train_loss) + np.std(train_loss)\n",
    "print(\"Threshold: \", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6d9afbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "autoencoder.encoder.save('./saved_model/untar_encoder.h5')\n",
    "autoencoder.decoder.save('./saved_model/untar_decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88dde7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "load_encoder = tf.keras.models.load_model('./saved_model/untar_encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "802b69dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.47843137, 0.47843137, 0.47843137],\n",
       "        [0.4       , 0.4       , 0.4       ],\n",
       "        [0.32941177, 0.32941177, 0.32941177],\n",
       "        ...,\n",
       "        [0.4       , 0.4       , 0.4       ],\n",
       "        [0.4       , 0.4       , 0.4       ],\n",
       "        [0.43529412, 0.43529412, 0.43529412]],\n",
       "\n",
       "       [[0.46666667, 0.46666667, 0.46666667],\n",
       "        [0.4       , 0.4       , 0.4       ],\n",
       "        [0.3372549 , 0.3372549 , 0.3372549 ],\n",
       "        ...,\n",
       "        [0.41568628, 0.41568628, 0.41568628],\n",
       "        [0.41568628, 0.41568628, 0.41568628],\n",
       "        [0.41960785, 0.41960785, 0.41960785]],\n",
       "\n",
       "       [[0.47058824, 0.47058824, 0.47058824],\n",
       "        [0.4       , 0.4       , 0.4       ],\n",
       "        [0.32156864, 0.32156864, 0.32156864],\n",
       "        ...,\n",
       "        [0.4       , 0.4       , 0.4       ],\n",
       "        [0.41568628, 0.41568628, 0.41568628],\n",
       "        [0.4       , 0.4       , 0.4       ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.81960785, 0.81960785, 0.81960785],\n",
       "        [0.81960785, 0.81960785, 0.81960785],\n",
       "        [0.81960785, 0.81960785, 0.81960785]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.85882354, 0.85882354, 0.85882354],\n",
       "        [0.84705883, 0.84705883, 0.84705883],\n",
       "        [0.8392157 , 0.8392157 , 0.8392157 ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "beab9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = load_encoder(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7712c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(26, 1, 1, 8), dtype=float32, numpy=\n",
       "array([[[[30.069407 ,  0.       ,  0.       , 35.08493  ,  0.       ,\n",
       "           0.       , 10.593007 , 25.348541 ]]],\n",
       "\n",
       "\n",
       "       [[[32.31512  ,  0.       ,  0.       , 31.568275 ,  0.       ,\n",
       "           0.       , 13.54235  , 20.370378 ]]],\n",
       "\n",
       "\n",
       "       [[[20.578207 ,  0.       ,  0.       , 20.825321 ,  0.       ,\n",
       "           0.       ,  5.6223454, 26.327078 ]]],\n",
       "\n",
       "\n",
       "       [[[26.992529 ,  0.       ,  0.       , 25.729515 ,  0.       ,\n",
       "           0.       ,  9.654528 , 25.395878 ]]],\n",
       "\n",
       "\n",
       "       [[[31.310493 ,  0.       ,  0.       , 22.988861 ,  0.       ,\n",
       "           0.       , 11.255538 , 24.986782 ]]],\n",
       "\n",
       "\n",
       "       [[[36.610676 ,  0.       ,  0.       , 22.557203 ,  0.       ,\n",
       "           0.       , 16.18159  , 15.006612 ]]],\n",
       "\n",
       "\n",
       "       [[[27.844097 ,  0.       ,  0.       , 29.891993 ,  0.       ,\n",
       "           0.       , 17.38746  , 20.939976 ]]],\n",
       "\n",
       "\n",
       "       [[[33.544846 ,  0.       ,  0.       , 34.958714 ,  0.       ,\n",
       "           0.       , 11.495839 , 21.288172 ]]],\n",
       "\n",
       "\n",
       "       [[[27.491093 ,  0.       ,  0.       , 25.897383 ,  0.       ,\n",
       "           0.       ,  7.6859937, 28.014494 ]]],\n",
       "\n",
       "\n",
       "       [[[35.154545 ,  0.       ,  0.       , 15.187793 ,  0.       ,\n",
       "           0.       ,  6.043251 , 14.187851 ]]],\n",
       "\n",
       "\n",
       "       [[[31.031775 ,  0.       ,  0.       , 29.799118 ,  0.       ,\n",
       "           0.       ,  9.866601 , 24.393818 ]]],\n",
       "\n",
       "\n",
       "       [[[37.50115  ,  0.       ,  0.       , 16.014456 ,  0.       ,\n",
       "           0.       ,  6.753453 , 23.4441   ]]],\n",
       "\n",
       "\n",
       "       [[[25.12801  ,  0.       ,  0.       , 22.978973 ,  0.       ,\n",
       "           0.       , 10.28287  , 16.48456  ]]],\n",
       "\n",
       "\n",
       "       [[[40.438705 ,  0.       ,  0.       , 21.123009 ,  0.       ,\n",
       "           0.       , 10.436735 , 18.002602 ]]],\n",
       "\n",
       "\n",
       "       [[[36.613735 ,  0.       ,  0.       , 32.20998  ,  0.       ,\n",
       "           0.       ,  6.025898 , 23.749355 ]]],\n",
       "\n",
       "\n",
       "       [[[47.436153 ,  0.       ,  0.       , 19.442343 ,  0.       ,\n",
       "           0.       ,  8.824191 , 18.443022 ]]],\n",
       "\n",
       "\n",
       "       [[[40.057865 ,  0.       ,  0.       , 18.22796  ,  0.       ,\n",
       "           0.       , 15.777102 , 13.555627 ]]],\n",
       "\n",
       "\n",
       "       [[[30.110361 ,  0.       ,  0.       , 15.924258 ,  0.       ,\n",
       "           0.       ,  9.378509 , 22.49882  ]]],\n",
       "\n",
       "\n",
       "       [[[22.23843  ,  0.       ,  0.       , 28.669962 ,  0.       ,\n",
       "           0.       , 11.473513 , 26.965927 ]]],\n",
       "\n",
       "\n",
       "       [[[36.26878  ,  0.       ,  0.       , 18.102612 ,  0.       ,\n",
       "           0.       , 13.015489 , 17.351244 ]]],\n",
       "\n",
       "\n",
       "       [[[37.105392 ,  0.       ,  0.       , 21.254122 ,  0.       ,\n",
       "           0.       ,  9.744409 , 20.292551 ]]],\n",
       "\n",
       "\n",
       "       [[[34.666084 ,  0.       ,  0.       , 21.45724  ,  0.       ,\n",
       "           0.       ,  8.541188 , 23.40567  ]]],\n",
       "\n",
       "\n",
       "       [[[39.287518 ,  0.       ,  0.       , 25.766447 ,  0.       ,\n",
       "           0.       ,  3.3904006, 27.55037  ]]],\n",
       "\n",
       "\n",
       "       [[[32.132755 ,  0.       ,  0.       , 27.909897 ,  0.       ,\n",
       "           0.       , 14.512713 , 18.47351  ]]],\n",
       "\n",
       "\n",
       "       [[[35.109936 ,  0.       ,  0.       , 22.591354 ,  0.       ,\n",
       "           0.       , 11.65532  , 19.887691 ]]],\n",
       "\n",
       "\n",
       "       [[[33.186115 ,  0.       ,  0.       , 21.97283  ,  0.       ,\n",
       "           0.       , 12.700904 , 19.642998 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbc5a42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 8), dtype=float32, numpy=\n",
       "array([[[30.069407,  0.      ,  0.      , 35.08493 ,  0.      ,\n",
       "          0.      , 10.593007, 25.348541]]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tar[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5f8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
