{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420a0eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "D:\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c83ec987",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_en = open('en.txt','r',encoding='utf-8').readlines()\n",
    "file_cn = open('cn.txt','r',encoding='utf-8').readlines()\n",
    "en = []\n",
    "cn = []\n",
    "for e,c in zip(file_en,file_cn):\n",
    "    en.append(e.strip())\n",
    "    cn.append(c.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c14b9673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929 or 1989?',\n",
       " 'PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.',\n",
       " 'At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.',\n",
       " 'Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.',\n",
       " 'The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).',\n",
       " 'Europe is being cautious in the name of avoiding debt and defending the euro, whereas the US has moved on many fronts in order not to waste an ideal opportunity to implement badly needed structural reforms.',\n",
       " 'For geo-strategists, however, the year that naturally comes to mind, in both politics and economics, is 1989.',\n",
       " 'Of course, the fall of the house of Lehman Brothers has nothing to do with the fall of the Berlin Wall.',\n",
       " 'Indeed, on the surface it seems to be its perfect antithesis: the collapse of a wall symbolizing oppression and artificial divisions versus the collapse of a seemingly indestructible and reassuring institution of financial capitalism.',\n",
       " 'Yet 2008-2009, like 1989, may very well correspond to an epochal change, whose unfolding consequences will be felt for decades.',\n",
       " 'The end of the East-West ideological divide and the end of absolute faith in markets are historical turning points.',\n",
       " 'And what happens in 2009 may jeopardize some of the positive results of 1989, including the peaceful reunification of Europe and the triumph of democratic principles over nationalist, if not xenophobic, tendencies.',\n",
       " 'In 1989, liberal democracy triumphed over the socialist ideology incarnated and promoted by the Soviet Bloc.',\n",
       " 'For many of his supporters, it was President Ronald Reagan who, with his deliberate escalation of the arms race, pushed the Soviet economy to the brink, thereby fully demonstrating the superiority of liberal societies and free markets.',\n",
       " 'Of course, there are obvious differences between 1989 and now.',\n",
       " 'First, and perhaps above all, the revolutions of 1989 and the subsequent collapse of the Soviet Union put an end to global bipolarity.',\n",
       " 'By contrast, 2009 is likely to pave the way to a new form of bipolarity, but with China substituting for the Soviet Union.',\n",
       " 'Second, whereas democracy and market capitalism appeared as clear – if more fragile than expected – winners in 1989, it is difficult in 2009, with the spread of the global crisis, to distinguish winners from losers.',\n",
       " 'Everyone seems to be a loser, even if some are more affected than others.',\n",
       " 'Yet, history is unfair, and the US, despite its greater responsibility for today’s global crisis, may emerge in better shape than most countries from the morass.',\n",
       " 'In better shape, but not alone.',\n",
       " 'As a visiting professor at Harvard and MIT, I am getting a good preview of what the world could look like when the crisis finally passes.',\n",
       " 'One senses something like the making of an American-Asian dominated universe.',\n",
       " 'From the incredible media lab at MIT to the mathematics and economics departments at Harvard, Asians – Chinese and Indians, in particular – are everywhere, like the Romans in Athens in the first century BC: full of admiration for those from whom they were learning so much, and whom they would overcome in the coming decades.',\n",
       " 'But before this new order appears, the world may be faced with spreading disorder, if not outright chaos.',\n",
       " 'What, for example, will happen to a country as central and vulnerable as Egypt when hundred of thousands of Egyptians working in the Gulf are forced to return to their homeland as a result of the crisis in the oil-producing countries?',\n",
       " 'When the rich get less rich, the poor get poorer.',\n",
       " 'And what about the foreign workers who have reached for the “European dream” and are now faced with potential explosions of xenophobia in Europe’s supposedly open countries?',\n",
       " 'The consequences of 1989 ended up being less enduring than many observers, including me, would have assumed.',\n",
       " 'We can only hope that, in the end, the consequences of 2009 similarly prove to be far less dramatic than we now – intuitively and in our historical reflexes – feel them to be.',\n",
       " 'What Failed in 2008?',\n",
       " 'BERKELEY – To solve a problem, it is not enough to know what to do.',\n",
       " 'You actually have to implement the solution – and be willing to change course if it turns out that you did not know quite as much as you thought.',\n",
       " 'As a result, markets were deregulated, making it easier to trade assets that were perceived to be safe, but were in fact not.',\n",
       " 'As a result, systemic risk proliferated beyond central bankers’ wildest imagination.',\n",
       " 'Untested – and ultimately incorrect – assumptions created a policymaking environment defined by what can only be called hubris.',\n",
       " 'Officials underestimated tail risks.',\n",
       " 'They set inflation targets at around 2% – leaving little room for maneuver when the water got choppy.',\n",
       " 'And, most audaciously of all, the European Union introduced the euro as a common currency.',\n",
       " 'Indeed, wrongheaded policymaking continued long after the crisis began.',\n",
       " 'Politicians responded to worsening economic conditions by hewing as closely as possible to failed prescriptions, making sure to do no more than absolutely necessary to address the biggest economic disaster since the Great Depression.',\n",
       " 'Wolf’s prescription for countering the crisis is simple, smart, and unassailable.',\n",
       " 'In the short term, he suggests that countries with reserve currencies spend more (especially to finance public-sector investments) and issue more debt.',\n",
       " 'Their central banks, he argues, should raise inflation targets to 3% or even 4% per year.',\n",
       " 'Over the medium term, according to Wolf, countries need to put in place regulatory measures that lower debt levels and discourage overleveraging.',\n",
       " 'The eurozone, too, must resolve its internal contradictions, either by disbanding or by introducing “a minimum set of institutions and policies” that allow the monetary union to function properly.',\n",
       " 'Wolf’s long-term solutions include tackling inequality, “more global regulation,” a greater degree of “freedom for individual countries to craft their own responses,” and economic analysis that is less in thrall to the free-market ideologues that led us into the crisis in the first place.',\n",
       " 'And yet, as recommendable as Wolf’s proposals may be, little has been done to implement them.',\n",
       " 'The reasons why are found in the second book: Hall of Mirrors, by&nbsp;my friend, teacher, and patron, Barry Eichengreen.',\n",
       " 'Eichengreen traces our tepid response to the crisis to the triumph of monetarist economists, the disciples of Milton Friedman, over their Keynesian and Minskyite peers – at least when it comes to interpretations of the causes and consequences of the Great Depression.',\n",
       " 'When the 2008 financial crisis erupted, policymakers tried to apply Friedman’s proposed solutions to the Great Depression.',\n",
       " 'Unfortunately, this turned out to be the wrong thing to do, as the monetarist interpretation of the Great Depression was, to put it bluntly, wrong in significant respects and radically incomplete.',\n",
       " 'The resulting policies were enough to prevent the post-2008 recession from developing into a full-blown depression; but that partial success turned out to be a Pyrrhic victory, for it allowed politicians to declare that the crisis had been overcome, and that it was time to embrace austerity and focus on structural reform.',\n",
       " 'The result is today’s stagnant economy, marked by anemic growth that threatens to become the new normal.',\n",
       " 'The United States and Europe are on track to have thrown away 10% of their potential wealth, while the failure to strengthen financial-sector regulation has left the world economy exposed to the risk of another major crisis.',\n",
       " 'Wolf and Eichengreen would agree that the main shortcomings that led to the 2008 financial crisis – and that continue to underpin our inadequate response to it – are intellectual.',\n",
       " 'Indeed, the only true lesson of the crisis so far seems to be that its lessons will never truly be learned.',\n",
       " 'A Comeback Strategy for Europe',\n",
       " 'STOCKHOLM/MADRID – When Pope Francis addressed the European Parliament last November, he compared the European Union to a grandmother – pleasant and rich with experience, but lacking the vitality and energy of the past.',\n",
       " 'It is high time, Francis argued, that EU leaders shed their dozy image, recognize the strategic challenges that Europe faces, and forge a clear policy for tackling them.',\n",
       " 'Admittedly, the pope’s characterization was alarmingly accurate in some respects.',\n",
       " 'But, despite its seeming lassitude, Europe retains significant strengths.',\n",
       " 'It is a hub of high-level thought and innovation; it is home to some of the world’s most competitive regions and industries; and, perhaps most impressive, it has built a community and market encompassing a half-billion people.',\n",
       " 'But the world is changing: the Asia-Pacific region is increasingly influencing global developments, economic and otherwise.',\n",
       " 'The Trans-Pacific Partnership – by which the United States and 11 other countries would create a mega-regional free-trade zone – would most likely accelerate this shift (all the more so if China eventually joins).',\n",
       " 'Though the TPP faces no shortage of hurdles to clear before an agreement is finalized, its potential to augment Asia’s economic power cannot be underestimated.',\n",
       " 'Europe must work to secure its position in the new world order – beginning by enhancing its own trade and investment ties with the US.',\n",
       " 'The problem is that, as the TPP negotiations progress, talks on the EU-US Transatlantic Trade and Investment Partnership (TTIP) have become so deeply mired in domestic controversies that the entire project may well be scuttled.',\n",
       " 'Business leaders on both sides of the Atlantic are convinced that a successful TTIP agreement would bring substantial economic benefits – a perception that many studies reinforce.',\n",
       " 'Yet trivial issues – for example, the use of chlorinated chicken and settlements of investor disputes – continue to dominate the debate.',\n",
       " 'The TTIP’s goal is to unleash the power of the transatlantic economy, which remains by far the world’s largest and wealthiest market, accounting for three-quarters of global financial activity and more than half of world trade.',\n",
       " '(If the TTIP was opened to other economies – such as Turkey, Mexico, and Canada – the benefits would be even greater.)',\n",
       " 'Even more compelling than the benefits of achieving an agreement, though, are the potentially catastrophic consequences of failure.',\n",
       " 'For starters, a breakdown of TTIP talks would give considerable ammunition to those in the United Kingdom who advocate withdrawal from the EU; conversely, if the TTIP were implemented, the UK would be unwise – and thus unlikely – to leave.',\n",
       " 'Moreover, the perception that the EU’s internal squabbles had led it to squander a strategic opportunity would probably drive the US to accelerate its disengagement from the continent.',\n",
       " 'And Russian President Vladimir Putin would invariably regard the EU’s failure as a major opportunity to exert more influence over parts of Europe.',\n",
       " 'All of this contributes to a starkly fundamental strategic risk: If the TTIP stalls or collapses, while the TPP moves forward and succeeds, the global balance will tip strongly in Asia’s favor – and Europe will have few options, if any, for regaining its economic and geopolitical influence.',\n",
       " 'When the TTIP was first proposed, Europe seemed to recognize its value.',\n",
       " 'Indeed, it was the EU that pushed the US, which initially doubted Europe’s commitment, to launch the negotiation process in June 2013.',\n",
       " 'The ambition was to complete the negotiations on “one tank of gas.”',\n",
       " 'No one wanted to endure protracted talks – or the associated political pain.',\n",
       " 'But EU leaders essentially abandoned the project, seemingly confirming American fears.',\n",
       " 'Trade negotiators struggled to make headway, while anti-globalization groups seized control of the public discourse, presenting the TTIP as a threat to everything from Europe’s democracy to its health.',\n",
       " 'This is dangerously inaccurate talk, and EU leaders must prevent it from gaining any more traction by making the strategic case for the agreement.',\n",
       " 'And they must revive their commitment to conclude the talks successfully in 2015.',\n",
       " 'This is not to say that resolving the remaining issues in the TTIP negotiations will be simple.',\n",
       " 'But establishing a trade agreement, especially one that entails so many regulatory issues, is always difficult, as it must account for the complexity and changeability of modern economies.',\n",
       " 'The fact is that the challenges inherent in completing the TTIP are no more intractable than those that EU leaders have faced in the last few years of crisis.',\n",
       " 'When the TTIP negotiations resume next month, EU leaders must push for genuine progress, with the goal of completing a deal by the end of the year.',\n",
       " 'The good news is that the recent midterm elections in the US might have improved their chances.',\n",
       " 'President Barack Obama now might get so-called fast-track negotiating authority from Congress.',\n",
       " 'If he does, Congress would simply approve or reject any negotiated agreement, rather than picking it apart.',\n",
       " 'The US presidential election season is starting, and other issues in the new year could easily take over the EU agenda.',\n",
       " 'That is why Europe’s leaders have no time to waste.',\n",
       " 'They must seize economic opportunity – and avert strategic disaster.',\n",
       " 'The Year That Ended an Epoch?',\n",
       " 'MADRID – As 2016 comes to an end, the outlook for 2017 is shrouded in uncertainty.',\n",
       " 'Tensions in the Middle East are rising, and populist movements have appeared in Europe and the United States.',\n",
       " 'In the Middle East, the tragic conflict in Syria continues, despite several fruitless attempts at rapprochement, which were marred by the fundamental disagreement about Syrian President Bashar al-Assad’s future role in any peace process or political transition.',\n",
       " 'Meanwhile, over the past week, Syrian government troops, backed by Russia and Iran, have retaken almost all of Aleppo – once Syria’s largest city, now utterly devastated by the war.',\n",
       " 'The world’s priority for the coming year must be to achieve peace in Syria, which will require close regional and international cooperation.',\n",
       " 'On December 27, Iran, Russia, and Turkey will hold a tripartite meeting in Moscow to discuss a political solution for the Syria conflict.',\n",
       " 'That meeting, if it takes place, is likely to be overshadowed by the fallout from the assassination of Russia’s ambassador to Turkey.',\n",
       " 'But it is nothing if not surprising that these parties, and not the US and the European Union, would be negotiating such an agreement.',\n",
       " 'One positive development this year came in March, when the EU and Turkey signed an agreement to address the refugee crisis.',\n",
       " 'Turkey has now taken in some three million Syrian refugees since the beginning of the conflict.',\n",
       " 'Although EU-Turkey relations are currently not at their best, the dialogue between the two sides must continue in 2017, not least because of their common interests, which are based not only on economic interdependence, but also on the refugee crisis and the collective fight against terrorism.',\n",
       " 'European politics next year, meanwhile, will be consumed by the Brexit negotiations.',\n",
       " 'In March, the United Kingdom will likely invoke Article 50 of the Treaty of Lisbon, triggering the formal procedure for withdrawal from the EU.',\n",
       " 'The challenge will be to reach an agreement that guarantees the wellbeing of future EU-UK relations.',\n",
       " 'This will not be easy, and EU negotiators have already set a timeline of only 18 months.',\n",
       " 'While much remains uncertain, what is clear is that if the UK wants to retain access to the European single market, it will have to accept the EU’s four freedoms, including the free movement of workers.',\n",
       " 'In 2017, several European countries will hold general elections, and there is a risk that isolationist, anti-European populist movements will make a strong showing.',\n",
       " 'For the EU to lose a country as militarily and economically important as the UK is bad enough; but to lose a founding EU member state, such as France, would be tragic.',\n",
       " 'Fortunately, many Europeans’ views toward the EU actually improved in the aftermath of the Brexit referendum.',\n",
       " 'But this will not lessen the challenge for EU governments in the year ahead.',\n",
       " 'They must unite societies divided by powerful global forces, such as globalization and rapid technological innovation.',\n",
       " 'The Brexit referendum, followed by Donald Trump’s victory in the US presidential election, signaled the rise of populism in the West.',\n",
       " 'But now that Trump is filling his cabinet with oligarchs and former military men, we have reason to doubt that he will keep his promise to govern without the Washington “establishment.”',\n",
       " 'Trump’s incoming administration is full of unknowns, but there can be no doubt that his rejection of multilateral institutions will endanger international efforts to cooperate on solutions to the world’s biggest problems.',\n",
       " 'This holds peril for US-EU relations.',\n",
       " 'In previous years, the Paris climate agreement and the nuclear agreement with Iran were rays of light in a world closing itself off to multilateralism.',\n",
       " 'In the coming years, such rays may become scarcer still.',\n",
       " 'Now more than ever, we need the kind of dialogue that builds strategic trust between great powers.',\n",
       " 'And yet, Trump’s statements casting doubt on continued US adherence to a “One China” policy vis-à-vis Taiwan could severely damage relations between the world’s two largest economies.',\n",
       " 'Similarly, notwithstanding the pro-Russian leanings of some among Trump’s team, the US-Russian relationship also lacks strategic trust, owing to Russia’s military intervention in Syria, its invasion of eastern Ukraine, and its alleged interference in the US election.',\n",
       " 'The coming year will be particularly important for Europe.',\n",
       " 'Relations between the EU and the US must remain strong, rooted in mutual respect for democracy, freedom, and human rights.',\n",
       " 'After a turbulent 2016, and with little positive news in international politics, 2017 is shaping up to be a year of challenges and uncertainty.',\n",
       " 'But the biggest uncertainty of all is whether this is simply the end of another year, or the end of a geopolitical epoch.',\n",
       " 'Another Slow Year for the Global Economy',\n",
       " 'WASHINGTON, DC – Last April, the International Monetary Fund projected that the world economy would grow by 3.5% in 2015.',\n",
       " 'In the ensuing months, that forecast was steadily whittled down, reaching 3.1% in October.',\n",
       " 'But the IMF continues to insist – as it has, with almost banal predictability, for the last seven years – that next year will be better.',\n",
       " 'But it is almost certainly wrong yet again.',\n",
       " 'For starters, world trade is growing at an anemic annual rate of 2%, compared to 8% from 2003 to 2007.',\n",
       " 'Whereas trade growth during those heady years far exceeded that of world GDP, which averaged 4.5%, lately, trade and GDP growth rates have been about the same.',\n",
       " 'Even if GDP growth outstrips growth in trade this year, it will likely amount to no more than 2.7%.',\n",
       " 'The question is why.',\n",
       " 'According to Christina and David Romer of the University of California, Berkeley, the aftershocks of modern financial crises – that is, since World War II – fade after 2-3 years.',\n",
       " 'The Harvard economists Carmen Reinhart and Kenneth Rogoff say that it takes five years for a country to dig itself out of a financial crisis.',\n",
       " 'And, indeed, the financial dislocations of 2007-2008 have largely receded.',\n",
       " 'So what accounts for the sluggish economic recovery?',\n",
       " 'One popular explanation lies in the fuzzy notion of “secular stagnation”: long-term depressed demand for goods and services is undermining incentives to invest and hire.',\n",
       " 'But demand would remain weak only if people lacked confidence in the future.',\n",
       " 'The only logical explanation for this enduring lack of confidence, as Northwestern University’s Robert Gordon has painstakingly documented and argued, is slow productivity growth.',\n",
       " 'Before the crisis – and especially from 2003 to 2007 – slow productivity growth was being obscured by an illusory sense of prosperity in much of the world.',\n",
       " 'In some countries – notably, the United States, Spain, and Ireland – rising real-estate prices, speculative construction, and financial risk-taking were mutually reinforcing.',\n",
       " 'At the same time, countries were amplifying one another’s growth through trade.',\n",
       " 'Central to the global boom was China, the rising giant that flooded the world with cheap exports, putting a lid on global inflation.',\n",
       " 'Equally important, China imported a huge volume of commodities, thereby bolstering many African and Latin American economies, and purchased German cars and machines, enabling Europe’s largest economy to keep its regional supply chains humming.',\n",
       " 'This dynamic reversed around March 2008, when the US rescued its fifth-largest investment bank, Bear Sterns, from collapse.',\n",
       " 'With the eurozone banks also deeply implicated in the subprime mortgage mess and desperately short of US dollars, America and much of Europe began a remorseless slide into recession.',\n",
       " 'Whereas in the boom years, world trade had spread the bounty, it was now spreading the malaise.',\n",
       " 'As each country’s GDP growth slowed, so did its imports, causing its trading partners’ growth to slow as well.',\n",
       " 'The US economy began to emerge from its recession in the second half of 2009, thanks largely to aggressive monetary policy and steps to stabilize the financial system.',\n",
       " 'Eurozone policymakers, by contrast, rejected monetary stimulus and implemented fiscal austerity measures, while ignoring the deepening distress of their banks.',\n",
       " 'The eurozone thus pushed the world into a second global recession.',\n",
       " 'Just when that recession seemed to have run its course, emerging economies began to unravel.',\n",
       " 'For years, observers had been touting the governance and growth-enhancing reforms that these countries’ leaders had supposedly introduced.',\n",
       " 'In October 2012, the IMF celebrated emerging economies’ “resilience.”',\n",
       " 'As if on cue, that facade began to crumble, revealing an inconvenient truth: factors like high commodity prices and massive capital inflows had been concealing serious economic weaknesses, while legitimizing a culture of garish inequality and rampant corruption.',\n",
       " 'These problems are now being compounded by the growth slowdown in China, the fulcrum of global trade.',\n",
       " 'And the worst is yet to come.',\n",
       " 'China’s huge industrial overcapacity and property glut needs to be wound down; the hubris driving its global acquisitions must be reined in; and its corruption networks have to be dismantled.',\n",
       " 'In short, the factors that dragged down the global economy in 2015 will persist – and in some cases even intensify – in the new year.',\n",
       " 'Emerging economies will remain weak.',\n",
       " 'The eurozone, having enjoyed a temporary reprieve from austerity, will be constrained by listless global trade.',\n",
       " 'Rising interest rates on corporate bonds portend slower growth in the US.',\n",
       " 'China’s collapsing asset values could trigger financial turbulence.',\n",
       " 'And policymakers are adrift, with little political leverage to stem these trends.',\n",
       " 'The IMF should stop forecasting renewed growth and issue a warning that the global economy will remain weak and vulnerable unless world leaders act energetically to spur innovation and growth.',\n",
       " 'Such an effort is long overdue.',\n",
       " 'Трампты? белг?с?зд?к',\n",
       " 'НЬЮ-ЙОРК – ?рб?р ?а?тар айында, мен алда?ы жыл?а арнал?ан болжамды дайындай бастаймын.',\n",
       " 'Экономикалы? болжау дайындау о?ай ?с емес; алайда, Гарри Трумэнн?? б?р ?олды экономисттерд?? болуына (?ек?нш? жа?ынан? деп айта алмайтын) ?т?н?ш?нде шынды?ты? барына ?арамастан, мен?? болжамдарым д?рыс болды деп айта аламын.',\n",
       " 'Со??ы жылдары, мен к?шт? фискалды? ынталандыру болма?ан жа?дайда, (Еуропада немесе Америка ??рама Штаттарында жа?ын арада болжанба?ан) 2008 жылды? ?лы ??лдырауынан сауы?у баяу болады деп д?рыс таптым.',\n",
       " 'Осы болжамдарды жаса?ан кезде мен кешенд? эконометрикалы? модельдерге ?ара?анда базалы? экономикалы? к?штерд? талдау?а толы?ыра? с?йенген болатынмын.',\n",
       " 'Мысалы, 2016 жылды? басында, со??ы б?рнеше жыл бойы орын ал?ан жа?анды? жиынты? с?ранысты? тапшылы?ыны? к?рт ?згеру? ек?талай екен? аны? к?р?нд?. Сонды?тан, мен ?серл? ?алып?а келуд? болжа?андар туралы олар ?лемд? тым жа?сы жа?ынан к?р?п, кемш?л?ктерд? ескермей жатыр деп ойладым.',\n",
       " 'Мен к?ткендей, к?птеген экономикалы? даму жа?дайлары орын алды.',\n",
       " 'Б?ра?, 2016 жылды? саяси о?и?алары олай жайлы болмады.',\n",
       " 'Мен, ?с?п келе жат?ан те?с?зд?к м?селес?, ?с?ресе А?Ш-та, сондай-а? б?к?л ?лем бойынша к?птеген елдерде, шеш?лмесе, онда оны? саяси салдары болатыны туралы к?птеген жылдар бойы жазып келд?м.',\n",
       " 'Б?ра? те?с?зд?кке байланысты жа?дай нашарлай бастады, т?пт? А?Ш-та орташа ?м?р с?ру ?за?ты?ы ?ыс?аруда деген шошынышты деректер де белг?л? болды.',\n",
       " 'Энн Кейс? ж?не Ангус Дитонны? ?ткен жыл?ы зерттеу?нде айтыл?андай, т?р?ын халы?ты? ?лкен сегменттер?нде ?м?р с?ру ?за?ты?ы ?ыс?аруда, онын ?ш?нде Американы? Тот белдеу?н?? ашулы ерлер? де бар.',\n",
       " 'Б?ра?, табысы т?мендерд?? 90%-ы ?асырды? ?штен б?р? бойы то?ырауда бол?ан (ж?не елеул? б?л?г? т?мендеп жат?ан) жа?дайда, денсаулы? са?тау деректер?не ж?г?нсек, ел?м?зд?? ?те ?лкен б?л?г?н?? жа?дайы нашар.',\n",
       " 'Америка б?л ?рд?ст?? басында бол?анымен, бас?а жерлерде жа?дай б?дан аса жа?сы емес.',\n",
       " 'Саяси салдарды? болуы аны? к?р?нген?мен, оларды? нысаны мен мерз?мдер? ?лде?айда ай?ын болмады.',\n",
       " 'Б?рын емес, ал д?л экономика т?зет?л?п келе жат?анда, А?Ш-та неге саяси ?арсылы? пайда болды?',\n",
       " 'Ж?не неге ол о??а ?арай бет алды?',\n",
       " 'Нег?з?, ?здер? ?атты жа?та?ан? жа?андану н?тижес?нде ж?мыс орындарын жо?алт?ан адамдар?а ?сынылатын к?мект? б??атта?ан республикашылдар болды. 26 штатта, медициналы? к?мект? ке?ейтуге республиканшылдар ?арсы болып, осылайша жа?дайлары т?мен адамдар?а медициналы? са?тандыру бер?лмед?.',\n",
       " 'Ж?не бас?аларды пайдалана отырып ?м?р с?рген, ашы? т?рде салы?ты т?лемеген?н мойындап ж?не салы?тан жалтаруды ма?таныш ?ыл?ан б?реу ?алайша же??ске жетт??',\n",
       " 'Дональд Трамп заман ?рд?с?н д?л тапты: жа?дай жа?сы болмады, ж?не к?птеген сайлаушылар ?згер?ст? ?алады.',\n",
       " 'Енд? оларды? т?лег? орындалады- еш?андай ?с ?деттег?дей болмайды. Б?ра? м?ндай белг?с?зд?к ?л? болма?ан.',\n",
       " 'Трамп ?андай саясат ж?рг?зет?н?, оларды? ?айсысы табыс?а жетед? немесе? оларды? салдары ?андай болады - м?ны? б?р? белг?с?з болып т?р.',\n",
       " 'Трамп ?алай болса да сауда со?ысын жасайтындай.',\n",
       " 'Б?ра? ?ытай мен Мексика ?андай жауап ?айтарады?',\n",
       " 'Трамп ?з?н?? ?сыныстары Д?ниеж?з?л?к сауда ?йымы ережес?н б?затынын т?с?ну?-а? м?мк?н, б?ра? ол сондай-а?, ДС? о?ан ?арсы шы?уы ?ш?н ?за? уа?ыт ?ажет екен?н б?лет?н шы?ар.',\n",
       " 'Ал ол уа?ыт?а дей?н, Американы? сауда шоты ?айта баланс?а келу? м?мк?н.',\n",
       " 'Б?ра? б?л ойынды ек? жа? та ойнай алады: ?ытай, ??сас ?рекеттерд? жасай алады, б?ра? оны? жауабы шеберлеу болуы м?мк?н.',\n",
       " 'Б?ра? сауда со?ысы орын алса, ?андай болар ед??',\n",
       " 'Трампты? же?е аламын деп ойлауында нег?з болуы м?мк?н; ?йткен?, ?ытай А?Ш-?а экспорт бойынша т?уелд?рек болып табылады, б?л ?рине А?Ш-?а арты?шылы? беред?.',\n",
       " 'Б?ра? сауда со?ысы б?р жа? ?ана же??п алатын ойын емес.',\n",
       " 'А?Ш та сондай-а? же??лед?. ?ытай ?тк?р саяси ауырсынуды тудыра отырып, кег?н ала алады.',\n",
       " '?ытайды? А?Ш-?а ?ара?анда, А?Ш-ты? ?здер?не зиян келт?ру?не ?арсы т?руында да арты?шылы?ы бар.',\n",
       " 'Ауырсынуды к?мн?? о?айыра? шыдай алатынын б?р?м?з б?лем?з ?ой.',\n",
       " 'Ол, ?арапайым хал?ы ?аз?рд?? ?з?нде ?за? зардап шеккен А?Ш па, немесе ?иын-?ыстау заман?а ?арамастан, 6% -дан асатын ?суд? ?алыптастыра б?лген ?ытай ма?',\n",
       " 'Ке??нен ал?анда, республикашылдар / Трамп к?н т?рт?б?, стандартты GOP рецепт?нде к?рсет?лгендег?ге ?ара?анда, салы? ?ыс?артуларымен б?рге байлар?а ?арай к?б?рек ба?ытталып, байлы?ты? т?мен сар?ылу идеясына нег?зделген, ол, шын м?н?нде ти?мд? болма?ан Рейган д?у?р?н?? ?сыным экономикасыны? жал?асындай к?р?нед?.? Алау ат?ан риторика, немесе Twitter-де та??ы ?ште жазыл?ан сандыра? ойлар, кем дегенде б?раз уа?ыт?а, Рейган революциясыны? салдарынан артты ?алып кеткендерд?? ашуын басуы м?мк?н.',\n",
       " 'Б?ра?? б?л ?аншалы?ты ?за? уа?ыт?а жетед??',\n",
       " 'Ал содан со? не болады?',\n",
       " 'Трамп вуду экономикасыны? ?з н?с?асын ж?рг?з?п, экономиканы? нег?зг? за?дарыны? к?ш?н жоюды ?алауы м?мк?н.',\n",
       " 'Б?ра? олай жасау м?мк?н емес. ?лемн?? е? ?р? экономикасы 2017 жылы ж?не одан ?р? белг?с?з саяси те??зге жол тарт?анда, ж?й б?р пенде ?ш?н болжам жасау ойсыз, а??ал ?ылы?тай болар, тек ай?ын ж?йттарды атау болмаса: су а?ыны ??былмалы болады, ж?не к?птеген саясаттанушы кемелер батып та кетер.',\n",
       " '9/11 and the New Authoritarianism',\n",
       " 'Five years after the attacks on the Twin Towers in New York and the Pentagon in Washington, “9/11” is no longer a mere date.',\n",
       " 'It has entered the history books as the beginning of something new, a new era perhaps, but in any case a time of change.',\n",
       " 'The terrorist bombings in Madrid and London and elsewhere will also be remembered; but it is “9/11” that has become the catchphrase, almost like “August 1914.”',\n",
       " 'But was it really a war that started on September 11, 2001?',\n",
       " 'Not all are happy about this American notion.',\n",
       " 'During the heyday of Irish terrorism in the UK, successive British governments went out of their way not to concede to the IRA the notion that a war was being waged.',\n",
       " '“War” would have meant acceptance of the terrorists as legitimate enemies, in a sense as equals in a bloody contest for which there are accepted rules of engagement.',\n",
       " 'This is neither a correct description nor a useful terminology for terrorist acts, which are more correctly described as criminal.',\n",
       " 'By calling them war – and naming an opponent, usually al-Qaeda and its leader, Osama bin Laden – the United States government has justified domestic changes that, before the 9/11 attacks, would have been unacceptable in any free country.',\n",
       " 'Most of these changes were embodied in the so-called “USA Patriot Act.”',\n",
       " 'Though some of the changes simply involved administrative regulations, the Patriot Act’s overall effect was to erode the great pillars of liberty, such as habeas corpus , the right to recourse to an independent court whenever the state deprives an individual of his freedom.',\n",
       " 'From an early date, the prison camp at Guantánamo Bay in Cuba became the symbol of something unheard of: the arrest without trial of “illegal combatants” who are deprived of all human rights.',\n",
       " 'The world now wonders how many more of these non-human humans are there in how many places.',\n",
       " 'For everyone else, a kind of state of emergency was proclaimed that has allowed state interference in essential civil rights.',\n",
       " 'Controls at borders have become an ordeal for many, and police persecution now burdens quite a few.',\n",
       " 'A climate of fear has made life hard for anyone who looks suspicious or acts suspiciously, notably for Muslims.',\n",
       " 'Such restrictions on freedom did not meet with much public opposition when they were adopted.',\n",
       " 'On the contrary, by and large it was the critics, not the supporters, of these measures who found themselves in trouble.',\n",
       " 'In Britain, where Prime Minister Tony Blair supported the US attitude entirely, the government introduced similar measures and even offered a new theory.',\n",
       " 'Blair was the first to argue that security is the first freedom.',\n",
       " 'In other words, liberty is not the right of individuals to define their own lives, but the right of the state to restrict individual freedom in the name of a security that only the state can define.',\n",
       " 'This is the beginning of a new authoritarianism.',\n",
       " 'The problem exists in all countries affected by the threat of terrorism, though in many it has not become quite as specific.',\n",
       " 'In most countries of continental Europe, “9/11” has remained an American date.',\n",
       " 'There is even a debate – and indeed some evidence – concerning the question of whether involvement in the “war against terrorism” has actually increased the threat of terrorist acts.',\n",
       " 'Germans certainly use this argument to stay out of the action wherever possible.',\n",
       " 'This stance, however, has not prevented the spread of something for which a German word is used in other languages, too: Angst .',\n",
       " 'A diffuse anxiety is gaining ground.',\n",
       " 'People feel uneasy and worried, especially when traveling.',\n",
       " 'Any train accident or airplane crash is now at first suspected of being an act of terrorism.',\n",
       " 'Thus, 9/11 has meant, directly or indirectly, a great shock, both psychologically and to our political systems.',\n",
       " 'While terrorism is fought in the name of democracy, the fight has in fact led to a distinct weakening of democracy, owing to official legislation and popular angst.',\n",
       " 'One of the worrying features of the 9/11 attacks is that it is hard to see their purpose beyond the perpetrators’ resentment of the West and its ways.',\n",
       " 'But the West’s key features, democracy and the rule of law, have taken a far more severe battering at the hands of their defenders than by their attackers.',\n",
       " 'Two steps, above all, are needed to restore confidence in liberty within the democracies affected by the legacy of 9/11.',\n",
       " 'First, we must make certain that the relevant legislation to meet the challenge of terrorism is strictly temporary.',\n",
       " 'Some of today’s restrictions on habeas corpus and civil liberties have sunset clauses restricting their validity; all such rules should be re-examined by parliaments regularly.',\n",
       " 'Second, and more importantly, our leaders must seek to calm, rather than exploit, public anxiety.',\n",
       " 'The terrorists with whom we are currently at “war” cannot win, because their dark vision will never gain broad popular legitimacy.',\n",
       " 'That is all the more reason for democrats to stand tall in defending our values – first and foremost by acting in accordance with them.',\n",
       " '9/11 in Perspective',\n",
       " 'NEW YORK – It was a decade ago that 19 terrorists took control of four planes, flew two into the twin towers of the World Trade Center, hit the Pentagon with a third, and crashed the fourth in a field in Pennsylvania after passengers resisted and made it impossible for the terrorists to complete their malevolent mission.',\n",
       " 'In a matter of hours, more than 3,000 innocent people, mostly Americans, but also people from 115 other countries, had their lives suddenly and violently taken from them.',\n",
       " 'September 11, 2001, was a terrible tragedy by any measure, but it was not a historical turning point.',\n",
       " 'It did not herald a new era of international relations in which terrorists with a global agenda prevailed, or in which such spectacular terrorist attacks became commonplace. On the contrary, 9/11 has not been replicated.',\n",
       " 'Despite the attention devoted to the “Global War on Terrorism,” the most important developments of the last ten years have been the introduction and spread of innovative information technologies, globalization, the wars in Iraq and Afghanistan, and the political upheavals in the Middle East.?',\n",
       " 'As for the future, it is much more likely to be defined by the United States’ need to put its economic house in order; China’s trajectory within and beyond its borders; and the ability of the world’s governments to cooperate on restoring economic growth, stemming the spread of nuclear weapons, and meeting energy and environmental challenges.',\n",
       " 'It is and would be wrong to make opposition to terrorism the centerpiece of what responsible governments do in the world.',\n",
       " 'Terrorists continue to be outliers with limited appeal at best.',\n",
       " 'They can destroy but not create.',\n",
       " 'It is worth noting that the people who went into the streets of Cairo and Damascus calling for change were not shouting the slogans of Al Qaeda or supporting its agenda.',\n",
       " 'Moreover, measures have been implemented to push back, successfully, against terrorists.',\n",
       " 'Intelligence assets have been redirected. Borders have been made more secure and societies more resilient.',\n",
       " 'International cooperation has increased markedly, in part because governments that cannot agree on many things can agree on the need to cooperate in this area.',\n",
       " 'Military force has played a role as well.',\n",
       " 'Al Qaeda lost its base in Afghanistan when the Taliban government that had provided it sanctuary was ousted from power.',\n",
       " 'Osama bin-Laden was finally found and killed by US Special Forces in the suburbs of Islamabad.',\n",
       " 'Drones – unmanned aircraft that are remotely steered – have proven to be effective in killing a significant number of terrorists, including many of the most important leaders.',\n",
       " 'Weak governments can be made stronger; governments that tolerate or support terrorism must be held accountable.',\n",
       " 'But progress is not to be confused with victory.',\n",
       " 'Terrorists and terrorism cannot be eliminated any more than we can rid the world of disease.',\n",
       " 'There will always be those who will resort to force against innocent men, women, and children in pursuit of political goals.',\n",
       " 'Indeed, terrorists are advancing in some areas.',\n",
       " 'Pakistan remains a sanctuary for Al Qaeda and some of the world’s other most dangerous terrorists.',\n",
       " 'A mixture of instability, government weakness, and ideology in countries such as Yemen, Libya, Somalia, and Nigeria are providing fertile territory for terrorists to organize, train, and mount operations – much as they did in Afghanistan did a decade ago.',\n",
       " 'New groups constantly emerge from the ruins of old ones.',\n",
       " 'There is also a growing danger of homegrown terrorism.',\n",
       " 'We have seen it in Great Britain and the US.',\n",
       " 'The Internet, one of the great inventions of the modern Western world, has shown itself to be a weapon that can be used to incite and train those who wish to cause harm to that world.',\n",
       " 'The question raised in October 2003 by then US Secretary of Defense Donald Rumsfeld is no less relevant today: “Are we capturing, killing, or deterring and dissuading more terrorists every day than the madrassas and the radical clerics are recruiting, training, and deploying against us?”',\n",
       " 'All things being equal, we probably are.',\n",
       " 'But even small terrorist successes are costly in terms of lives, money, and making open societies less so.',\n",
       " 'What is to be done?',\n",
       " 'Alas, there is no single or silver bullet.',\n",
       " 'The establishment of a Palestinian state will not be enough for those terrorists who want to see the elimination of the Jewish state, any more than reaching a compromise over Kashmir will satisfy those Pakistan-based terrorists with bigger agendas vis-à-vis India.',\n",
       " 'Reducing unemployment is desirable, of course, but many terrorists do not come from poverty.',\n",
       " 'Helping to make societies in the Middle East and elsewhere more democratic might reduce the alienation that can lead to radicalism and worse, but this is easier said than done.',\n",
       " 'Of course, we want to continue to find ways to make ourselves less vulnerable and terrorists more so.',\n",
       " 'But what may be most important, particularly in the Arab and Islamic communities, is to end any acceptance of terrorism.',\n",
       " 'The Nigerian father who warned the US embassy in Lagos that he feared what his own son might do – before that same young man attempted to detonate a bomb aboard a flight to Detroit on Christmas Day 2009 – is an example of just this.',\n",
       " 'Only when more parents, teachers, and community leaders behave likewise will recruitment of terrorists dry up and law-enforcement authorities receive full cooperation from the populations they police.',\n",
       " 'Terrorism must lose its legitimacy among those who have historically supported or tolerated it before it will lose its potency.',\n",
       " 'Transatlantic Trade for All',\n",
       " 'WASHINGTON, DC – The negotiations to create a Transatlantic Trade and Investment Partnership between the European Union and the United States are being widely welcomed.',\n",
       " 'British Prime Minister David Cameron has called the TTIP a “once-in-a-generation prize,” citing potential gains of ￡80 billion ($125.5 billion) each for the EU and the US and ￡85 billion for the rest of the world.',\n",
       " 'For a world weary of waiting for the World Trade Organization’s interminable Doha trade round to conclude, even a bilateral trade initiative may seem like a boon, especially when, as a recent Financial Times editorial pointed out, “bilateral” covers half of the world’s economy.',\n",
       " 'But there is a serious downside: The deal could hurt developing-country exporters, unless the EU and the US make a concerted effort to protect these actors’ interests.',\n",
       " 'The feature of the proposed pact that elicits the most excitement – its focus on regulatory barriers like mandatory product standards – should actually incite the greatest concern.',\n",
       " 'Given low tariffs in the EU and the US – less than 5%, on average – further preferential reductions will not seriously handicap outsiders.',\n",
       " 'But, when it comes to standards – such as those governing safety, health, and the environment – the market-access requirements are brutal and binary: either you meet the established standard or you do not sell.',\n",
       " 'As a result, third-country firms’ options will depend on how TTIP standards are established: through harmonization (adoption of a common standard) or mutual recognition (acceptance of goods that meet one another’s established standards).',\n",
       " 'The first option would enable producers everywhere to take advantage of economies of scale.',\n",
       " 'But, in some cases, the harmonized standard could be more stringent than some countries’ original standards.',\n",
       " 'Even though new standards would apply to suppliers from all exporting countries, compliance costs usually vary, meaning that those less equipped to meet higher standards could suffer.',\n",
       " 'In the late 1990’s, when the EU decided to harmonize standards for aflatoxins (a group of toxic compounds produced by certain molds), eight member states – including Italy, the Netherlands, and Spain – raised their national standards substantially, which is likely to have caused African exports of cereals, dried fruits, and nuts to Europe to decline by as much as $670 million.',\n",
       " 'With mutual recognition, the EU and the US would accept each other’s standards or conformity-assessment procedures, allowing firms to adhere to the less stringent requirements in each area.',\n",
       " 'If the policy were extended to third-country firms, it would have a powerful liberalizing impact.',\n",
       " 'For example, Malaysian television producers could choose to comply with, say, America’s easier-to-meet safety standards, then sell the same product in both markets, reaping the benefits of economies of scale while lowering compliance costs.',\n",
       " 'If, however, the TTIP excluded third-country firms from the mutual recognition policy, their competitiveness vis-à-vis European and American companies would diminish substantially.',\n",
       " 'Indeed, our research shows that when mutual-recognition agreements include restrictive rules of origin, intra-regional trade increases – at the expense of trade with other countries – and that developing countries tend to suffer most.',\n",
       " 'In fact, excessively constraining rules of origin have proved problematic for some of the EU’s previous recognition agreements, such as those governing professional-services standards.',\n",
       " 'While a Brazilian orange admitted for sale in Portugal can be sold throughout the EU, a Brazilian engineer or accountant licensed in Portugal must fulfill separate licensing requirements to work elsewhere in the EU, hampering much-needed labor mobility by forcing non-European workers to endure costly and inefficient bureaucratic procedures.',\n",
       " 'Furthermore, when it comes to tariffs and standards, WTO rules are not created equal.',\n",
       " 'While they protect countries excluded from bilateral or regional tariff agreements, thereby ensuring that integrated markets do not receive additional advantages, few safeguards exist to shield third countries from the fallout of agreements on mandatory standards.',\n",
       " 'Even in the absence of international rules, the EU and the US could take two actions to ensure that the TTIP does not have adverse consequences for developing economies.',\n",
       " 'First, they could allow all countries to reap the benefits of a bilateral mutual-recognition deal by agreeing not to impose restrictive rules of origin.',\n",
       " 'Second, where they do consider harmonization, they could favor the less stringent of the original standards, unless there is credible evidence that it would not support the relevant regulatory objective.',\n",
       " 'This is akin to a WTO test for departures from established international standards.',\n",
       " 'If the EU and the US made these two commitments, the rest of the world could follow the TTIP negotiations with hope, rather than trepidation.',\n",
       " 'A Balanced Look at Sino-American Imbalances',\n",
       " 'BEIJING – Before July 2007, most economists agreed that global imbalances were the most important threat to global growth.',\n",
       " 'It was argued that the United States’ rising net foreign debt-to-GDP ratio – the result of chronic current-account deficits – would put a sharp brake on capital inflows, in turn weakening the dollar, driving up interest rates, and plunging the US economy into crisis.',\n",
       " 'But this scenario failed to materialize. Instead, the crisis stemmed from the US sub-prime debacle, which quickly dragged the global economy into its deepest recession since the 1930’s.',\n",
       " 'Most economists failed to foresee the economic dynamics that actually led to the crisis, because they failed to pay enough attention to the rapid increase in US total debt.',\n",
       " 'Instead, they focused exclusively on US foreign debt, ignoring household debt (mortgage and consumer debt), public debt, business debt, and financial debt.',\n",
       " 'In particular, they should have paid greater attention to the sustainability of US mortgage and consumer debt.',\n",
       " 'In 2007, the mortgage and consumer debt-to-GDP ratio was more than 90%, compared to 24% for net foreign debt.',\n",
       " 'Of course, the various components of debt differ considerably in their character and sources of financing – and thus in their sustainability.',\n",
       " 'But all parts of a country’s total debt and how it is financed are interconnected.',\n",
       " 'This means two things. First, funds from different sources of finance are interchangeable to a certain degree: deficiency of funds for one component of total debt can be supplemented by surplus funds originally aimed at financing other components.',\n",
       " 'Second, troubles in any single component of total debt will have an impact on all the other components.',\n",
       " 'After the subprime crisis erupted, mortgage and consumer debt was paid down by households either with their savings or by default.',\n",
       " 'The fall in US total debt, and the narrowing of the financing gap between total debt and domestic funds, led to a significant improvement in the US current-account deficit in 2008-2009, disproving US Federal Reserve Board Chairman Ben Bernanke’s claim that the deficit was caused by a global “saving glut.”',\n",
       " 'Indeed, America’s current-account position strengthened despite the dollar’s appreciation in the face of safe-haven demand.',\n",
       " 'Unfortunately, as a result of the private-sector deleveraging and an increase in household savings, the US economy, driven by debt and consumption, slid into recession.',\n",
       " 'To offset the negative impact of private-sector deleveraging on growth, the US government has maintained expansionary fiscal and monetary policies.',\n",
       " 'Now, with household debt sustained on a knife-edge after feverish government intervention, the fiscal position has deteriorated dramatically and the current-account balance has worsened again.',\n",
       " 'Sustainability of public debt has replaced sustainability of private debt as the biggest threat to financial stability, and the focus of debate about the US current account has shifted from the sustainability of foreign debt to the impact of reducing the external deficit on growth and employment.',\n",
       " 'The dilemma facing US policymakers is how to stimulate growth while lowering the level of total debt.',\n",
       " 'The most important way to achieve both objectives is to increase exports by strengthening US competitiveness.',\n",
       " 'But where will increased competitiveness come from?',\n",
       " 'Devaluation of the dollar could improve US competitiveness in the short run, but it is not a solution.',\n",
       " 'Because rapid fiscal deterioration now has investors worrying about capital losses on US government securities, devaluation would make foreigners more hesitant to finance America’s budget deficit.',\n",
       " 'If foreign financing is not forthcoming, yields on US government debt will rise and the US economy will fall back into recession.',\n",
       " 'In the long run, America’s growth pattern must undergo a structural shift from reliance on debt and consumption one based on Americans vaunted capacity for creativity and innovation.',\n",
       " 'Only then will America improve its competitiveness enough to allow the government to reduce both private and public debt to sustainable levels while maintaining a respectable growth rate.',\n",
       " 'But neither improved competitiveness, nor reduction of total debt, can be achieved overnight.',\n",
       " 'In the short run, the US current-account deficit will remain, regardless of which country runs bilateral surpluses.',\n",
       " 'Thus, China’s continued reinvestment of its current-account surplus in US government securities is of utmost important for US growth and financial stability.',\n",
       " 'Given that America benefits mightily from China’s purchases of US government securities, it is difficult to understand why the US government and Congress have been complaining so much about the bilateral current-account deficit.',\n",
       " 'It is also difficult to grasp why China is so reluctant to reduce its bilateral surplus, given meager returns on its massive holdings of US government securities and a sustained risk of large capital losses in the future.',\n",
       " 'The good news is that, following President Hu Jintao’s recent visit to Washington, both America and China have been taking positive steps to resolve their differences over the bilateral current-account balance.',\n",
       " 'That augurs well for a more rational and constructive Sino-American dialogue on global imbalances, which would certainly benefit the global economy.',\n",
       " 'A Banking Union Baby Step',\n",
       " 'BRUSSELS – At the beginning of the financial crisis, it was said that banks were, in Charles Goodhart’s crisp phrase, “international in life, but national in death.”',\n",
       " 'At the time (2008-2009), large international banks had to be rescued by their home countries’ governments when they ran into trouble.',\n",
       " 'But the problem now in Europe is the opposite: banks are “national in life, but European in death.”',\n",
       " 'In Spain, for example, local savings banks (cajas) financed an outsize real-estate boom.',\n",
       " 'As the boom turned to bust, the losses threatened to overwhelm the capacity of the Spanish state, and the problem became European, because it threatened the very survival of the euro.',\n",
       " 'The Spanish case is symptomatic of a larger problem.',\n",
       " 'National supervisors always tend to minimize problems at home.',\n",
       " 'Their instinct (and their bureaucratic interest) is to defend their countries’ “national champion” bank(s) abroad.',\n",
       " 'But their resistance to recognizing problems at home runs even deeper.',\n",
       " 'Until recently, the Spanish authorities maintained that the problems in their country’s real-estate sector were temporary.',\n",
       " 'To acknowledge the truth would have meant admitting that for years they had overlooked the build-up of an unsustainable construction boom that now threatens to bankrupt the entire country.',\n",
       " 'In the case of Ireland, the situation was initially not much different.',\n",
       " 'When problems started to surface, the finance minister at the time initially claimed that the country would carry out “the cheapest bank rescue ever.”',\n",
       " 'Given national supervisors’ predictable tendency not to recognize problems at home, it seemed natural that the cost of cleaning up insolvent banks should also be borne at the national level.',\n",
       " 'It thus seemed to make sense that even in the eurozone, banking supervision remained largely national.',\n",
       " 'The recently created European Banking Authority has only limited powers over national supervisors, whose daily work is guided mainly by national considerations.',\n",
       " 'But reality has shown that this approach is not tenable.',\n",
       " 'Problems might originate at the national level, but, owing to monetary union, they quickly threaten the stability of the entire eurozone banking system.',\n",
       " 'At their June summit, Europe’s leaders finally recognized the need to rectify this situation, transferring responsibility for banking supervision in the eurozone to the European Central Bank.',\n",
       " 'Given that financial integration is particularly strong within the monetary union, putting the ECB in charge was an obvious choice.',\n",
       " 'Moreover, the ECB already bears de facto responsibility for the stability of the eurozone’s banking system.',\n",
       " 'But, until now, it had to lend massive amounts to banks without being able to judge their soundness, because all of that information was in the hands of national authorities who guarded it jealously and typically denied problems until it was too late.',\n",
       " 'Putting the ECB in charge should also help to stop the creeping disintegration process, which is not publicly visible, but is very real nonetheless.',\n",
       " 'Just ask any of the large international banking groups headquartered in financially stressed eurozone countries.',\n",
       " 'Consider the case of a bank headquartered in Italy, but with an important subsidiary in Germany.',\n",
       " 'The German operations naturally generate a surplus of funds (given that savings in Germany far exceed investment on average).',\n",
       " 'The parent bank would like to use these funds to reinforce the group’s liquidity.',\n",
       " 'But the German supervisory authorities consider Italy at risk and thus oppose any transfer of funds there.',\n",
       " 'The supervisor of the home country (Italy) has the opposite interest.',\n",
       " 'It would like to see the “internal capital market” operate as much as possible.',\n",
       " 'Here, too, it makes sense to have the ECB in charge as a neutral arbiter with respect to these opposing interests.',\n",
       " 'But, while putting the ECB in charge of banking supervision solves one problem, it creates another: can national authorities still be held responsible for saving banks that they no longer supervise?',\n",
       " 'Economic (and political) logic requires that the eurozone will soon also need a common bank rescue fund.',\n",
       " 'Officially, this has not yet been acknowledged.',\n",
       " 'But that is often the way that European integration proceeds: an incomplete step in one area later requires further steps in related areas.',\n",
       " 'This incremental approach has worked well in the past; indeed, today’s European Union resulted from it.',\n",
       " 'But a financial crisis does not give policymakers the time that they once had to explain to voters why one step required another.',\n",
       " 'They will have to walk much more quickly to save the euro.',\n",
       " 'The Renewed Promise of Abenomics',\n",
       " 'TOKYO – Japan’s Liberal Democratic Party scored a decisive victory in the December 14 parliamentary election, with Japanese voters demonstrating their overwhelming approval of Prime Minister Shinzo Abe’s macroeconomic policy agenda.',\n",
       " 'Though voter turnout was relatively low, owing largely to the somewhat technical nature of the issues, the election’s message was clear: most Japanese abhor the prospect of a return to the grim economic trajectory that prevailed in Japan before “Abenomics.”',\n",
       " \"When the first “arrow” of Abenomics – a fiscal stimulus program – was launched nearly two years ago, asset markets' immediate response was positive.\",\n",
       " 'The second arrow of Abenomics – monetary easing – intensified these effects.',\n",
       " 'In the last two years, Japan’s stock market has almost doubled in value, increasing the wealth of Japanese consumers.',\n",
       " 'Moreover, the yen has fallen by nearly one-third against the US dollar, from around ￥80 to nearly ￥120 per dollar, invigorating Japan’s export industries.',\n",
       " 'Even more encouraging are developments in the labor market, which, unlike those in asset markets, reflect outcomes, not expectations.',\n",
       " 'Here, too, the news is good.',\n",
       " 'The labor market has tightened, with unemployment standing at 3.5% and the job-to-applicant ratio above parity.',\n",
       " 'To be sure, there have been some setbacks: Japan’s GDP shrank in the second and third quarters of 2014.',\n",
       " 'But the downturn, which resulted from April’s consumption-tax hike – from 5% to 8% – cannot be blamed on Abenomics.',\n",
       " 'Indeed, Abe was honoring a law enacted by the previous government, led by the Democratic Party of Japan.',\n",
       " 'The first two arrows of Abenomics were aimed at stimulating demand – and they were extremely effective.',\n",
       " 'The consumption-tax hike was needed to sustain them in flight. Unfortunately, the hike was too large to keep them aloft.',\n",
       " 'The good news is that the tax hike’s impact is temporary. Soon, it will begin to taper off, and industrial output will approach full capacity.',\n",
       " 'When demand begins to exceed supply, demand-side stimulus policies will become increasingly ineffective, and it will be time to launch the third arrow of Abenomics: growth-enhancing structural reforms.',\n",
       " 'Such reforms are essential to raise productivity growth and improve the Japanese economy’s competitiveness.',\n",
       " 'Four imperatives stand out.',\n",
       " 'The first task should be to eliminate – or, at least, reduce – the thicket of government regulations that is stifling economic dynamism.',\n",
       " 'The current system is so convoluted and complex that it took more than three decades to open a new medical school in Tokyo.',\n",
       " 'Likewise, flights to Haneda airport, a convenient connection to the Tokyo city area, have been rationed.',\n",
       " 'This is no formula for long-term economic success.',\n",
       " 'Furthermore, Japan’s government should push to complete negotiations for the Trans-Pacific Partnership, which is currently being negotiated among 12 countries, from Mexico to the United States to Vietnam.',\n",
       " 'The TPP would improve Japan’s trade prospects considerably, including in sensitive sectors like agriculture, where exports of fast-moving consumer goods like flowers and vegetables would benefit.',\n",
       " 'Japan’s leaders must also work to expand the workforce, which faces severe constraints, owing largely to the country’s rapidly aging population.',\n",
       " 'In the absence of large-scale immigration, to which Japanese remain unamenable, one relatively simple solution would be to integrate more women into the labor force.',\n",
       " 'A 10% increase in Japan’s female labor-force participation rate – an entirely attainable goal – would translate into an almost 5% gain in total labor-force participation.',\n",
       " 'Finally, Abe’s government must reduce the corporate-tax rate to align it more closely with international standards.',\n",
       " 'Amid increasingly intense international competition to attract foreign investment, reducing the corporate tax would actually increase Japan’s tax revenues, by spurring companies to invest their vast cash stockpiles in more productive activities.',\n",
       " 'Now that Abe’s government has a renewed mandate from Japanese voters, it must deliver on its promises – and that means decisive and comprehensive implementation of structural reforms.',\n",
       " 'Of course, this will require some sacrifices.',\n",
       " 'Indeed, households have already endured some hardship, brought about by the consumption-tax hike.',\n",
       " 'The next step is for Abe’s government to use its political capital to overcome vested interests, both in the bureaucracy and the business community.',\n",
       " 'This means compelling businesses to give up some of the special tax benefits they now enjoy.',\n",
       " 'For their part, politicians must participate in the taxpayer identification system.',\n",
       " 'And bureaucrats must forego some of the power that excessive regulation affords them.',\n",
       " 'If all of these groups join the Japanese public in accepting reasonable sacrifices, Abe’s government can fulfill its promise and build a thriving economy.',\n",
       " 'For the sake of all Japanese – not to mention a world economy in need of a new source of dynamism – that promise deserves to be met.',\n",
       " 'The Missing Arrow of Abenomics',\n",
       " 'TOKYO – In his drive to kick-start the Japanese economy, Prime Minister Shinzo Abe, shortly after taking office in 2012, introduced a large fiscal stimulus and put in place a bold program of monetary easing.',\n",
       " 'Since then, Japanese policymakers have been working to launch what Abe calls the third “arrow” of his agenda: arduous reforms of key industries and the demolition of structural barriers to growth.',\n",
       " 'But the focus on public policy has left a “fourth arrow” – the private sector – untouched and seemingly ignored.',\n",
       " 'This is unfortunate, because the government cannot fix Japan’s ills on its own.',\n",
       " 'Annual productivity growth has been stubbornly sluggish, rarely rising above 2% for much of the past two decades, reflecting both missed opportunities and declining cost competitiveness.',\n",
       " 'Japan’s productivity slump permeates the entire economy; labor and capital productivity gains have nearly stalled in almost every sector – even in Japan’s signature advanced manufacturing industries.',\n",
       " 'Labor productivity in the transport-equipment sector, for example, is barely half that of Germany.',\n",
       " 'This trend puts annual GDP growth on course to average only 1.3% through 2025, implying a third consecutive decade of stagnation.',\n",
       " 'Such an outcome would coincide with – and exacerbate the effects of – an adverse demographic shift that will constrain fiscal revenues and drive up costs for universal health care and pension benefits.',\n",
       " 'Japan’s ability to alter its trajectory depends on individual companies making decisions to invest, change workplace policies, deploy new technologies, and test untried business models.',\n",
       " 'Abe’s structural reforms will take time and political will to enact, but Japanese companies cannot afford to sit still.',\n",
       " 'They can and must act, without waiting for the government to change its policies.',\n",
       " 'In many cases, the economy’s bottlenecks are not regulatory in nature, but stem from entrenched ways of doing business.',\n",
       " 'New research by the McKinsey Global Institute examines Japan’s advanced manufacturing, retail, financial services, and health-care industries in detail – and finds substantial untapped productivity potential in every area.',\n",
       " 'For starters, Japanese firms must become more globally integrated.',\n",
       " 'Exporting to the fastest-growing overseas markets is one obvious route to overcoming sluggish demand growth at home.',\n",
       " 'But, rather than just selling products abroad, Japanese enterprises need to expand operations beyond their borders and cast a wider net for international talent.',\n",
       " 'Japanese companies have formidable R&amp;D operations, but most will need to reconfigure them to obtain better returns and impact.',\n",
       " 'The process must start with an understanding of what the customer wants and a determination to deliver solutions accordingly.',\n",
       " 'Closed and tightly managed R&amp;D operations must be transformed into more fluid, open processes involving collaboration with customers and suppliers.',\n",
       " 'Japanese companies will also need to improve their capabilities in areas such as marketing, pricing, and talent development.',\n",
       " 'While there are some pockets of excellence, most Japanese firms are severely lacking in these areas.',\n",
       " 'To compete in global markets, they will need to achieve the same consistency in these areas that they have in their traditional areas of strength.',\n",
       " 'Many Japanese companies have yet to digitize paper-based processes and replace outdated information-technology systems.',\n",
       " 'Others would benefit from moving beyond basic digitization to next-generation technologies, such as big-data analytics.',\n",
       " 'Companies can also head off looming labor shortages with intelligent software systems and robotics.',\n",
       " 'Manufacturers can augment or replace their assembly lines with technologies such as the Internet of Things and 3D printing.',\n",
       " 'More broadly, Japanese companies have to organize for performance and discipline.',\n",
       " 'As policy changes unleash market forces, businesses will face greater competition.',\n",
       " 'Some may need to reorganize or exit unprofitable markets; others may have to undertake mergers and acquisitions to achieve economies of scale.',\n",
       " 'Finally, shareholders and senior executives should tie performance goals to incentives.',\n",
       " 'Some of Japan’s corporate giants have already begun shifting from traditional seniority-based advancement toward merit-based pay structures.',\n",
       " 'Others should follow their lead.',\n",
       " 'Promoting younger and more diverse talent can create agile organizations with fresh ideas.',\n",
       " 'If Japan’s private sector rises to the challenge, it can move the economy onto a path of faster growth.',\n",
       " 'Innovations in one company would cascade across its entire industry by forcing competitors to raise their game.',\n",
       " 'In the 1950s and 1960s, for example, Toyota introduced more efficient production processes that were eventually adopted by the entire automobile industry.',\n",
       " 'Instead of settling for a future of 1.3% annual GDP growth, Japan could attain roughly 3% annual growth through 2025.',\n",
       " 'Doing so would require the growth rate of labor productivity to more than double, but this is an attainable goal.',\n",
       " 'More than half of this growth increment can be met by adopting best practices that companies around the world already use, while technology can close much of the remaining gap.',\n",
       " 'Japanese business leaders need to combine big thinking with a focused attention to detail.',\n",
       " 'They will need to create innovative products, penetrate new markets, and make bold investments in equipment, technology, and talent, while simultaneously scrutinizing every aspect of their operations for inefficiency and waste.',\n",
       " 'Traditional ways of doing business may have to be abandoned.',\n",
       " 'But there is ample scope to make progress and spur faster economic growth.',\n",
       " 'Immense trade flows, the rise of billions of new consumers in the emerging world, and technology breakthroughs are rapidly transforming the global economy.',\n",
       " 'Japan can shift its current trajectory by turning this wave of disruption into opportunity.',\n",
       " 'From Russia With Unrequited Love',\n",
       " 'NEW DELHI – Japanese Prime Minister Shinzo Abe has assiduously courted Russian President Vladimir Putin, meeting with him more than a dozen times in four years.',\n",
       " 'This month he hosted Putin in Tokyo and in his hometown of Nagato (famed for its onsen, or natural hot springs).',\n",
       " 'But Abe’s courtship has so far yielded little for Japan, and much for Russia.',\n",
       " 'Abe’s diplomatic overtures to Putin are integral to his broader strategy to position Japan as a counterweight to China, and to rebalance power in Asia, where Japan, Russia, China, and India form a strategic quadrangle.',\n",
       " 'Abe has already built a close relationship with India, and he sees improved relations with Russia – with which Japan never formally made peace after World War II – as the missing ingredient for a regional power equilibrium.',\n",
       " 'But Abe’s trust-building efforts with Russia are not aimed only at checking Chinese aggression.',\n",
       " 'He also wants Russia to return its southernmost Kuril Islands – a resource-rich area known as the Northern Territories in Japan – which the Soviet Union seized just after the United States dropped nuclear bombs on Hiroshima and Nagasaki in August 1945.',\n",
       " 'In exchange, Abe has offered economic aid, investments in Russia’s neglected Far East, and major energy deals.',\n",
       " 'Abe has, however, encountered several obstacles.',\n",
       " 'For starters, Japan is a participant in the US-led sanctions that were imposed on Russia after it annexed Crimea in March 2014.',\n",
       " 'These sanctions have pushed Russia closer to its traditional rival, China; and Putin has publicly identified the sanctions as a hindrance to concluding a peace treaty with Japan.',\n",
       " 'In response to Abe’s overtures, Putin has doggedly tried to drive a hard bargain.',\n",
       " 'Against this backdrop, it is not surprising that Abe left the recent “onsen summit” with dashed hopes of resolving the territorial dispute, while Putin returned home with 68 new commercial accords.',\n",
       " 'Many of the new agreements are symbolic, but some are substantive, including deals worth $2.5 billion and an agreement to set up a $1 billion bilateral-investment fund.',\n",
       " 'Under the latter agreement, Japan and Russia are supposed create a “special framework” for joint economic activities on the disputed islands.',\n",
       " 'But the plan has already run into trouble.',\n",
       " 'Peter Shelakhaev, a senior Russian official who leads the government’s Far East Investment and Export Agency, has indicated that there are legal hurdles to establishing such a framework, and that Japanese firms doing business on the Kurils would have to pay taxes to Russia.',\n",
       " 'If Japan did that, however, it would effectively be recognizing Russia’s jurisdiction over the islands.',\n",
       " 'Abe has thus been denied the legacy that he sought, while Putin has succeeded in easing Russia’s international isolation.',\n",
       " 'Abe was the first G7 leader to hold a summit with Putin after Russia annexed Crimea, and now Russia has won Japan’s economic cooperation, too.',\n",
       " 'Japan is the only G7 country that has a territorial dispute with Russia, and it is clearly more eager to reach a deal than the Kremlin is.',\n",
       " 'But this has only strengthened Russia’s hand.',\n",
       " 'While Japan has softened its position, and signaled that it may accept only a partial return of the islands, Russia has grown only more intransigent.',\n",
       " 'After the recent summit, Abe revealed that Putin now seems to be reneging on a 1956 agreement between Japan and the Soviet Union, which stipulates that the smaller two of the four islands will be returned to Japan after a peace treaty is signed.',\n",
       " 'As it happens, this year marks the 60th anniversary of that joint declaration, which was widely viewed as a breakthrough at the time.',\n",
       " 'The Kremlin is now suggesting that its commitment to fulfilling the declaration was conditional on Japan not joining any security alliance against Russia.',\n",
       " 'And Putin has expressed concerns that the 1960 Japan-US Security Treaty would extend to the disputed islands if they were returned, thus allowing the US to establish a military presence there.',\n",
       " 'Japan is in no position to address Russia’s concerns.',\n",
       " 'It cannot opt out of the US-led sanctions regime; and it cannot exempt the disputed Kurils from its security treaty with the US, especially now that it has been urging the US to provide an explicit commitment to defend the Japanese-controlled Senkaku Islands, over which China claims sovereignty.',\n",
       " 'Putin, for his part, appears smugly content with his negotiating position.',\n",
       " 'Not only did he arrive almost three hours late to the onsen summit, in keeping with his habit of leaving foreign leaders waiting; he also declined a Japanese government gift – a male companion for his native Japanese Akita dog, which Japan gave him in 2012.',\n",
       " 'There is little hope now that Abe will see tangible returns on the political capital he has invested in cultivating Putin.',\n",
       " 'And Japan’s dilemma will only deepen.',\n",
       " 'US President-elect Donald Trump’s desire to improve relations with Russia may give Abe leeway to continue wooing Putin; but if Russia gets the US in its corner, it won’t need Japan anymore.',\n",
       " 'A Berlin Consensus?',\n",
       " 'HONG KONG – A recent trip to Berlin brought back memories of an earlier visit in the summer of 1967, when I was a poor student who marveled at the Wall that would divide and devastate an entire society for another two decades.',\n",
       " \"Berlin today is vibrant and rejuvenated, rebuilt by the German peoples' hard work and sacrifice to unify the country, and an apt setting for the conference of the Institute for New Economic Thinking (INET), which I was there to attend.\",\n",
       " 'The conference’s theme was “Paradigm Lost,” with more than 300 economists, political scientists, systems analysts, and ecologists gathering to rethink economic and political theory for the challenges and uncertainty posed by growing inequality, rising unemployment, global financial disarray, and climate change.',\n",
       " 'Almost everyone agreed that the old paradigm of neoclassical economics was broken, but there was no agreement on what can replace it.',\n",
       " 'Nobel laureate Amartya Sen attributed the European crisis to four failures – political, economic, social, and intellectual.',\n",
       " 'The global financial crisis, which began in 2007 as a crisis of US subprime lending and has broadened into a European sovereign-debt (and banking) crisis, has raised questions that we cannot answer, owing to over-specialization and fragmentation of knowledge.',\n",
       " 'And yet there is no denying that the world has become too intricate for any simple, overarching theory to explain complex economic, technological, demographic, and environmental shifts.',\n",
       " 'In particular, the rise of emerging markets has challenged traditional Western deductive and inductive logic.',\n",
       " 'Deductive inference enables us to predict effects if we know the principles (the rule) and the cause.',\n",
       " 'By inductive reasoning, if we know the cause and effects, we can infer the principles.',\n",
       " 'Eastern thinking, by contrast, has been abductive, moving from pragmatism to guessing the next steps.',\n",
       " 'Abductive inference is pragmatic, looking only at outcomes, guessing at the rule, and identifying the cause.',\n",
       " 'Like history, social-scientific theory is written by the victors and shaped by the context and challenges of its time.',\n",
       " 'Free-market thinking evolved from Anglo-Saxon theorists (many from Scotland), who migrated and colonized territories, allowing fortunate individuals to assume that there were no limits to consumption.',\n",
       " 'European continental thinking, responding to urbanization and the need for social order, emphasized institutional analysis of political economy.',\n",
       " 'Thus, the emergence of neoclassical economics in the nineteenth century was very much influenced by Newtonian and Cartesian physics, moving from qualitative analysis to quantifying human behavior by assuming rational behavior and excluding uncertainty.',\n",
       " 'This “predetermined equilibrium” thinking – reflected in the view that markets always self-correct – led to policy paralysis until the Great Depression, when John Maynard Keynes’s argument for government intervention to address unemployment and output gaps gained traction.',\n",
       " 'By the 1970’s, the neoclassical general-equilibrium school captured Keynesian economics through real-sector models that assumed that “finance is a veil,” thereby becoming blind to financial markets’ destabilizing effects.',\n",
       " 'Economists like Hyman Minsky, who tried to correct this, were largely ignored as Milton Friedman and others led the profession’s push for free markets and minimal government intervention.',\n",
       " 'But then technology, demographics, and globalization brought dramatic new challenges that the neoclassical approach could not foresee.',\n",
       " 'Even as the world’s advanced countries over-consumed through leveraging from derivative finance, four billion of the world’s seven billion people began moving to middle-income status, making huge demands on global resources and raising the issue of ecological sustainability.',\n",
       " 'New thinking is required to manage these massive and systemic changes, as well as the integration of giants like China and India into the modern world.',\n",
       " 'A change of mindset is needed not just in the West, but also in the East.',\n",
       " 'In 1987, the historian Ray Huang explained it for China:',\n",
       " '“As the world enters the modern era, most countries under internal and external pressure need to reconstruct themselves by substituting the mode of governance rooted in agrarian experience with a new set of rules based on commerce.…This is easier said than done.',\n",
       " 'The renewal process could affect the top and bottom layers, and inevitably it is necessary to recondition the institutional links between them.',\n",
       " 'Comprehensive destruction is often the order; and it may take decades to bring the work to completion.”',\n",
       " 'Using this macro-historical framework, we can see Japanese deflation, European debt, and even the Arab Spring as phases of systemic changes within complex structures that are interacting with one another in a new, multipolar global system.',\n",
       " 'We are witnessing simultaneous global convergence (the narrowing of income, wealth, and knowledge gaps between countries) and local divergence (widening income, wealth, and knowledge gaps within countries).',\n",
       " 'Adaptive systems struggle with order and creativity as they evolve.',\n",
       " 'As the philosopher Bertrand Russell presciently put it: “Security and justice require centralized governmental control, which must extend to the creation of a world government if it is to be effective.',\n",
       " 'Progress, on the contrary, requires the utmost scope for personal initiative that is compatible with social order.”',\n",
       " 'A new wave of what the economist Joseph Schumpeter famously called “creative destruction” is under way: even as central banks struggle to maintain stability by flooding markets with liquidity, credit to business and households is shrinking.',\n",
       " 'We live in an age of simultaneous fear of inflation and deflation; of unprecedented prosperity amid growing inequality; and of technological advancement and resource depletion.',\n",
       " 'Meanwhile, existing political systems promise good jobs, sound governance, a sustainable environment, and social harmony without sacrifice – a paradise of self-interested free riders that can be sustained only by sacrificing the natural environment and the welfare of future generations.',\n",
       " 'We cannot postpone the pain of adjustment forever by printing money.',\n",
       " 'Sustainability can be achieved only when the haves become willing to sacrifice for the have-nots.',\n",
       " 'The Washington Consensus of free-market reforms for developing countries ended more than two decades ago.',\n",
       " 'The INET conference in Berlin showed the need for a new one – a consensus that supports sacrifice in the interest of unity.',\n",
       " 'Europe could use it.',\n",
       " 'Accepting Japan at Its Word',\n",
       " 'TOKYO – In recent years, the number of tourists visiting Japan has been increasing rapidly, reaching a record 13.4 million last year, a 29% increase from 2013.',\n",
       " 'Japan seems to be making great strides toward its goal of recapturing the position as an Asian cultural center that it held a century ago, when the Indian Nobel laureate poet Rabindranath Tagore lived in Tokyo.',\n",
       " 'Chinese revolutionary leaders Sun Yat-sen and Chiang Kai-shek, along with many other prominent Asians, moved there as well.',\n",
       " 'Anyone visiting Japan today would do well to learn two key words: domo, meaning “hello,” “thanks,” or “well,” and sumimasen, which can carry any of the meanings of domo, as well as “sorry” or “excuse me.”',\n",
       " 'Ordinary Japanese say sumimasen countless times each day, to apologize to friends or strangers for even the most trivial accident or mistake.',\n",
       " 'But, as Japan’s leaders have experienced firsthand since World War II, expressing regret to other countries is not so simple.',\n",
       " 'Yet that is precisely what Prime Minister Shinzo Abe must do in his upcoming statement marking the 70th anniversary of the end of the war.',\n",
       " 'The statement will be based on consultations with many of Japan’s, and the world’s, leading WWII historians, as well as – and more important – with himself, his conscience, and his heart, because he understands the significance of his words on this highly fraught topic.',\n",
       " 'Of course, Abe is far from the first Japanese leader to confront this challenge.',\n",
       " 'His statement will follow a long line of declarations by prime ministers and chief cabinet secretaries expressing sincere remorse over the events of WWII.',\n",
       " 'Twenty years ago, Prime Minister Tomiichi Murayama, the head of the Socialist Party, acknowledged that “Japan, through its colonial rule and aggression, caused tremendous damage and suffering to the people of many countries,” particularly in Asia.',\n",
       " 'He went on to express “feelings of deep remorse” and offer a “heartfelt apology” to the victims.',\n",
       " 'Ten years later, Prime Minister Junichiro Koizumi reiterated Murayama’s words, adding that since the war, Japan had been “manifesting its remorse for the war through actions,” especially development assistance and humanitarian activities.',\n",
       " 'Koizumi also pledged that “Japan, as a peace-loving nation, will work to achieve peace and prosperity for all humankind with all its resources.”',\n",
       " 'Despite these straightforward declarations of regret, some governments and citizens continue to demand more, giving the impression that nothing a Japanese leader says or does will convince them of the country’s remorse.',\n",
       " 'This intractability is, in some cases, understandable; the pain of survivors and their descendants remains acute.',\n",
       " 'But in many other cases, the unwillingness to move beyond history is driven by political interests.',\n",
       " 'Indeed, political motivations are behind claims that Abe does not agree with past official apologies, despite his repeated assurances that he does, as well as suggestions that he is seeking to revise history, even though he has never denied Japan’s colonial aggression.',\n",
       " 'Moreover, some have produced portrayals of Japan, as a whole, as an unrepentant country – or, worse, as one that is hell-bent on remilitarization.',\n",
       " 'Such depictions are breathtaking in their audacity, given Japan’s seven-decade record as a peaceful and constructive member of the international community.',\n",
       " 'This is not lost on those in Japan who ask for how long their country will have to apologize, with some even suggesting that after 70 years, a “tweet” on the subject should amount to adequate acknowledgement by Abe.',\n",
       " 'The prime minister, however, remains committed to issuing a strong and sincere statement on the subject.',\n",
       " 'Early this year, Abe announced his intention to use the 70th anniversary statement to communicate Japan’s remorse for the war, describe the progress the country has made in upholding peace, and describe the contributions that Japan can make to Asia and the rest of the world in the coming decades.',\n",
       " 'In fact, it is the third component of the announcement that inspires fear in some observers: By helping to build a strong security architecture in the Asia-Pacific region, Japan could undermine the ability of some actors to advance their own interests.',\n",
       " 'That is why they launched a whisper campaign against Abe’s statement months before he even began to write it.',\n",
       " 'But, of course, Asian security and prosperity is in everyone’s interest.',\n",
       " 'Given this, not even the language of Abe’s statement is particularly important; what matters is the determination he expresses, and the actions he takes to follow through – with appropriate humility – on his pledges.',\n",
       " 'And it seems that Abe is, indeed, determined to make real contributions to peace, based on effective cooperation with Japan’s friends and allies.',\n",
       " 'But if Asia is to move beyond its past, the victims of Japan’s wartime aggression must recognize that the Japan of 2015 is not the Japan of 1931, 1941, or even 1945, and that, as many Asian leaders have realized over the years, forgiveness benefits everyone.',\n",
       " 'In 1998, South Korean President Kim Dae-jung responded positively to a statement by former Japanese Prime Minister Keizo Obuchi.',\n",
       " 'The governments of Indonesia, the Philippines, Vietnam, and other countries have done the same, and now welcome Japan’s commitment to act with its allies to protect regional security.',\n",
       " 'These countries’ openness to reconciliation have enabled Japan to recast itself as a key arbiter of regional peace and prosperity, not to mention an increasingly dynamic cultural hub.',\n",
       " 'It is time for the rest of the region to follow suit, accepting at face value Japan’s sincere apologies and working with the country to build a better future.',\n",
       " 'At a time when Asia is facing serious security challenges, this stance could not be more urgent.',\n",
       " 'A Big Chance for Small Farmers',\n",
       " 'NEW YORK – The G-8’s $20 billion initiative on smallholder agriculture, launched at the group’s recent summit in L’Aquila, Italy, is a potentially historic breakthrough in the fight against hunger and extreme poverty.',\n",
       " 'With serious management of the new funds, food production in Africa will soar.',\n",
       " 'Indeed, the new initiative, combined with others in health, education, and infrastructure, could be the greatest step so far toward achieving the Millennium Development Goals, the internationally agreed effort to reduce extreme poverty, disease, and hunger by half by 2015 .',\n",
       " 'During 2002-2006, I led the United Nations Millennium Project, which aimed to achieve the Millennium Development Goals, for then-UN Secretary General Kofi Annan.',\n",
       " 'One cornerstone of the project was “smallholder farmers,” meaning peasant farm families in Africa, Latin America, and Asia – working farms of around one hectare (2.5 acres) or less.',\n",
       " 'These are some of the poorest households in the world, and, ironically, some of the hungriest as well, despite being food producers.',\n",
       " 'They are hungry because they lack the ability to buy high-yield seeds, fertilizer, irrigation equipment, and other tools needed to increase productivity.',\n",
       " 'As a result, their output is meager and insufficient for their subsistence.',\n",
       " 'Their poverty causes low farm productivity, and low farm productivity reinforces their poverty.',\n",
       " 'It’s a vicious circle, technically known as a poverty trap.',\n",
       " 'The UN Millennium Project’s Hunger Task Force, led by two world-leading scientists, M. S. Swaminathan and Pedro Sanchez, examined how to break this vicious circle.',\n",
       " 'The Hunger Task Force determined that Africa could substantially increase its food production if help was given to smallholder farmers, in the form of agricultural inputs.? The Millennium Project recommended a big increase in global funding for this purpose.? Drawing on that work and related scientific findings, Annan launched a call in 2004 for an African Green Revolution, based on an expanded partnership between Africa and donor countries.',\n",
       " 'Many of us, notably current UN Secretary General Ban Ki-moon, have worked hard to make this possible, with Ban repeatedly emphasizing the special emergency arising from the global food, financial, and energy crises of the past two years.',\n",
       " 'The G-8 announcement reflects these years of effort, and of course the boost from the leadership of US President Barack Obama, Spanish Prime Minister Jose Luis Zapatero, Australian Prime Minister Kevin Rudd, World Bank President Robert Zoellick, European Commissioner Louis Michel, European Parliamentarian Thijs Berman, and others.',\n",
       " 'Now the key is to make this effort work.',\n",
       " 'The lessons of history are clear.',\n",
       " 'Getting seed and fertilizer to smallholder farmers at highly subsidized prices (or even free in some cases) will make a lasting difference.',\n",
       " 'Not only will food yields rise in the short term, but farm households will use their higher incomes and better health to accumulate all sorts of assets: cash balances, soil nutrients, farm animals, and their children’s health and education.',\n",
       " 'That boost in assets will, in turn, enable local credit markets, such as micro-finance, to begin operating.',\n",
       " 'Farmers will be able to buy inputs, either out of their own cash, or by borrowing against their improved creditworthiness.',\n",
       " 'A consensus has now been reached on the need to assist smallholders, but obstacles remain.',\n",
       " 'Perhaps the main risk is that the “aid bureaucracies” now trip over each other to try to get their hands on the $20 billion, so that much of it gets taken up by meetings, expert consultations, overhead, reports, and further meetings.',\n",
       " '“Partnerships” of donors can become an expensive end in themselves, merely delaying real action.',\n",
       " 'If donor governments really want results, they should take the money out of the hands of thirty or more separate aid bureaucracies and pool it in one or two places, the most logical being the World Bank in Washington and the International Fund for Agricultural Development (IFAD) in Rome.',\n",
       " 'One or both of these agencies would then have an account with several billion dollars.',\n",
       " 'Governments in hunger-stricken regions, especially Africa, would then submit national action plans that would provide details on how they would use the donor funds to get high-yield seeds, fertilizer, irrigation, farm tools, storage silos, and local advice to impoverished farmers.',\n",
       " 'An independent expert panel would review the national plans to verify their scientific and managerial coherence.',\n",
       " 'Assuming that a plan passes muster, the money to support it would quickly be disbursed.',\n",
       " 'Afterward, each national program would be monitored, audited, and evaluated.',\n",
       " 'This approach is straightforward, efficient, accountable, and scientifically sound.',\n",
       " 'Two major recent success stories in aid have used this approach: the Global Alliance on Vaccines and Immunizations, which successfully gets immunizations to young children, and the Global Fund to Fight AIDS, TB, and Malaria, which supports national action plans to battle these killer diseases.',\n",
       " 'Both have saved millions of lives during the past decade, and have paved the way to a new more efficient and scientifically sound method of development assistance.',\n",
       " 'Not surprisingly, many UN agencies and aid agencies in rich countries fight this approach.',\n",
       " 'All too often, the fight is about turf, rather than about the most effective way to speed help to the poor.',\n",
       " 'Obama, Rudd, Zapatero, and other forward-thinking leaders can therefore make a huge difference by following up on their pledges at the G-8 and insisting that the aid really works.',\n",
       " 'The bureaucracies must be bypassed to get help to where it is needed: in the soil tilled by the world’s poorest farm families.',\n",
       " 'A Black and White Question',\n",
       " 'NEW YORK – In the afternoon of July 16 two men appeared to be breaking into a fine house in an expensive area of Cambridge, Massachusetts.',\n",
       " 'Alerted by a telephone call, a policeman arrived smartly on the scene.',\n",
       " 'He saw one black male standing inside the house and asked him to come out.',\n",
       " 'The man refused.',\n",
       " 'He was then told to identify himself.',\n",
       " 'The man, still refusing to step out, said he was a Harvard professor, showed his ID, and warned the cop not to mess with him.',\n",
       " 'He said something about black men in America being singled out, and asked the cop, who was white, for his name and identification.',\n",
       " 'The cop, joined by several colleagues, arrested the professor for disorderly conduct.',\n",
       " 'We now know that the professor had broken into his own home, with the help of his chauffeur, because the door was jammed.?',\n",
       " 'What was unusual here was not the cop’s heavy-handedness.',\n",
       " 'Most people in the US know that if you talk back to the police, they will get nasty very fast.',\n",
       " 'The fact that the man was black might or might not have made the cop go for his handcuffs even sooner than he might normally have done.',\n",
       " 'That, too, would not have been unusual.',\n",
       " 'What made this case special was that Henry Louis “Skip” Gates is one of the most celebrated professors in the country, famous for his books, his articles, and numerous television appearances.',\n",
       " 'He is a grandee, a mover and shaker in the academic and media world, a friend of President Barack Obama.',\n",
       " 'That is why he warned the cop, Sgt. James Crowley, a veteran of the Cambridge police force, not to mess with him.',\n",
       " 'Class and race overlap in the US.',\n",
       " 'In this instance, it is impossible to pry them apart.',\n",
       " 'Gates, deeply conscious, indeed a specialist of the terrible history of race relations in his country, instinctively assumed that he was a victim of prejudice.',\n",
       " 'From his words it appears that he was equally conscious of not getting the proper respect due to a distinguished Harvard professor and media celebrity.',\n",
       " 'As he put it to his daughter in an interview published online: “[Crowley] should have gotten out of there and said, ‘I’m sorry, sir, good luck.',\n",
       " 'Loved your [television] series—check with you later!’”',\n",
       " 'Alas, Sgt.Crowley had never heard of Professor Gates.',\n",
       " 'A local man whose brothers all serve in the police force, a sports fan, and an amateur basketball coach, Crowley does not move in the same social circles as Gates.',\n",
       " 'As it happens, the charges were duly dropped, and there the case might have rested if President Obama, tired and frustrated after weeks of fighting for his healthcare bill, had not weighed in on behalf of his “friend” Gates, and called the police “stupid.”',\n",
       " 'Both he and Gates then spoke of “learning” from the incident.',\n",
       " 'Gates might even be planning a television documentary on racial profiling.',\n",
       " 'One thing to be learned, if we didn’t know this already, is how close racial sensitivities are to the surface of US life, despite the election of a black president.',\n",
       " 'The complexities of black anger, white guilt, and of black, and white fear, are so vexed that most Americans prefer not to talk about race at all.',\n",
       " 'The field is too full of mines.',\n",
       " 'One of Obama’s great achievements is that he made it into a serious topic through the brilliance and subtlety of his rhetoric.',\n",
       " 'One might argue that it',\n",
       " 'There is, however, a danger that it will have an adverse affect on the necessary national discussion about race.',\n",
       " 'By having made such a big issue out of what was in fact a relatively minor event Gates could be accused of trivializing much worse instances of abuse.',\n",
       " 'Indeed, we don’t even know for certain whether this was such an instance.',\n",
       " 'Crowley never mentioned the color of Gates’ skin. There was no question of violence.',\n",
       " 'There were just very raw nerves and hypersensitivity to hints of disrespect, on the part of the professor, and of the cop.',\n",
       " 'Outrage about a professor who is not to be messed with is not the best way to discuss the plight of countless, poor, anonymous people, whom most of us find it too easy to ignore.',\n",
       " 'A Bollywood Bride for Sarkozy?',\n",
       " 'PARIS &#45;&#45; Ever since French President Nicolas Sarkozy took himself off his country’s most-eligible-bachelor list by publicly acknowledging his affair with supermodel-turned-pop-musician Carla Bruni during a romantic trip to Euro Disney, he’s run into trouble.?',\n",
       " 'His ratings have dipped below 50% for the first time.',\n",
       " 'Older French citizens don’t find the public spectacle of their leader in love very amusing.',\n",
       " 'Abroad, Egyptian lawmakers were so exercised over the prospect of the French head of state sharing a bed with his girlfriend that several vented their disapproval on the floor of the parliament.?',\n",
       " 'Likewise, India is all in a quandary over how to handle protocol during Sarkozy’s impending visit to the subcontinent as the guest of honor at the country’s Republic Day celebrations on January 26.',\n",
       " 'Should the First Girlfriend have her own motorcade, as a first lady would?',\n",
       " 'Meanwhile, the same hard-right Hindu groups that protest Valentine’s Day as a decadent Western holiday have warned that if Sarkozy arrives with his girlfriend in tow, they’ll be out in the streets to welcome him.',\n",
       " 'This controversy has threatened to cast a pall over a much-heralded summit between two of the world’s great democracies.',\n",
       " 'With lucrative deals at stake for the big-ticket products that drive the French economy – military hardware, nuclear power plants, and Airbus planes – France has a strong interest in a successful summit in India.',\n",
       " '“You’ll probably find out after it’s happened,” he taunted.',\n",
       " 'Rumor has it the couple has set February 8 or 9 for the wedding.?? Others say that Sarkozy has already outsmarted the media by secretly marrying in the Elysee Palace, even as he was dodging wedding questions.',\n",
       " 'If that is true, then Sarkozy missed the romantic opportunity of a lifetime.',\n",
       " 'If the couple sizzled for cameras with Luxor and Petra as the backdrop, just imagine how hot things could get at the most romantic spot on Earth, the Taj Mahal.',\n",
       " 'And, given the current rage for all things Bollywood in France, a lavish Indian wedding would be fitting.',\n",
       " 'Bruni’s own life path closely resembles any number of Bollywood stars who have made the transition from model to actress.',\n",
       " 'A comely brunette who sings is perfect for a Bollywood makeover.',\n",
       " 'The Indian government will be nothing if not relieved to see the first girlfriend made a wife.',\n",
       " 'As one of India’s leading daily newspapers, the',\n",
       " 'Despite the sometimes downright pornographic on-screen writhing of Bollywood starlets, India is still a deeply conservative society.',\n",
       " 'Divorce is anathema. (Sarkozy is now twice divorced.)',\n",
       " 'And, while mistresses abound among the privileged classes, they do not strut publicly by their power-mates’ sides.',\n",
       " 'Kissing and fondling in public, even by spouses, is taboo.',\n",
       " 'In this respect, India more resembles the France with which Sarkozy wants to make a clean break than the current one.',\n",
       " 'Most Indians, as it seems many French, would prefer not to shed the “hypocrisy” Sarkozy laments on the part of his predecessors (read: former French President Fran?ois Mitterrand, who had a child with his mistress about whom the public knew nothing until the man’s funeral).',\n",
       " 'Sarkozy, of all people, should know that a large part of the gravitas of office derives from pomp and circumstance.',\n",
       " 'Statecraft is a realm where appearances are meant to be deceiving.',\n",
       " 'When Sarkozy, who otherwise has such finely tuned media instincts, protests that he’s no different from any other man, he comes dangerously close to confusing the office and the person of the president.',\n",
       " 'Most French people could only dream of an exotic wedding in India.',\n",
       " 'Sarkozy could make that dream come true.',\n",
       " 'If he really is as head-over-heels in love with Bruni as he claims, and plans to marry her imminently, why not take advantage of his upcoming trip to India and make this a wedding to remember?',\n",
       " 'He could meet his bride seated majestically on the caparison of an elaborately decorated elephant, and she would look ravishing swathed and bejeweled in Indian finery.',\n",
       " 'The “bling-bling” president, as Sarkozy has been dubbed, can wear all the gold he wants and heap yet more diamonds on his bride.?',\n",
       " 'The cameras would roll, Indians would smile, and France would be treated to a Bollywood spectacle beyond its wildest dreams.',\n",
       " 'And if it’s too late for the wedding, there’s always the reception.',\n",
       " 'A Born-Again CAP',\n",
       " 'WAGENINGEN, NETHERLANDS – Born in 1957, the Common Agricultural Policy (CAP) is now more than 50 years old, and the European Commission is proposing what it calls a health check for its middle-aged child.',\n",
       " 'But superficial repairs will not meet the European Union’s future needs. The CAP must be born again.',\n",
       " 'Work on its renewal is due to start now, with the completed project ready in 2013. But a much more profound re-think is needed.',\n",
       " 'The CAP’s original aim was to provide a secure source of food for the six original member states of the Union, which were importers of food and sought a degree of self-sufficiency.',\n",
       " 'Good, healthy, and cheap food had to be accessible for all citizens.',\n",
       " 'Improved agricultural productivity would benefit rural areas and give farmers a comparative share in the Union’s growing wealth.',\n",
       " 'Instruments to achieve those objectives were developed, and food security was achieved.',\n",
       " 'The CAP quickly came to be seen as the jewel in the crown of the European project.',\n",
       " 'As the EU has evolved and expanded, food systems have become more complex, involving production, processing, supply-chain organization, and wholesale and retail distribution, with all of these involving new issues like health and the environment.',\n",
       " 'The use of land is also receiving more serious scrutiny.',\n",
       " 'A 1991 study by the Netherlands Scientific Council for Governmental Policy, entitled',\n",
       " 'Those figures were for an EU of 15 countries, so with today’s 27 members the possibilities are even greater.',\n",
       " 'A Dutch analysis of land use has shown that by employing the best technical and ecological means on the best available land, substantial gains could be made in food production.',\n",
       " 'So it is not surprising that the number of farmers needed has fallen substantially.',\n",
       " 'A simplified CAP would encourage cleaner, more productive, and efficient agriculture.',\n",
       " 'A side benefit for the EU’s standing in the world could be that the World Trade Organization’s stalled Doha negotiations could be restarted once farmers in developing countries are assured of getting a fair deal from Europe.',\n",
       " 'Moreover, the CAP’s role as a motor of political and social integration in Europe could be restored once renewed policies are in place.',\n",
       " 'But renewal of this sort cannot be left to global market forces, as the results might not necessarily benefit European agriculture and society.',\n",
       " 'If the market “misbehaves,” farmers could be reduced to poverty, leading to the neglect of large areas of Europe.',\n",
       " 'That is a real enough danger to which policymakers must give serious thought as they reform the CAP on the basis of the following five pillars.',\n",
       " '1.&#160;&#160;&#160;&#160;&#160; The EU needs a knowledge and innovation policy that strengthens European agriculture’s competitiveness.',\n",
       " 'Such a policy has been successful in the Netherlands, substantially contributing to the development and power of the country’s agribusiness.',\n",
       " 'Ten of 21 branches of Dutch agribusiness, including horticultural seeds, ornamentals, seed potatoes, and veal, are among the top contributors to the national economy and the country’s trade balance.',\n",
       " 'In the EU as a whole, a policy directed toward research programs stimulating scientific excellence and greater coherence in the European knowledge system would greatly strengthen agriculture’s competitiveness and contribute to food security and sustainable development.',\n",
       " '2.&#160;&#160;&#160;&#160;&#160; Europe also needs a restructuring policy for land use.',\n",
       " 'Many structural improvement programs have been financed at the European level, but agricultural production and land use are not among them.',\n",
       " 'The development of an Agricultural Main Structure would compliment the European Ecological Main Structure.',\n",
       " 'Reforestation and the repair of natural ecosystems should also be part of a land use policy.',\n",
       " '3.&#160;&#160;&#160;&#160;&#160; A policy for European food systems would treat production, processing, distribution, logistics, and retailing in combination.',\n",
       " 'Consumption patterns and preferences are an integral part of such systems.',\n",
       " 'Preliminary studies by the European Science Foundation’s “Forward Look on European Food Systems” could prove useful in devising an EU-wide policy.',\n",
       " '4.&#160;&#160;&#160;&#160;&#160; Metropolitan agriculture in a rapidly urbanizing world can provide high-quality produce on small amounts of land.',\n",
       " 'It offers an answer to rising demand for healthy food with minimal environmental side effects.',\n",
       " '5.&#160;&#160;&#160;&#160;&#160; A new CAP should include a policy to safeguard Europe’s landscapes.',\n",
       " 'But a cultural heritage should not be maintained everywhere, nor should it ignore cost.',\n",
       " 'And it should not be a defensive policy of the sort that tends to concentrate on poor-quality land.',\n",
       " 'These five pillars involve drastic choices, but they will probably require less money from Europe’s taxpayers, not more.',\n",
       " 'They could make a real contribution to cleaner, more productive, and efficient farming and land use, while addressing social needs.',\n",
       " 'A Breakthrough Against Hunger',\n",
       " 'NEW YORK – Today’s world hunger crisis is unprecedentedly severe and requires urgent measures.',\n",
       " 'Nearly one billion people are trapped in chronic hunger – perhaps 100 million more than two years ago.',\n",
       " 'Spain is taking global leadership in combating hunger by inviting world leaders to Madrid in late January to move beyond words to action.',\n",
       " 'With Spain’s leadership and United Nations Secretary General Ban Ki-moon’s partnership, several donor governments are proposing to pool their financial resources so that the world’s poorest farmers can grow more food and escape the poverty trap.',\n",
       " 'The benefits of some donor help can be remarkable.',\n",
       " 'Peasant farmers in Africa, Haiti, and other impoverished regions currently plant their crops without the benefit of high-yield seed varieties and fertilizers.',\n",
       " 'The result is a grain yield (for example, maize) that is roughly one-third less than what could be achieved with better farm inputs.',\n",
       " 'African farmers produce roughly one ton of grain per hectare, compared with more than four tons per hectare in China, where farmers use fertilizers heavily.',\n",
       " 'African farmers know that they need fertilizer; they just can’t afford it.',\n",
       " 'With donor help, they can.',\n",
       " 'Not only do these farmers then feed their families, but they also can begin to earn market income and to save for the future.',\n",
       " 'By building up savings over a few years, the farmers eventually become creditworthy, or have enough cash to purchase vitally necessary inputs on their own.',\n",
       " 'There is now widespread agreement on the need for increased donor financing for small farmers (those with two hectares or less of land, or impoverished pastoralists), which is especially urgent in Africa.',\n",
       " 'The UN Secretary General led a steering group last year that determined that African agriculture needs around $8 billion per year in donor financing – roughly four times the current total – with a heavy emphasis on improved seeds, fertilizer, irrigation systems, and extension training.',\n",
       " 'In addition to direct help for small farms, donors should provide more help for the research and development needed to identify new high-yielding seed varieties, especially to breed plants that can withstand temporary flooding, excess nitrogen, salty soils, crop pests, and other challenges to sustainable food production.',\n",
       " 'Helping the poor with today’s technologies, while investing in future improved technologies, is the optimum division of labor.',\n",
       " 'This investment pays off wonderfully, with research centers such as the International Rice Research Institute and the International Maize and Wheat Improvement Centre providing the high-yield seeds and innovative farming strategies that together triggered the Asian Green Revolution.',\n",
       " 'These centers are not household names, but they deserve to be.',\n",
       " 'Their scientific breakthroughs have helped to feed the world, and we’ll need more of them.',\n",
       " 'Dozens of low-income, food-deficit countries, perhaps as many as 40-50, have elaborated urgent programs for increased food production by small farms, but are currently held back by the lack of donor funding.',\n",
       " 'These countries have appealed to the World Bank for financing, and the Bank made a valiant effort in 2008 to help through its new Global Food Crisis Response Program (GFCRP).',\n",
       " 'But the Bank does not yet have sufficient funds to meet these countries’ urgent needs, and has had to ration assistance to a small fraction of the flows that could be effectively and reliably used.',\n",
       " 'Hundreds of millions of people, in the meantime, remain trapped in hunger.',\n",
       " 'Many individual donor countries have declared that they are now prepared to increase their financial support for smallholder agriculture, but are searching for the appropriate mechanisms to do so.',\n",
       " 'The current aid structures are inadequate.',\n",
       " 'The more than 20 bilateral and multilateral donor agencies for agriculture are highly fragmented and of insufficient scale individually and collectively.',\n",
       " 'Despite the dedicated efforts of many professionals, the response to the hunger crisis remains utterly inadequate.',\n",
       " 'The 2008 planting seasons came and went with much too little additional help for impoverished small farmers.',\n",
       " 'African countries search endlessly, and mostly fruitlessly, for the small amounts of funding needed for their purchases of fertilizer and improved seeds.',\n",
       " 'My colleagues and I, serving on an advisory committee for the Spanish initiative, have recommended that donors pool their funds into a single international account, which we call the Financial Coordination Mechanism (FCM).',\n",
       " 'These pooled funds would enable farmers in poor countries to obtain the fertilizer, improved seed varieties, and small-scale irrigation equipment that they urgently need.',\n",
       " 'Poor countries would receive prompt and predictable financing for agricultural inputs from a single account, rather than from dozens of distinct and fragmented donors.',\n",
       " 'By pooling financial resources into a single-donor FCM, aid programs’ administrative costs could be kept low, the availability of aid flows could be assured, and poor countries would not have to negotiate 25 times in order to receive help.?',\n",
       " 'The time for business as usual is over.',\n",
       " 'The donors promised to double aid to Africa by 2010, but are still far off track.',\n",
       " 'Indeed, during the past 20 years, they actually cut aid for agriculture programs, and only now are reversing course.',\n",
       " 'Meanwhile, a billion people go hungry each day.',\n",
       " 'We need a breakthrough that is demonstrable, public, clear, and convincing, that can mobilize the public’s hearts and minds, and that can demonstrate success.',\n",
       " 'History can be made in Madrid at the end of January, when the world’s richest and poorest countries converge to seek solutions to the global hunger crisis.',\n",
       " 'The lives of the billion poorest people depend on it.',\n",
       " 'A Breakthrough Opportunity for Global Health',\n",
       " 'NEW YORK – Every year, millions of people die from preventable and treatable diseases, especially in poor countries.',\n",
       " 'In many cases, lifesaving medicines can be cheaply mass-produced, but are sold at prices that block access to those who need them.',\n",
       " 'And many die simply because there are no cures or vaccines, because so little of the world’s valuable research talent and limited resources is devoted to addressing the diseases of the poor.',\n",
       " 'This state of affairs represents a failure of economics and law that urgently needs to be corrected.',\n",
       " 'The good news is that there are now opportunities for change, most promisingly through an international effort headed by the World Health Organization that would begin to fix the broken intellectual-property regime that is holding back the development and availability of cheap drugs.',\n",
       " 'Two main problems limit the availability of medicines today.',\n",
       " 'One is that they are very costly; or, more accurately, the price charged for them is very high, though the cost of producing them is but a fraction of that amount.',\n",
       " 'Second, drug development is geared toward maximizing profit, not social benefit, which skews efforts directed at the creation of medicines that are essential to human welfare.',\n",
       " 'Because the poor have so little money to spend, drug companies, under current arrangements, have little incentive to do research on the diseases that afflict them.',\n",
       " 'It doesn’t have to be this way.',\n",
       " 'Drug companies argue that high prices are necessary to fund research and development.',\n",
       " 'But, in the United States, it is actually the government that finances most health-related research and development – directly, through public support (National Institutes of Health, National Science Foundation), and indirectly, through public purchases of medicine, both in the Medicare and Medicaid programs.',\n",
       " 'Even the part that is not government-financed is not a conventional market; most individuals’ purchases of prescription medicines are covered by insurance.',\n",
       " 'Government finances health-care research because improved medicines are a public good.',\n",
       " 'The resulting knowledge benefits everyone by stopping epidemics and limiting the economic and human toll of widespread illness.',\n",
       " 'Efficiency requires sharing research as widely as possible as soon as it is available.',\n",
       " 'Thomas Jefferson compared knowledge to candles: when one is used to light another, it does not diminish the light of the first.',\n",
       " 'On the contrary, everything becomes brighter.',\n",
       " 'Yet, in America and most of the world, drug prices are still exorbitant and the spread of knowledge is tightly limited.',\n",
       " 'That is because we have created a patent system that gives innovators a temporary monopoly over what they create, which encourages them to hoard their knowledge, lest they help a competitor.',\n",
       " 'While this system does provide incentives for certain kinds of research by making innovation profitable, it allows drug companies to drive up prices, and the incentives do not necessarily correspond to social returns.',\n",
       " 'In the health-care sector, it may be more profitable to devote research to a “me-too” drug than to the development of a treatment that really makes a difference.',\n",
       " 'The patent system may even have adverse effects on innovation, because, while the most important input into any research is prior ideas, the patent system encourages secrecy.',\n",
       " 'A solution to both high prices and misdirected research is to replace the current model with a government-supported prize fund.',\n",
       " 'With a prize system, innovators are rewarded for new knowledge, but they do not retain a monopoly on its use.',\n",
       " 'That way, the power of competitive markets can ensure that, once a drug is developed, it is made available at the lowest possible price – not at an inflated monopoly price.',\n",
       " 'Fortunately, some US lawmakers are taking a strong interest in this approach.',\n",
       " 'The Prize Fund for HIV/AIDS Act, a congressional bill introduced by Senator Bernie Sanders, is just such an initiative.',\n",
       " 'His bill also contains an important provision aimed at encouraging open-source research, which would move the current research model away from secrecy toward sharing.',\n",
       " 'But, globally, our innovation system needs much bigger changes.',\n",
       " 'The WHO’s efforts to encourage broad reforms at the international level are crucial.',\n",
       " 'This spring, the WHO released a report that recommends solutions similar to those proposed in the US Senate bill, but on a global level.',\n",
       " 'Importantly, the report, “Research and Development to Meet Health Needs in Developing Countries,” recommends a comprehensive approach, including mandatory funding contributions from governments for research on developing countries’ health needs; international coordination of health-care priorities and implementation; and a global observatory that would monitor where needs are greatest.',\n",
       " 'In late May, the international community will have a chance to begin implementing these ideas at the WHO World Health Assembly – a moment of hope for public health around the world.',\n",
       " 'Reforming our innovation system is not just a matter of economics.',\n",
       " 'It is, in many cases, a matter of life and death.',\n",
       " 'It is therefore essential to de-link R&amp;D incentives from drug prices, and to promote greater sharing of scientific knowledge.',\n",
       " 'For America, the Sanders bill marks important progress.',\n",
       " 'For the world, the WHO’s recommendations represent a once-in-a-generation opportunity to remedy a long-standing and egregious inequity in health care, and, more broadly, to set a model for governance of global public goods befitting an era of globalization.',\n",
       " 'We cannot afford to let this opportunity pass us by.',\n",
       " 'Absent-Minded Killers',\n",
       " 'As a species, human beings have a major self-control problem.',\n",
       " 'We humans are now so aggressively fishing, hunting, logging, and growing crops in all parts of the world that we are literally chasing other species off the planet.',\n",
       " 'Our intense desire to take all that we can from nature leaves precious little for other forms of life.',\n",
       " 'In 1992, when the world’s governments first promised to address man-made global warming, they also vowed to head off the human-induced extinction of other species.',\n",
       " 'The Convention on Biological Diversity, agreed at the Rio Earth Summit, established that “biological diversity is a common concern of humanity.”',\n",
       " 'The signatories agreed to conserve biological diversity, by saving species and their habitats, and to use biological resources (e.g., forests) in a sustainable manner.',\n",
       " 'In 2002, the treaty’s signatories went further, committing to “a significant reduction in the current rate of biodiversity loss” by 2010.',\n",
       " 'Unfortunately, like so many other international agreements, the Convention on Biological Diversity remains essentially unknown, un-championed, and unfulfilled.',\n",
       " 'That neglect is a human tragedy.',\n",
       " 'For a very low cash outlay – and perhaps none at all on balance – we could conserve nature and thus protect the basis of our own lives and livelihoods.',\n",
       " 'We kill other species not because we must, but because we are too negligent to do otherwise.',\n",
       " 'Consider a couple of notorious examples.',\n",
       " 'Some rich countries, such as Spain, Portugal, Australia, and New Zealand, have fishing fleets that engage in so-called “bottom trawling.”',\n",
       " 'Bottom trawlers drag heavy nets over the ocean bottom, destroying magnificent, unexplored, and endangered marine species in the process.',\n",
       " 'Complex and unique ecologies, most notably underground volcanoes known as seamounts, are ripped to shreds, because bottom trawling is the “low cost” way to catch a few deep sea fish species.',\n",
       " 'One of these species, orange roughy, has been caught commercially for only around a quarter-century, but already is being fished to the point of collapse.',\n",
       " 'Likewise, in many parts of the world, tropical rainforest is being cleared for pasture land and food crops.',\n",
       " 'The result is massive loss of habitat and destruction of species, yielding a tiny economic benefit at a huge social cost.',\n",
       " 'After cutting down a swath of rainforest, soils are often quickly leached of their nutrients so that they cannot sustain crops or nutritious grasses for livestock.',\n",
       " 'As a result, the new pasture land or farmland is soon abandoned, with no prospect for regeneration of the original forest and its unique ecosystems.',\n",
       " 'Because these activities’ costs are so high and their benefits so low, stopping them would be easy.',\n",
       " 'Bottom trawling should simply be outlawed; it would be simple and inexpensive to compensate the fishing industry during a transition to other activities.',\n",
       " 'Forest clearing, on the other hand, is probably best stopped by economic incentives, perhaps combined with regulatory limits.',\n",
       " 'Simply restricting the practice of land clearing probably would not work, since farm families and communities would face a strong temptation to evade legal limits.',\n",
       " 'On the other hand, financial incentives would probably succeed, because cutting down forest to create pastureland is not profitable enough to induce farmers to forego payments for protecting the land.',\n",
       " 'Many rainforest countries have united in recent years to suggest the establishment of a rainforest conservation fund by the rich countries, to pay impoverished small farmers a small amount of money to preserve the forest.',\n",
       " 'A well-designed fund would slow or stop deforestation, preserve biodiversity, and reduce emissions of carbon dioxide the burning of cleared forests.',\n",
       " 'At the same time, small farmers would receive a steady flow of income, which they could use for micro-investments to improve their household’s wealth, education, and health.',\n",
       " 'Aside from banning bottom trawling and establishing a global fund for avoided deforestation, we should designate a global network of protected marine areas, in which fishing, boating, polluting, dredging, drilling, and other damaging activities would be prohibited.',\n",
       " 'Such areas not only permit the regeneration of species, but also provide ecological benefits that spill over to neighboring unprotected areas.',\n",
       " 'We also need a regular scientific process to present the world with the evidence on species abundance and extinction, just as we now have such a process for climate change.',\n",
       " 'Politicians don’t listen very well to individual scientists, but they are forced to listen when hundreds of scientists speak with a united voice.',\n",
       " 'Finally, the world should negotiate a new framework no later than 2010 to slow human-induced climate change.',\n",
       " 'There can be little doubt that climate change poses one of the greatest risks to species’ viability.',\n",
       " 'As the planet warms, and rain and storm patterns change dramatically, many species will find themselves in climate zones that no longer support their survival.',\n",
       " 'Some can migrate, but others (such as polar bears) are likely to be driven to extinction unless we take decisive action to head off climate change.',\n",
       " 'These measures are achievable by 2010.',\n",
       " 'They are affordable, and in each case would ultimately deliver large net benefits.',\n",
       " 'Most importantly, they would allow us to follow through on a global promise.',\n",
       " 'It is too painful to believe that humanity would destroy millions of other species – and jeopardize our own future – in a fit of absent-mindedness.',\n",
       " 'Making Do With More',\n",
       " 'BERKELEY – In the United States, just three out of ten workers are needed to produce and deliver the goods we consume.',\n",
       " \"Everything we extract, grow, design, build, make, engineer, and transport – down to brewing a cup of coffee in a restaurant kitchen and carrying it to a customer's table – is done by roughly 30% of the country's workforce.\",\n",
       " 'The rest of us spend our time planning what to make, deciding where to install the things we have made, performing personal services, talking to each other, and keeping track of what is being done, so that we can figure out what needs to be done next.',\n",
       " 'And yet, despite our obvious ability to produce much more than we need, we do not seem to be blessed with an embarrassment of riches.',\n",
       " 'One of the great paradoxes of our time is that workers and middle-class households continue to struggle in a time of unparalleled plenty.',\n",
       " 'John Maynard Keynes was not off by much when he famously predicted in 1930 that the human race\\'s “economic problem, the struggle for subsistence,\" was likely to be “solved, or be at least within sight of solution, within a hundred years.\"',\n",
       " \"It will take another generation, perhaps, before robots have completely taken over manufacturing, kitchen work, and construction; and the developing world looks to be 50 years behind. But Keynes would have been spot on had he targeted his essay at his readers' great-great-great-great grandchildren.\",\n",
       " 'And yet there are few signs that working- and middle-class Americans are living any better than they did 35 years ago.',\n",
       " 'Even stranger, productivity growth does not seem to be soaring, as one would expect; in fact, it seems to be decelerating, according to research by John Fernald and Bing Wang, economists in the Economic Research Department of the Federal Reserve Bank of San Francisco.',\n",
       " 'Growth prospects are even worse, as innovation hits gale-force headwinds.',\n",
       " 'One way to reconcile the changes in the job market with our lived experience and statistics like these is to note that much of what we are producing is very different from what we have made in the past.',\n",
       " 'For most of human experience, the bulk of what we produced could not be easily shared or used without permission.',\n",
       " 'The goods we made were what economists call “rival\" and “excludible\" commodities.',\n",
       " 'Being “rival\" means that two people cannot use the same product at the same time.',\n",
       " 'Being “excludible\" means that the owner of a product can easily prevent others from using it.',\n",
       " 'These two traits put a great deal of bargaining power in the hands of those who control production and distribution, making them ideal for a market economy based on private property.',\n",
       " 'Money naturally flows to where utility and value are being provided – and those flows are easy to track in national accounts.',\n",
       " 'But much of what we are producing in the information age is neither rival nor excludible – and this changes the entire picture.',\n",
       " 'The creation of information-age goods is difficult to incentivize; their distribution is hard to monetize; and we lack the tools to track them easily in national accounts.',\n",
       " 'The result is an ever-growing discrepancy between what people would be willing to pay for a given service and growth as measured in national statistics.',\n",
       " 'In other words, we are producing and consuming much more than our economic indicators suggest – and the creators of many of those products are not being adequately compensated.',\n",
       " 'This produces a set of unique problems.',\n",
       " 'To ensure that the workers of today and tomorrow are able to capture the benefits of the information age will require us to redesign our economic system to stimulate the creation of these new types of commodities.',\n",
       " 'In addition to developing ways to account for this new type of wealth, we will have to develop channels through which demand for a product contributes to the income of its creator.',\n",
       " 'Only by finding ways to put true value on the goods we produce will we be able to sustain a middle-class society, rather than one of techno-plutocrats and their service-sector serfs.',\n",
       " 'The Closing of the Academic Mind',\n",
       " 'LONDON – I would wager that I have been Chancellor of more universities than anyone alive today.',\n",
       " 'This is partly because when I was Governor of Hong Kong, I was made Chancellor of every university in the city.',\n",
       " 'I protested that it would surely be better for the universities to choose their own constitutional heads.',\n",
       " 'But the universities would not allow me to resign gracefully.',\n",
       " 'So for five years I enjoyed the experience of giving tens of thousands of students their degrees and watching what this rite of passage meant for them and their families.',\n",
       " 'When I came back to Britain in 1997, I was asked to become Chancellor of Newcastle University.',\n",
       " 'Then, in 2003, I was elected Chancellor by the graduates of Oxford University, one of the world’s greatest institutions of learning.',\n",
       " 'So it should not be surprising that I have strong views about what it means to be a university and to teach, do research, or study at one.',\n",
       " 'Universities should be bastions of freedom in any society.',\n",
       " 'They should be free from government interference in their primary purposes of research and teaching; and they should control their own academic governance.',\n",
       " 'I do not believe it is possible for a university to become or remain a world-class institution if these conditions do not exist.',\n",
       " 'The role of a university is to promote the clash of ideas, to test the results of research with other scholars, and to impart new knowledge to students.',\n",
       " 'Freedom of speech is thus fundamental to what universities are, enabling them to sustain a sense of common humanity and uphold the mutual tolerance and understanding that underpin any free society.',\n",
       " 'That, of course, makes universities dangerous to authoritarian governments, which seek to stifle the ability to raise and attempt to answer difficult questions.',\n",
       " 'But if any denial of academic liberty is a blow struck against the meaning of a university, the irony today is that some of the most worrying attacks on these values have been coming from inside universities.',\n",
       " 'In the United States and the United Kingdom, some students and teachers now seek to constrain argument and debate.',\n",
       " 'They contend that people should not be exposed to ideas with which they strongly disagree.',\n",
       " 'Moreover, they argue that history should be rewritten to expunge the names (though not the endowments) of those who fail to pass today’s tests of political correctness.',\n",
       " 'Thomas Jefferson and Cecil Rhodes, among others, have been targeted.',\n",
       " 'And how would Churchill and Washington fare if the same tests were applied to them?',\n",
       " 'Some people are being denied the chance to speak as well – so-called “no platforming”, in the awful jargon of some clearly not very literate campuses.',\n",
       " 'There are calls for “safe spaces” where students can be protected from anything that assaults their sense of what is moral and appropriate.',\n",
       " 'This reflects and inevitably nurtures a harmful politics of victimization – defining one’s own identity (and thus one’s interests) in opposition to others.',\n",
       " 'When I was a student 50 years ago, my principal teacher was a leading Marxist historian and former member of the Communist Party.',\n",
       " 'The British security services were deeply suspicious of him.',\n",
       " 'He was a great historian and teacher, but these days I might be encouraged to think that he had threatened my “safe space.”',\n",
       " 'In fact, he made me a great deal better informed, more open to discussion of ideas that challenged my own, more capable of distinguishing between an argument and a quarrel, and more prepared to think for myself.',\n",
       " 'Of course, some ideas – incitement of racial hatred, gender hostility, or political violence – are anathema in every free society.',\n",
       " 'Liberty requires some limits (decided freely by democratic argument under the rule of law) in order to exist.',\n",
       " 'Universities should be trusted to exercise that degree of control themselves.',\n",
       " 'But intolerance of debate, of discussion, and of particular branches of scholarship should never be tolerated.',\n",
       " 'As the great political philosopher Karl Popper taught us, the only thing we should be intolerant of is intolerance itself.',\n",
       " 'That is especially true at universities.',\n",
       " 'Yet some American and British academics and students are themselves undermining freedom; paradoxically, they have the liberty to do so.',\n",
       " 'Meanwhile, universities in China and Hong Kong are faced with threats to their autonomy and freedom, not from within, but from an authoritarian government.',\n",
       " 'In Hong Kong, the autonomy of universities and free speech itself, guaranteed in the city’s Basic Law and the 50-year treaty between Britain and China on the city’s status, are under threat.',\n",
       " 'The rationale seems to be that, because students strongly supported the pro-democracy protests in 2014, the universities where they study should be brought to heel.',\n",
       " 'So the city’s government blunders away, stirring up trouble, clearly on the orders of the government in Beijing.',\n",
       " 'Indeed, the Chinese authorities only recently showed what they think of treaty obligations and of the “golden age” of Sino-British relations (much advertised by British ministers), by abducting a British citizen (and four other Hong Kong residents) on the city’s streets.',\n",
       " 'The five were publishing books that exposed some of the dirty secrets of China’s leaders.',\n",
       " 'On the mainland, the Chinese Communist Party has launched the biggest crackdown on universities since the aftermath of the killings in Tiananmen Square in 1989.',\n",
       " 'There is to be no discussion of so-called Western values in China’s universities. Only Marxism can be taught.',\n",
       " 'Did no one tell President Xi Jinping and his Politburo colleagues where Karl Marx came from?',\n",
       " 'The trouble these days is precisely that they know little about Marx but a lot about Lenin.',\n",
       " 'Westerners should take a closer interest in what is happening in China’s universities and what that tells us about the real values underpinning scholarship, teaching, and the academy.',\n",
       " 'Compare and contrast, as students are asked to do.',\n",
       " 'Do you want universities where the government decides what it is allegedly safe for you to learn and discuss?',\n",
       " 'Or do you want universities that regard the idea of a “safe space” – in terms of closing down debate in case it offends someone – as an oxymoron in an academic setting?',\n",
       " 'Western students should think occasionally about their counterparts in Hong Kong and China who must fight for freedoms that they take for granted – and too often abuse.',\n",
       " 'The New Brain Drain in Science',\n",
       " 'DUBAI – In December 2013, the Nobel laureate physicist Peter Higgs told The Guardian that if he were seeking a job in academia today, “I don’t think I would be regarded as productive enough.”',\n",
       " 'Having published fewer than ten papers since his groundbreaking work in 1964, Higgs believes that no university would employ him nowadays.',\n",
       " 'Academics are well acquainted with the notion of “publish or perish.”',\n",
       " 'They must publish their work in peer-reviewed journals increasingly often to climb the career ladder, protect their jobs, and secure funding for their institutions.',\n",
       " 'But what happens to scientists and other scholars, such as those in the Middle East, who have different research concerns from – and scant connections to – the professional journals that can make or break an academic/scientific career?',\n",
       " 'Scholars and institutions with high publishing rates in the established journals receive better productivity scores, which translate into bigger rewards, in terms of enhanced careers and greater research funding.',\n",
       " 'Whether the work they are publishing has a measurable impact on their field of study is, sadly, too often a secondary concern.',\n",
       " 'The incentives they face mean that quantity often comes before quality.',\n",
       " 'Academic journals determine the various disciplinary rankings that academic institutions are compelled to climb, which leads institutions to hire and retain only those scholars who can produce at high rates.',\n",
       " 'This has given rise to a deeper, twofold problem: academic journals have become disproportionately influential, and they have placed a premium on empirical research.',\n",
       " 'With respect to the first problem, journals are gradually replacing institutions as the arbiters of quality within academic communities.',\n",
       " 'Scholars in almost any discipline seeking jobs at “A-level” institutions must publish in a select few A-level journals that are seen as gateways.',\n",
       " 'These journals’ editorial boards increasingly privilege positivist theoretical work – meaning research that is based on empirical data analysis.',\n",
       " 'Qualitative research – such as ethnographies, participatory surveys, and case studies – is often designated as suitable only for B- and C-level journals.',\n",
       " 'Academics conducting empirical research have a big advantage over those carrying out qualitative work, because they can use efficient software and powerful computers to test their hypotheses quickly and account for different variables in data sets.',\n",
       " 'This kind of work can be cheaper, too, because a single data set can generate multiple journal articles.',\n",
       " 'To be sure, there is nothing wrong with scientific practices evolving along with technology, or with academics using richer data sets and better software.',\n",
       " 'But adoption of this quantitative approach should not be the single most important criterion for assessing scientific excellence and deciding career trajectories.',\n",
       " 'After all, knowledge is acquired in different ways, and empirical positivism is only one method in a larger epistemological inventory.',\n",
       " 'The positivist trend in science today is particularly problematic for developing countries, where data sets are scarce and often of poor quality.',\n",
       " 'Thus, scientists working in developing countries face a dilemma: either work on rich-world problems for which there is abundant data, or risk career advancement by conducting qualitative work that will not make it into A-level journals.',\n",
       " 'Academics who move from data-rich countries in Europe and North America to data-poor countries in the Middle East and elsewhere often face this problem.',\n",
       " 'As researchers at my institution in Abu Dhabi know, conducting surveys for qualitative research is feasible; but generating rich data from scratch for theory-building research is extremely difficult.',\n",
       " 'At the International Conference on Science and Technology Indicators this year, a French academic researching soil in Africa reported that only 5% of the published work in his field has originated from African researchers.',\n",
       " 'When he dug deeper into his own research, he found that 50% of what he had learned about African soil came from African researchers, who have not or could not publish their work in international academic journals.',\n",
       " 'Countries where English is not the lingua franca are particularly disadvantaged in science, not because they lack academic excellence, but because English-language journals call the shots.',\n",
       " 'Non-English academic journals simply do not command the same attention in the science community.',\n",
       " 'As a result, the scope of research topics that many countries can undertake is limited, and they must struggle to retain scientific talent.',\n",
       " 'This is particularly true in the Middle East, where governments are struggling to diversify their economies, in order to make them more resilient.',\n",
       " 'As English-language empirical-research journals consolidate their hold on the channels that determine whether or not a scientist will have a successful career, developing countries will have to invest heavily in their own data infrastructure to place domestic researchers on a more competitive footing.',\n",
       " 'But even – or especially – if developing countries do make such investments, much will be lost to science.',\n",
       " 'With (mostly) United States-based academic journals reigning over global science, no one has to move to become part of a new brain drain, whereby scientists’ research priorities, problems, and methods gravitate to the dominant positivist epistemology, at the expense of all alternatives.',\n",
       " 'Polluters Must Pay',\n",
       " 'NEW YORK – When BP and its drilling partners caused the Deepwater Horizon oil spill in the Gulf of Mexico in 2010, the United States government demanded that BP finance the cleanup, compensate those who suffered damages, and pay criminal penalties for the violations that led to the disaster.',\n",
       " 'BP has already committed more than $20 billion in remediation and penalties.',\n",
       " 'Based on a settlement last week, BP will now pay the largest criminal penalty in US history – $4.5 billion.',\n",
       " 'The same standards for environmental cleanup need to be applied to global companies operating in poorer countries, where their power has typically been so great relative to that of governments that many act with impunity, wreaking havoc on the environment with little or no accountability.',\n",
       " 'As we enter a new era of sustainable development, impunity must turn to responsibility.',\n",
       " 'Polluters must pay, whether in rich or poor countries.',\n",
       " 'Major companies need to accept responsibility for their actions.',\n",
       " 'Nigeria has been Exhibit A of corporate environmental impunity.',\n",
       " 'For decades, major oil companies, including Shell, ExxonMobil, and Chevron, have been producing oil in the Niger Delta, an ecologically fragile environment of freshwater swamp forests, mangroves, lowland rainforests, and coastal barrier islands.',\n",
       " 'This rich habitat supports remarkable biodiversity – or did before the oil companies got there –&#160;and more than 30 million local inhabitants, who depend on the local ecosystems for their health and livelihoods.',\n",
       " 'Twenty years ago, the International Union for Conservation of Nature and Natural Resources classified the Niger Delta as a region of high biodiversity of marine and coastal flora and fauna – tree species, fish, birds, and mammals, among other forms of life – and therefore rated it as a very high priority for conservation.',\n",
       " 'Yet it also noted that the region’s biodiversity was under massive threat, with little or no protection.',\n",
       " 'The global companies operating in the delta have spilled oil and flared natural gas for decades, without regard for the natural environment and the communities impoverished and poisoned by their actions.',\n",
       " 'One estimate puts the cumulative spills over the past 50 years at approximately 10 million barrels – twice the size of the BP spill.',\n",
       " 'The data are uncertain: there have been many thousands of spills during this period – often poorly documented and their magnitude hidden or simply unmeasured by either the companies or the government.',\n",
       " 'Indeed, just as BP was being hit with new criminal penalties, ExxonMobil announced yet another pipeline leak in the Niger Delta.',\n",
       " 'The environmental destruction of the delta is part of a larger saga: corrupt companies operating hand in hand with corrupt government officials.',\n",
       " 'The companies routinely bribe officials to gain oil leases, lie about output, evade taxes, and dodge responsibility for the environmental damage that they cause.',\n",
       " 'Nigerian officials have become fabulously wealthy, owing to decades of payoffs by international companies that have plundered the delta’s natural wealth.',\n",
       " 'Shell, the largest foreign operator in the Niger Delta, has been criticized repeatedly for its egregious practices and its unwillingness to be held to account.',\n",
       " 'Meanwhile, the local population has remained impoverished and beset by diseases caused by unsafe air, poisoned drinking water, and pollution in the food chain.',\n",
       " 'Local lawlessness has led to gang warfare and persistent illegal tapping into the pipelines to steal oil, leading to further massive oil spills and frequent explosions that kill dozens, including innocent bystanders.',\n",
       " 'In the colonial era, it was the official purpose of imperial power to extract wealth from the administered territories.',\n",
       " 'In the post-colonial period, the methods are better disguised.',\n",
       " 'When oil companies misbehave in Nigeria or elsewhere, they are protected by the power of their home countries.',\n",
       " 'Don’t mess with the companies, they are told by the United States and Europe.',\n",
       " ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bac4645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1929年还是1989年?',\n",
       " '巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。',\n",
       " '一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。',\n",
       " '如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。',\n",
       " '目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。',\n",
       " '欧洲在避免债务和捍卫欧元的名义下正变得谨慎，而美国已经在许多方面行动起来，以利用这一理想的时机来实行急需的结构性改革。',\n",
       " '然而，作为地域战略学家，无论是从政治意义还是从经济意义上，让我自然想到的年份是1989年。',\n",
       " '当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。',\n",
       " '事实上，从表面上看，两者似乎是完全是相反的：一个是象征着压抑和人为分裂的柏林墙的倒塌，而另一个是看似坚不可摧的并令人安心的金融资本主义机构的倒塌。',\n",
       " '然而，和1989年一样，2008-2009年很可能也能被视为一个划时代的改变，其带来的发人深省的后果将在几十年后仍能让我们感受得到。',\n",
       " '东西方意识形态鸿沟的结束，以及对市场绝对信心的后果，都是历史的转折点。',\n",
       " '而2009年所发生的事情可能会威胁1989年革命所带来的积极成果，包括欧洲的和平统一和民主制度战胜了民族主义倾向，如果不是恐外倾向的话。',\n",
       " '1989年，自由民主战胜了由苏联集团具体化并推崇的社会主义意识形态。',\n",
       " '对于里根总统的许多的支持者来说，就是他精心策划的军备竞赛的升级，把苏联经济推向了崩溃的边缘，从而充分显示了自由社会和自由市场的优越性。',\n",
       " '当然，现在的情况和1989年的情况明显不同了。',\n",
       " '首先，也许是最重要的，1989年的革命和随后的苏联解体结束了全球的两极化。',\n",
       " '与此相反，2009年很可能会为一种新的两极化形式铺平道路，只是中国取代了苏联。',\n",
       " '其二，民主制度和市场资本主义，或许要比预期的要脆弱些，看来确实是当时的赢家。 而在2009年，随着全球危机的蔓延，却很难区分赢家和输家；',\n",
       " '每个人似乎都是输家，即使有些国家比其它国家受到的影响更大。',\n",
       " '而历史是不公平的。 尽管美国要为当今的全球危机负更大的责任，但美国可能会比大多数国家以更良好的势态走出困境。',\n",
       " '美国会恢复得更好，但并不是唯一能恢复的国家。',\n",
       " '作为哈佛大学和麻省理工学院的访问教授，我能看到危机过后的世界是什么样子的。',\n",
       " '我们能感受到一个美国和亚洲占主导地位的世界正在形成。',\n",
       " '从一流的麻省理工学院的媒体实验室到哈佛大学的数学和经济系，亚洲人-尤其是中国和印度人-到处都是，犹如公元前一世纪在雅典的罗马人一样：他们对那里学到太多东西的人们充满了敬佩，而他们将在今后几十年打败他们学习的对象。',\n",
       " '但是，在这一新秩序的出现之前，世界可能会面临更广泛的混沌，如果不是彻底的混乱的话。',\n",
       " '例如，对埃及这样一个重要而又脆弱的国家，有数十万的埃及人曾在海湾工作但现在因石油生产国出现的危机而被迫返回了自己的家园，那埃及会发生什么情况呢？',\n",
       " '当富人不再那么富了，穷人就会更穷。',\n",
       " '还有，对于那些追求“欧洲梦”的外来工人，现在他们面临着理应是开放的欧洲国家的仇外心态可能的爆发，他们会有怎样的遭遇？',\n",
       " '1989年革命所带来的成果，最后没有包括我在内许多观察家所想象的那么持久。',\n",
       " '我们只能希望2009年的危机同样地最后被证明是远远低于我们现在以直觉和历史回顾的方式??感觉到的那么剧烈。',\n",
       " '2008年败在何处？',\n",
       " '伯克利—要解决问题，光知道做什么是不够的。',\n",
       " '你实际上必须实施解决办法——并且在事实证明你知道的并不如你认为的那样多时，你得愿意改变办法。',\n",
       " '结果，市场监管被放松，交易被认为安全但实际并不安全的资产变得更加容易。',\n",
       " '结果，系统性风险悄然酝酿，并且超过了央行官员最大胆的想象。',\n",
       " '未受检验并且最终错误的假设形成了一个只能用狂妄自大形容的决策环境。',\n",
       " '官员低估了尾部风险。',\n",
       " '它们将通胀目标设定在2%左右——这意味着当波涛汹涌时他们根本没有多少施展空间。',\n",
       " '此外，最大胆的动作要数欧盟引入欧元作为共同货币。',\n",
       " '事实上，方向错误的决策在危机开始后仍然能维持了很长时间。',\n",
       " '政客对恶化的经济条件的反应是坚定实施错误的药方，对于这场大萧条以来最严重的经济灾难，他们除了绝对需要做的事情意外什么都不做。',\n",
       " '沃尔夫给抵御这场危机开出的药方简单、明智并且无懈可击。',\n",
       " '在短期，他建议拥有储备货币的国家增加支出（特别是公共部门投资融资）、发行更多债务。',\n",
       " '他指出，这些国家的央行应该将通胀目标提高到每年3%甚至4%。',\n",
       " '在中期，沃尔夫认为各国需要实施监管措施降低债务水平、阻止过度举债。',\n",
       " '欧元区也必须解决其内部矛盾，要么通过解散欧元区实现，要么通过引入能令货币联盟正常运行的“最小限度的机构和政策组合”实现。',\n",
       " '沃尔夫的长期解决方案包括遏制不平等、“更加全球化的监管”、更大程度的“个体国家自己决定反应方式的自由”以及更少受到自由市场思想束缚的经济分析。 一开始正是自由市场思想让我们陷入了危机。',\n",
       " '但是，尽管沃尔夫的方案值得推荐，但几乎没有任何实施。',\n",
       " '原因可以从第二本书中找到：由我的朋友、老师和“老板”巴里·艾肯格林（Barry Eichengreen）所著的《镜厅》（Hall of Mirrors）。',\n",
       " '艾肯格林认为，我们对这场危机反应不温不火，原因要追溯到作为弗里德曼（MiltonFriedman）门徒的???币经济学家对凯恩斯主义和明斯基（MInskyite）主义同行的胜利——至少在解释大萧条的原因和后果方面是如此。',\n",
       " '2008年金融危机爆发时，决策者试图使用弗里德曼提出的大萧条方案。',\n",
       " '不幸的是，事实证明这是错的，直言不讳地说，货币主义者对大萧条的解释在众多方面都是错误的，并且非常不完整。',\n",
       " '由此产生的政策足以阻止2008年后的衰退演变为完全的萧条； 但这一局部成功的代价也是巨大的，它让政客宣布危机已经被克服，应该采取紧缩并专注于结构改革。',\n",
       " '结果就是现在的经济停滞，其特征是增长无力并且随时可能成为新常态。',\n",
       " '美国和欧洲损失了10%的潜在财富，而没有强化金融部门监管让世界经济随时可能迎来新一场大危机。',\n",
       " '沃尔夫和艾肯格林会认为，导致2008年金融危机的主要缺陷——以及继续破坏我们对危机的不充分反应的主要缺陷——是知识缺陷。',\n",
       " '事实上，到目前为止，危机的唯一真正教训是它的教训从未被真正汲取。',\n",
       " '欧洲的重振战略',\n",
       " '斯托克霍姆/马德里—去年11月，教皇方济各在其欧洲议会演讲中将欧盟比作祖母——和蔼而富于经验，但已不再有过去的活力和能量。',\n",
       " '方济各说，现在是一个关键时刻，欧盟领导人必须摆脱懈怠形象，承认欧洲所面临的战略挑战，并构建应对挑战的明确政策。',\n",
       " '诚然，教皇的描述在某些角度讲非常准确。',\n",
       " '但是，尽管欧洲看起来疲态尽露，但仍拥有巨大优势。 欧洲是高水平思想和创新的枢纽；',\n",
       " '是世界最有竞争力地区和产业的所在地； 最重要的是，它还构建了一个由五亿人组成的社会和市场。',\n",
       " '但世界正在变化：亚太地区正在日益影响全球经济和其他发展趋势。',\n",
       " '跨太平洋合作伙伴关系（Trans-Pacific Partnership，TPP，由美国和其他11国建立的巨型地区自由贸易区）很有可能加速这一变革（如果中国最终也能加入就更加如此了）。',\n",
       " '尽管TPP距离最终完成协议还有很多障碍需要跨越，但其扩大亚洲经济实力的潜力绝不容???觑。',\n",
       " '欧洲必须致力于确保其在新世界秩序中的地位——首先要强化其自身与美国的贸易和投资联系。',\n",
       " '在TPP谈判不断推进的同时，欧盟-美国跨大西洋贸易和投资伙伴关系（Transatlantic Trade and Investment Partnership，TTIP）谈判却深陷国内矛盾的掣肘，整个工程都有可能夭折。',\n",
       " '大西洋两岸的商业领袖相信，成功的TTIP协议将带来巨大的经济收益——许多研究都强化了这一感觉。',\n",
       " '但一些微妙的问题——比如“氯化鸡”（欧洲怀疑美国鸡肉进入市场前都用氯消毒）和投资者纠纷和解等问题——仍主导着争论。',\n",
       " 'TTIP的目标是释放跨大西洋经济的力量——目前大西洋两岸仍是世界最大最富有的市场，占全球金融活动的四分之三和世界贸易量的一大半。',\n",
       " '（如果TTIP向其他经济体开放——比如土耳其、墨西哥和加拿大——则收益还将更大。 ）',\n",
       " '但是，比达成协议的好处更加让人感到必须成功的是谈判失败可能带来的灾难性结果。',\n",
       " '首先，TTIP谈判的破裂将极大地加强英国鼓吹离开欧盟阵营的力量； 反之，如果TTIP获得实施，英国离开欧盟将是不明智之举，因此也不会发生。',\n",
       " '此外，欧洲的内讧导致其白白浪费了一个战略机会，这一感觉可能促使美国加速远离欧洲。',\n",
       " '而俄罗斯总统普京也必然会将欧洲的失败视为影响欧洲更多地区的良机。',\n",
       " '所有这些凸显出一个根本新战略风险：如果TTIP无法取得进展或失败，而TPP取得进展并成功，则全球平衡将朝着极有利于亚洲的方向发展——欧洲极有可能无法重振其经济和地缘政治影响力。',\n",
       " 'TTIP首次提出时，欧洲似乎能够认识到其价值。',\n",
       " '事实上，是欧洲推动一开始对欧洲的承诺有所怀疑的美国在2013年6月启动了谈判程序。',\n",
       " '当时人们雄心万丈，意图“一鼓作气”完成谈判。',\n",
       " '没人想让谈判长久拖延——或忍受因此带来的政治痛苦。',\n",
       " '但欧盟领导人在事实上抛弃了这一工程，而这也印证了美国的担心。',\n",
       " '贸易谈判在挣扎中前行，而反全球化组织掌握了公共观点的控制权，将TTIP描述为对一切事物的威胁——从欧洲的民主到欧洲的健康莫不如此。',\n",
       " '这是非常危险的错误言论，欧盟领导人必须拿出推进TTIP的战略性理由，从而阻止这一言论获得更大的市场。',\n",
       " '他们还必须重振2015年成功完成谈判的承诺。',\n",
       " '这并不是说解决TTIP谈判中仍然存在的问题轻而易举。',\n",
       " '但签订贸易协议，特别是包含如此多监管问题的贸易协议，永远不是件容易事，因为它必须考虑到现代经济的复杂性和易变性。',\n",
       " '事实上，完成TTIP所包含的挑战并不比欧盟领导人在过去几年的危机中所遇到的挑战更棘手。',\n",
       " 'TTIP谈判将在下个月重启，届时欧盟领导人必须推动真正的进展，并以在年底完成协议为目标。',\n",
       " '好消息是最近的美国中期选举有可能增加他们成功的机会。',\n",
       " '现在，美国总统奥巴马可能将从国会获得所谓的快速谈判授权。',\n",
       " '果真如此的话，美国国会将对谈成的协议予以批准或否决，而不能束之高阁。',\n",
       " '美国总统竞选季即将来开帷幕，在新的一年里，其他问题可能轻而易举地取代欧盟日程。',\n",
       " '正因如此，欧洲领导人已无时间可以浪费。',\n",
       " '他们必须抓住经济机会——并避免战略灾难。',\n",
       " '标志着时代终结的一年？',\n",
       " '马德里——随着2016年的结束，2017年的前景被不确定性所笼罩。',\n",
       " '中东紧张局势正在加剧，欧洲和美国出现了民粹主义运动。',\n",
       " '因为对叙利亚总统巴沙尔·阿萨德未来在和平进程或政治过渡中所起的作用存在根本性的分歧，在几次修复关系的努力无果而终后，中东的叙利亚悲剧性冲突仍将继续下去。',\n",
       " '此外，过去一周，叙利亚政府军在俄罗斯和伊朗的支持下，已经夺回了几乎整个阿勒颇——阿勒颇曾是叙利亚最大的城市，现在已经彻底毁灭在战火之中。',\n",
       " '世界各国来年的首要任务必须是通过密切的地区和国际合作来实现叙利亚和平。',\n",
       " '12月27日，伊朗、俄罗斯和土耳其将在莫斯科举行三方会议，探讨政治解决叙利亚冲突。',\n",
       " '如果那次会议真的召开，也有可能笼罩在俄罗斯驻土耳其大使被杀的阴影当中。',\n",
       " '最出人意料的是居然是俄、伊、土三方，而非美国和欧盟，参与谈判这样的协定。',\n",
       " '今年的积极进展出现在3月份，当时欧盟和土耳其就解决难民危机的协议达成。',\n",
       " '从这次冲突开始到现在，土耳其已经接收了约300万叙利亚难民。',\n",
       " '尽管欧盟土耳其关系现在并不在历史高点，但双方必须在2017年继续对话，尤其因为双方拥有基于经济相互依存、难民危机及反恐集体斗争等诸多共同利益。',\n",
       " '此外，英国退欧谈判将影响明年的欧洲政治。',\n",
       " '英国或将于3月启动《里斯本条约》第五十条，从而触发退出欧盟的正式程序。',\n",
       " '难点在于就确保未来欧盟-英国关系健康发展达成一致。',\n",
       " '做到这一点绝非易事。 欧盟谈判人员已经制订了仅有18个月的时间表。',\n",
       " '虽然不确定因素仍然很多，但有一点十分明确，即如果英国想要保留单一市场准入权，就必须接受包括劳动力自由流动在内的欧盟四项自由政策。',\n",
       " '有几个欧洲国家将在2017年举行大选，孤立主义者、反欧洲民粹运动或将在此期间表现强势。',\n",
       " '对欧盟而言，失去像英国这样在军事和经济上都很重要的国家无疑是一种打击； 但失去法国这样的创始成员国或将酿成悲剧性的后果。',\n",
       " '幸运的是，许多欧洲人对欧盟的看法事实上在英国退欧公投后有改善的迹象。',\n",
       " '但这并不意味着欧盟政府在今后一年所面临的挑战减轻。',\n",
       " '他们必须重新团结被全球化和快速技术创新等强劲全球势力切割得四分五裂的社会。',\n",
       " '英国退欧公投、而后是唐纳德·特朗普在美国总统大选中的获胜标志着西方民粹主义势力抬头。',\n",
       " '但现在，特朗普正在征召寡头和前军人进入其内阁，我们有理由质疑他是否会履行自己摆脱华盛顿“建制派”进行治理的承诺。',\n",
       " '特朗普即将执政的新内阁充满了未知数，但毫无疑问他对多边机构的抗拒会危及合作解决全球最严重问题的工作。',\n",
       " '这必将对美国-欧盟关系构成威胁。',\n",
       " '过去几年，巴黎气候协议及伊朗核协议曾为这个远离多边主义的世界带来一线曙光。',\n",
       " '而在未来几年，这样的曙光会变得越来越稀有。',\n",
       " '现在比以往任何时候都更需要在大国间建立战略互信的对话。',\n",
       " '而特朗普质疑美国是否将继续坚持“一个中国”政策的声明极有可能严重损害世界两大经济体之间的关系。',\n",
       " '同样，尽管特朗普团队不乏一定程度的亲俄倾向，但因为俄罗斯军事入侵叙利亚、东乌克兰以及干涉美国大选的指控，美俄关系同样缺乏战略互信。',\n",
       " '来年对欧洲而言尤其重要。',\n",
       " '植根于对民主、自由和人权的共同尊重，欧盟和美国之间的关系必须保持稳固。',\n",
       " '在充满动荡的2016年后，因为国际政治中几乎鲜有积极消息，2017年注定充满挑战和不确定性。',\n",
       " '但最大的不确定性却是2016年仅仅是又一年的结束，还是整个地缘政治时代的终结。',\n",
       " '全球经济缓慢增长又一年',\n",
       " '华盛顿—去年4月，国际货币基金组织（IMF）预测2015年世界经济将增长3. 5%。',\n",
       " '在随后的几个月中，这一预测被逐步调低，10月份时下降到3.',\n",
       " '1%。 但IMF仍然坚持——在过去七年中它一贯如此，几乎没有什么预见性可言——明年会更好。',\n",
       " '但几乎可以肯定，IMF又错了。',\n",
       " '首先，世界贸易年增长率只有区区2%，而2003—2007年间高达8%。',\n",
       " '这几年中，贸易增长远远高于世界GDP（平均增长率为4. 5%），但最近贸易和GDP增长大致相当。',\n",
       " '即使今年GDP增长超过贸易增长，也可能无法超过2. 7%。',\n",
       " '问题在于为什么。',\n",
       " '据加州大学伯克利分校的克里斯蒂娜和大卫·罗默（Christina and David Romer），现代金融危机（二战后）的余波会持续2—3年时间才会消失。',\n",
       " '哈佛大学经济学家卡门·莱因哈特（Carmen Reinhart）和肯尼斯·罗格夫（Kenneth Rogoff）说，一国需要五年时间才能从金融危机中走出。',\n",
       " '而事实上，2007—2008年的金融动荡已经基本消失。',\n",
       " '因此，是什么导致了经济复苏滞后？',\n",
       " '一个流行的解释是一个叫作“长期停滞”的模糊概念——长期受到抑制的商品和服务需求破坏了投资和招聘激励。',\n",
       " '但需求只有在人们对未来信心不足时才会持续低迷。',\n",
       " '这一信心持续缺失的唯一符合逻辑的解释，正如西北大学的罗伯特·戈登（Robert Gordon）费煞苦心地记录和论证的，是生产率增长缓慢。',\n",
       " '危机前，特别是2003—2007年，生产率增长缓慢被一种世界大部分国家都在欣欣向荣的幻觉所蒙蔽。',\n",
       " '在一些国家——特别是美国、西班牙和爱尔兰——高升的放假、投机性建设和金融冒险互相强化。',\n",
       " '与此同时，各国通过贸易放大彼此的增长。',\n",
       " '这场全球繁荣的核心是中国，这个崛起的巨无霸向全世界输出廉价出口品，遏制了全球通胀。',\n",
       " '同样重要的是，中国进口了巨量大宗商品，从而提振了许多非洲和拉丁美洲经济体； 还购买了大量德国汽车和机器，让欧洲最大经济体保持其地区供应链满负荷运行。',\n",
       " '这一动态在2008年3月左右发生了逆转。 当时，美国拯救了其第五大投资银行贝尔斯登，使其免于破产。',\n",
       " '欧元区银行业深陷次贷乱局，急缺美元，因此美国和欧洲的大部开始滑向衰退。',\n",
       " '在繁荣年份，世界贸易传播着财富，而如今，世界贸易传播着经济萎靡。',\n",
       " '随着各国GDP增长的放缓，进口也开始放缓，贸易伙伴的增长也随之放缓。',\n",
       " '美国经济从2009年下半年开始走出衰退，这主要是为稳定金融体系而采取的激进货币政策和措施的功劳。',\n",
       " '相反，欧洲决策者拒绝采取货币刺激而实施了财政紧缩，无视其银行压力的加剧。',\n",
       " '于是，欧元区将世界拖入了第二场全球衰退。',\n",
       " '而正当这场衰退眼看就要痊愈时，新兴经济体开始崩溃。',\n",
       " '多年来，观察家们一直在鼓吹这些国家领导人如何如何实施治理和刺激增长的改革。',\n",
       " '2012年10月，IMF大大赞扬了新兴经济体的“恢复力”。',\n",
       " '恰逢此时，门面撑不住了，暴露出一个令人不快的事实：大宗商品价格高企和大规模资本流入等因素掩盖了严重的经济弱点，也让剧烈的不平等性和无孔不入的腐败成为合法的文化。',\n",
       " '如今，这些问题因为全球贸易的支柱——中国增长放缓而雪上加霜。 而最坏的时刻还没有到来。',\n",
       " '中国巨大的工业过剩产能和房地产过剩还没有消化；',\n",
       " '推动其全球兼并的傲慢必须得到遏制； 其腐败网络也必须打破。',\n",
       " '简言之，2015年拖累全球经济的因素在新的一年里还会持续——有的甚至还会加剧。',\n",
       " '新兴经济体将继续疲软。',\n",
       " '暂时摆脱了紧缩的欧元区将受到全球贸易萎靡的束缚。',\n",
       " '公司债券利率的提高预示美国增长减速。',\n",
       " '中国资产价值崩盘可能引发金融动荡。',\n",
       " '而决策者就像是无根之木，没有政治手段来遏制这些趋势。',\n",
       " 'IMF应该停止更新增长预测，而是发布警告：全球经济将继续保持萎靡和脆弱，除非世界领导人着力刺激创新和增长。',\n",
       " '这样的措施早就应该实行了。',\n",
       " '特朗普的不确定性',\n",
       " '发自纽约——每年一月，我都会尝试对未来一年作出预测。 经济预测是件极为困难的事情；',\n",
       " '以至于哈里·杜鲁门总统（Harry Truman）曾要求手下给他找一位独臂经济学家——这样此人就无法模棱两可地说“另一方面（on the other hand）”了——但一直以来我的预测还是靠谱的。',\n",
       " '这几年来我正确地预见到，在缺乏更强有力财政刺激政策（在欧洲或美国都一样）的情况下，2008年大衰退的复苏进程将较为缓慢。',\n",
       " '同时我的预测更多地依赖于对基本经济力量的分析，而非复杂的经济计量模型。',\n",
       " '例如在2016年初，很显然在过去几年中呈现的全球总需求不足将不大可能出现巨大改变。',\n",
       " '因此我认为那些相信会实现强劲复苏的预测者是过于乐观了，最终实际的经济发展也与我的预测一致。',\n",
       " '2016年的政治事件也是如此。',\n",
       " '多年来我一直指出，除非在全球许多国家（尤其是美国）日益严重的不平等问题得到解决，否则必将产生某些政治后果。',\n",
       " '但不平等状况依然持续恶化——令人震惊的数据显示，美国的预期平均寿命正在下降。',\n",
       " '正如安娜·凯斯（Anne Case）和安格斯·迪顿（Angus Deaton）的一项研究表明：美国大部分人口的预期寿命都在下降，包括所谓北部老工业区的愤怒居民们。',\n",
       " '而由于90％的底层人士收入增长已经停滞了将近1/3个世纪（且其中很大比例都出现了下滑），医疗数据清晰明白地证实了美国大部分地区处境不妙。',\n",
       " '但美国可能处于这种趋势的极端，其他地方的情况会好一点。',\n",
       " '但如果这似乎必然预示着某些政治后果，那么其形式和时间就不那么明显了。',\n",
       " '为什么美国的下行会在经济恢复期出现，而不是更为提前？',\n",
       " '为何这会表现为政治右倾？',\n",
       " '毕竟正是共和党人拒绝援助那些因他们全力推动全球化而失业的人们，是共和党人否决在26个州扩大医疗补助，从而拒绝为底层的人提供医疗保险。',\n",
       " '为什么一个依靠占他人便宜来谋生，公开承认并未支付合理份额税收，并以避税为荣的人会成为大选最终赢家？',\n",
       " '唐纳德·特朗普（Donald Trump）抓住了时代精神：境况不佳，许多选民都想来点改变。',\n",
       " '但如今他们会意识到真相：再也没有什么常态可言，但也不会存在更多的不确定性。',\n",
       " '特朗普将实施何种政策仍然是未知之数，更遑论哪些政策将会成功或者后果将是什么了。',\n",
       " '特朗普似乎拼命想发起一场贸易战。',\n",
       " '那么中国和墨西哥将如何应对？',\n",
       " '特朗普清楚知道他的提议会违反世贸规则，但他估计也明白世贸组织需要很长时间来推翻这一做法。',\n",
       " '到那时候美国的贸易账户可能早就实现重新平衡了。',\n",
       " '但这个博弈可是双方面的：中国也可以采取类似但更为隐秘微妙的行动。',\n",
       " '那么如果真爆发一场贸易战的话，将会出现何种状况？',\n",
       " '特朗普可能觉得自己有把握能赢； 毕竟中国对向美国出口的依赖要强于美国对中国出口，这是美国的优势。',\n",
       " '但贸易战可不是一场此消彼长的零和游戏，美国也会输。',\n",
       " '中国的报复可能更具针对性，足以对美国造成严重的政治压力。',\n",
       " '而中国人可能比美国人处于更有利的地位来应对对方施加的危机。',\n",
       " '大家都在猜测谁的承受能力更强：是普通民众在泥潭里长期挣扎的美国？',\n",
       " '还是即便在困难时期也得以实现超过6％增长的中国？',\n",
       " '更宏观地看来，共和党/特朗普的议程——其税收削减甚至比标准的共和党方案更为偏袒富人——意味着这些政策都是基于涓滴繁荣（富人多赚钱消费从而穷人有工作）的理念，也是里根时代供给侧经济学政策的延续——但这一政策从未奏效过。 惹火的言论或者半夜三点在推特上发表的胡言乱语至少能在一段时间内安抚那些被里根革命所抛弃的愤世者。',\n",
       " '但这能维持要多久呢？',\n",
       " '随后又会是何种状况？',\n",
       " '特朗普可能喜欢颠覆常规的经济学法则，因为他乐于奉行自己那一套巫术经济学，但这是行不通的。',\n",
       " '而随着美国这个全球最大经济体在2017年及以后的年月进入一片未知的政治水域，除了指出一些显而易见的事实之外，任何人类的预测都是不智的：这片水域几乎肯定会激起风浪，而许多——如果不是大多数——预测专家的航船也将随之倾覆沉没。',\n",
       " '9/11和新独裁专制主义',\n",
       " '袭击纽约双子塔和华盛顿五角大楼五年后，“9/11”已经不再仅仅是一个日期了。',\n",
       " '它已经作为一个变革时代的某种开端或者是一个新的时代的开始而载入史册。',\n",
       " '恐怖分子在马德里和伦敦以及其他地方制造的爆炸事件也将被人铭记； 但是，只有“9/11”几乎像“1914年8月”那样成为了流行语。',\n",
       " '但是，那果真是一场开于2001年9月11日的战争吗？',\n",
       " '并不是所有人都赞同这一美国式的观点。',\n",
       " '当爱尔兰恐怖主义在英国猖獗之际，各届英国政府都竭力不屈从于爱尔兰共和军的观点，即正在进行一场战争。',\n",
       " '“战争”将会意味着把恐怖分子接受为合法的敌人，其含义与具有认同的交战规则的一场血腥战争的对等体相同。',\n",
       " '这既非对恐怖主义行为的正确描述，也不是有用的词汇。 恐怖分子应该更为准确地被称为罪犯。',\n",
       " '但是通过称之为战争、并把基地组织及其领导人本拉登点名为对手，美国政府给某些国内变化提供了理由。 这些国内变化在9/11袭击之前是不会被任何自由国家所接受的。',\n",
       " '这些变化的大多数都体现在所谓的“美国爱国者法案”上。',\n",
       " '尽管某些变化仅仅涉及到行政规定，但是，爱国者法案的整体效果却是侵蚀自由的支柱，例如人身保护，也就是无论何时国家剥夺个人自由，均有向独立法院寻求救助的权利。',\n",
       " '很久以来，古巴关塔那摩湾的战俘营成为了闻所未闻的象征，也就是不经审判逮捕被剥夺所有人权的“非法战斗人员”。',\n",
       " '如今，世界并不知道在多少地方还有多少更多的遭受非人待遇的人。',\n",
       " '对于其他所有人而言，好像是宣布了某种紧急状态，允许国家干涉基本民权。',\n",
       " '边境检查已经成为许多人的煎熬，警察起诉给很多人带来了负担。',\n",
       " '一种恐惧的气候让任何看似可疑或者行为可疑的人、特别是穆斯林生活困难。',\n",
       " '当采取这些限制自由的措施的时候并没有遇到很多公众的反对。',\n",
       " '相反，总体而言，那些措施的批评者，而非支持者发现遇到了麻烦。',\n",
       " '在英国，布莱尔首相全力支持美国的态度，政府引入了类似的措施，甚至提出了一个新的理论。',\n",
       " '布莱尔第一个主张安全是第一自由。',\n",
       " '换言之，自由并非是个人确定自身生活的权利，而是国家以安全的名义限制个人自由的权利。 只有国家才能确定安全的含义。',\n",
       " '这就是一个新的独裁主义的开始。',\n",
       " '这一问题存在于所有受到恐怖主义威胁的国家，尽管在许多国家中这已经变得非常具体。',\n",
       " '在大多数欧洲大陆国家，”9/11“已经成为一个美国人的日子。',\n",
       " '甚至还有人辩论，而且确实还有证据表明，参与到“反恐战争”之中是否实际上增加了恐怖主义行动的威胁。',\n",
       " '德国人当然在可能的情况下利用这一论点置身度外。',\n",
       " '但是，这一立场并没有防止德语中所说的忧虑的扩散。',\n",
       " '一种弥散性的焦虑正在扩大。',\n",
       " '人们感到不安，特别是在旅行的时候。',\n",
       " '任何一个火车事故或者飞机坠毁现在都首先被怀疑是恐怖主义行为。',\n",
       " '这样，9/11已经直接或间接地意味着一个巨大的震惊事件，在心理上以及对于政治体系而言都是如此。',\n",
       " '尽管以民主的名义与恐怖主义作战，实际上这一斗争由于官方的立法以及民众的忧虑而已经导致对民主明显的削弱。',\n",
       " '9/11攻击的令人担忧的特性之一就是除了罪犯们仇恨西方及其生活方式之外，很难看到他们的目的。',\n",
       " '但是西方社会的主要特征，即民主和法治却已经在捍卫者手中遭到了严重的损害，其程度超过远远超过其攻击者。',\n",
       " '最重要的是，需要采取两个步骤来在受到9/11后果影响的??主国家恢复对自由的信心。',\n",
       " '首先，我们必须确保应对恐怖主义挑战的相关立法是严格具有时限的。',\n",
       " '当今的某些对人身自由以及民权的限制含有限制其时效的废止条款。 所有这些法规都应当定期接受议会的重新审查。',\n",
       " '第二，也是更为重要的是，我们的领导人应当力求平息、而不是利用公众的焦虑。',\n",
       " '我们目前“交战”的恐怖分子无法获胜，因为他们的黑暗的观点永远不会赢得广泛的公众合法认可。',\n",
       " '这也就是民主派人士应当身先士卒、首要地以遵循我们的价值观的方式来捍卫它们的原因。',\n",
       " '9·11：十年后的盘点',\n",
       " '纽约——十年前，19名恐怖分子劫持了四架民航客机，其中两架撞上世贸中心双子塔，第三架撞击了五角大楼，第四架在乘客的反抗下无法完成袭击，坠毁于宾夕法尼亚旷野。',\n",
       " '只不过几个小时，3000多无辜平民被突如其来的暴行夺去了生命，其中大多是美国人，但也有来自其他115个国家的人。',\n",
       " '2001年9月11日，以任何标准看都是一个可怕的惨剧，但它并非一个历史转折点。',\n",
       " '它并未开启一个由全球恐怖主义主导国际关系的新时代，类似的大规模恐怖袭击也没有变成家常便饭。',\n",
       " '虽然人们在“全球反恐战争”上投注了大量精力，但是过去10年最重要的进展是信息技术的创新与传播、全球化、伊拉克与阿富汗战争以及中东的政治动荡。',\n",
       " '未来一个阶段的关键，则是美国整修其经济大厦的努力，中国在国内国际事务中的走向，各国政府在重振经济增长、控制核扩散以及应对能源与环境挑战方面的合作能力。',\n",
       " '如果把反对恐怖主义当成负责任的政府在世界上的核心要务，这是错误的。',\n",
       " '恐怖主义仍是边缘角色，它们的吸引力终归是有限的。',\n",
       " '它们只破坏，不建设。',\n",
       " '值得注意的是，人们走上开罗和大马士革街头，呼喊的并不是基地组织的口号，也没有对它的纲领表示支持。',\n",
       " '此外，种种措施已经成功地令恐怖分子退入守势。',\n",
       " '情报资源已重新配置，国境线已变得更安全，社会已变得更有弹性。',\n",
       " '反恐国际合作显著增强，部分是因为，各国政府都觉得需要在这一领域加强合作，尽管它们在其他领域颇有分歧。',\n",
       " '军事力量也扮演着某种角色。',\n",
       " '基地组织丧失了位于阿富汗的巢穴，正式因为向它提供庇护的塔利班政府被推翻。',\n",
       " '奥萨马·本·拉登最终被美国特种部队找到并击毙。',\n",
       " '军事无人机在击毙恐怖组织头目的行动中作用非凡。',\n",
       " '软弱的政府可以得到加强，纵容乃至支持恐怖主义的政府则必须为此负责。',\n",
       " '但进展不应被称作胜利。',\n",
       " '恐怖分子和恐怖主义不可能根除，就像我们不可能从地球上消灭疾病一样。',\n",
       " '总会有人试图对无辜者动用武力以追求其政治目标，不论是对男人、妇女还是儿童。',\n",
       " '事实上，恐怖分子在某些领域走在了前头。',\n",
       " '巴基斯坦仍在为基地组织和世界上其他一些最危险的恐怖分子提供庇护所。',\n",
       " '由于局势动荡、政府软弱和意识形态等等原因，也门、利比亚、索马里和尼日利亚也在沦为恐怖主义滋生的沃土，它们在那些地方组织、训练、实施行动，就像在十年前的阿富汗一样。',\n",
       " '旧的组织刚被摧毁，新的组织就冒出头来。',\n",
       " '本土恐怖主义的危险也在增长。',\n",
       " '我们在英国和美国都看到了这种迹象。',\n",
       " '互联网，这一现代西方世界的伟大发明，也被当成武器，激励并训练那些试图危害这个世界的人。',\n",
       " '2003年10月，时任美国国防部长拉姆斯菲尔德曾提出一个问题，至今仍很中肯：“我们每天俘虏、击毙、关押和改造过来的恐怖分子，能比伊斯兰学校和激进宗教人士每天招募、训练和派遣出来反对我们的人数多吗？',\n",
       " '”如果将一切等量齐观，那我们也许已经做到了。',\n",
       " '但恐怖分子的任何一点微小成功，对开放社会都意味着高昂的生命和金钱成本，从这个意义上说，我??得不偿失。',\n",
       " '那我们该怎么办？',\n",
       " '可惜没有一蹴而就的办法。',\n",
       " '巴勒斯坦建国还是不够，因为恐怖分子想要消灭犹太人的国家； 在克什米尔问题上达成妥协，也无法满足巴基斯坦恐怖分子对印度更大的领土要求。',\n",
       " '减少失业当然是件好事，但许多恐怖分子并非穷人出身。',\n",
       " '帮助中东和其他地方的社会变得更民主有助于缓解边缘化作用，遏制极端主义，但这说起来比做起来容易。',\n",
       " '当然，我们需要继续想办法，让我们变强，让恐怖分子变弱。',\n",
       " '但最重要的是阻止恐怖主义以任何方式进行传播，尤其是在阿拉伯和伊斯兰社会。',\n",
       " '2009年圣诞节，尼日利亚的一位父亲向美国驻拉各斯大使馆警告说，他担心他的儿子会做出可怕的事情来，从而阻止了后者在飞往底特律的航班上引爆一枚炸弹，这就是一个很好的例子。',\n",
       " '只有当更多的父母、教师和社会领袖以这样的方式行动起来，才能令恐怖分子的来源枯竭，执法部门才能获得民众的全力配合。',\n",
       " '在那些传统上支持或纵容恐怖主义的地方，恐怖主义必须先丧失其合法性，才会进一步丧失能量。',\n",
       " '所有人的跨大西洋贸易',\n",
       " '华盛顿—欧盟和美国之间关于成立跨大西洋贸易和投资合作伙伴（TTIP）的谈判受到了广泛的欢迎。',\n",
       " '英国首相卡梅伦将TTIP称为“一代人只有一次的机会”，认为它将给欧盟和美国各带来800亿英镑的潜在收益，给世界其他地区带来850亿英镑的潜在收益。',\n",
       " '对于一个等待世贸组织冗长的多哈回合最后结果已经不耐烦的世界来说，即使是双边计划也像是天降恩惠，特别是在《金融时报》最近所指出的“双边”关系包括了世界经济半壁江山的当下。',\n",
       " '但风险也是巨大的：TTIP协议可能伤害发展中国家的出口，除非欧盟和美国携手保护这些行动方的利益。',\n",
       " '计划中的契约引起最多兴奋情绪的部分——其对强制性产品标准等监管壁垒的强调——实际上最应该引起关注。',\n",
       " '欧盟和美国关税很低——平均不到5%——因此进一步的优惠降税并不会严重影响其他国家。',\n",
       " '但是，在标准问题上——比如涉及安全、健康和环境的标准——市场准入标准是严酷且二分的：要么满足规定的标准，要么别来这里卖东西。',\n",
       " '其结果是第三国企业的选择将取决于TTIP标准如何设置：通过协调（采取共同标准）还是通过互相承认（接受满足彼此规定标准的商品和服务）。',\n",
       " '第一种选择将让世界各地的生产者都能够利用规模经济的优势。',\n",
       " '但是，在一些场合，协调后的标准???能比某些国家原先的标准更加苛刻。',\n",
       " '尽管新标准将适用于所有出口国供应商，但合规成本通常各不相同，这意味着更难以满足高标准的企业会遭受损失。',\n",
       " '20世纪90年代末，欧盟决定协调黄曲霉素标准（黄曲霉素是某些霉菌产生的有毒成分族），八个成员国——包括意大利、荷兰和西班牙——大幅度提高了国家标准，这或许就是非洲国家队欧洲的谷物、水果干和坚果对欧出口下降6. 7亿美元的原因。',\n",
       " '若采取互相承认，那么欧盟和美国将承认彼此的标准或合规评估程序，企业会在各种标准中选择不太苛刻的那种。',\n",
       " '如果该政策扩展到第三国企业，就会产生极大的自由化作用。',\n",
       " '比如，马来西亚电视生产商可能选择符合（比如）美国较易符合的安全标准，然后在欧美两地销售同样的产品，既享受了规模经济的好处，又降低了合规成本。',\n",
       " '但是，如果TTIP将第三国企业排除在互相承认政策之外，那么它们相对欧洲和美股公司的竞争力将受到极大的削弱。',\n",
       " '事实上，我们的研究表明，若互相承诺协议包括严格的来源国限制，则地区内贸易将增加，与其他国家的贸易会削弱，发展中国家损失最大。',\n",
       " '事实上，欧盟此前的不少承认协议（比如在专业服务标准方面）表明，过度约束的来源地规则很有问题大有问题。',\n",
       " '尽管获准在葡萄牙出售的巴西橙可以在整个欧盟出售，但获得葡萄牙执照的巴西工程师和会计师仍必须满足欧盟其他地区各自的执业要求，这迫使非欧洲工人忍受费时费力的官僚主义流程，不利于急需的劳动力流动。',\n",
       " '此外，在关税和标准方面，世贸组织规则并不平等。',\n",
       " '尽管世贸组织规则保护着被双边和地区关税协定排除在外的国家，从而确保一体化市场不会获得其他附加的优势，但保护第三国免受强制性标准协议影响的保护措施几乎不存在。',\n",
       " '即使没有国际法，欧盟和美国也可以采取两个动作确保TTIP不会损害发展中经济体。',\n",
       " '首先，它们可以同意不实施严格的来源地规则，这将使所有国家都能享受到双边互相承认协议的好处。',\n",
       " '其次，在考虑协调的领域，它们可以倾向于原始标准中较不苛刻的一个，除非有充分的证据表明这样做不利于相关监管目标。',\n",
       " '这类似于世贸组织的脱离现行国际标准测试。',\n",
       " '如果欧盟和美国实施了这两点，世界其他国家将可以带着希望而不是恐惧追踪TTIP。',\n",
       " '中美贸易不平衡的平衡观',\n",
       " '发自北京——在2007年7月以前，大多数经济学家都认为全球增长的最重大威胁来自于全球贸易不平衡。',\n",
       " '他们觉得美国净外债与GDP比率的不断上升——这是长期经常账户赤字所引发的后果——将导致资金流入急剧减少，随之削弱美元，而后利率上涨，并最终令美国经济陷入危机当中。',\n",
       " '但这一设想却未能成为现实，事实上本次危机源自于美国次级贷款的崩盘，并当即将全球经济拖入了一场自1930年代以来最为严重的衰退当中。',\n",
       " '而大多数经济学家之所以无法预见到危机背后的真正经济驱动力，是因为他们对美国总债务的快速增长缺乏足够的关注。',\n",
       " '实际上他们把注意力都放在了美国的外债上，却忽视了家庭负债（抵押贷款和消费者债务）、公共负债，商业债务和金融债务等方面的问题。',\n",
       " '他们尤其应当对美国抵押贷款和消费者债务的可持续性给予更多的关注。',\n",
       " '在2007年，上述两项负债与GDP的比率已经上升到90%以上，而与此同时净外债的相关比率只有24%。',\n",
       " '当然，债务的不同组成部分也因各自的特性和资金来源而大不相同——而其可持续性也是如此。',\n",
       " '但一国总债务的各个组成部分及其资助方式之间显然是有所关联的。',\n",
       " '这意味着两点，首先：来自不同融资来源的资金在某种程度上是可互换的：总债务中某项资金出现的赤字可以用原本提供给其他用途的多余资金来填补。',\n",
       " '其二，总债务中某一部分所导致的问题将对其他部分造成影响。',\n",
       " '在次贷危机爆发之后，美国家庭只能用储蓄来支付抵押贷款和消费者债务，或者被迫放弃偿还。',\n",
       " '美国总债务的下降，以及总债务和国内资金之间财政缺口的缩窄，都使得美国经常账户赤字状况在2008-2009财年大大改善，也证明美联储主席伯南克宣称该赤字是由全球“储蓄偏好”所引发的观点是错误的。',\n",
       " '事实上尽管美元在避险需求下升值，美国经常账户状况却得到了巩固。',\n",
       " '但不幸的是，随着私人部门去杠杆化以及家庭储蓄的增加，由负债和消费驱动的美国经济陷入了衰退。',\n",
       " '为了减少私人部门去杠杆化对经济增长的负面影响，美国政府采取了一系列扩张性的货币和财政政策。',\n",
       " '如今政府大力干预后的家庭负债状况依然处于危险边缘，同时财政状况急剧恶化，经常账户平衡状况也再度变差。',\n",
       " '公共债务的可持续性已经取代私人债务成为美国金融稳定的最大问题，而针对美国经常账户状况的争论焦点也从外债的可持续性转移到外部赤字降低对增长和就业的影响。',\n",
       " '对此美国的政策制定者目前陷入了困境：一方面要促进增长，另一方面又得降低总债务水平。',\n",
       " '而实现这两大目标的最重要方式就是透过提升美国国家竞争力的方式来增加出口。',\n",
       " '但这些竞争力的提升又从何而来？ ，',\n",
       " '将美元贬值的做法能在短期内提升美国的竞争力，但却非长久之策。',\n",
       " '因为财政状况的迅速恶化已经令投资者们对自己美国政府债券上的资本损失忧虑不已，此外贬值也将使外国人更不愿意资助美国的预算赤字。',\n",
       " '如果得不到外国资金，美国政府债务的收益率将被迫提升而美国经济也将重新陷入衰退。',\n",
       " '长期来说，美国的发展模式必须由原本对负债和消费的依赖转向美国人引以为荣的创新能力。',\n",
       " '只有这样美国才能真正提升其竞争力，并使政府可以在保持合理增长率的同时将私人和公共债务都降低到一个可持续的水平上。',\n",
       " '但提升竞争力和减少总债务这两个目标都不可能一蹴而就。',\n",
       " '无论对其拥有双边贸易顺差的是哪一个国家，美国的经常账户赤字将在短期内继续存在。',\n",
       " '因此中国将其贸易盈余持续投资在美国政府债券上的行为对美国的经济增长和金融稳定都是极为重要的。',\n",
       " '考虑到美国在中国购买美国国债方面获取的重大利益，我们很难理解美国政府和国会为何总是对中美双边经常账户赤字大肆抱怨。',\n",
       " '而于此同时，我们也难以解释中国为何不愿减少其双边贸易盈余，因为大规模持有美国政府债券回报极低，而且还存在着巨额资本流失风险。',\n",
       " '但好消息是，在中国国家主席胡锦涛最近的访美之行中，中美两国都在解决双边贸易不平衡方面做出了积极的举措。',\n",
       " '这预示着一个更加理性也更富建设性的中美全球贸易不平衡对话，而这也将对全球经济有所裨益。',\n",
       " '银行联盟第一步',\n",
       " '布鲁塞尔—金融危机开始时，人们用查尔斯·古德哈特（Charles Goodhart）的名言来描述银行：“生而国际，死而民族。',\n",
       " '”当是时（2008—2009年），国际大银行在深陷困境时不得不由母国政府出手相救。',\n",
       " '但如今，欧洲的问题与此正好相反，银行“生而民族，死而欧洲”。',\n",
       " '比如，在西班牙，地方储蓄银行是超级地产繁荣的融资提供者。',\n",
       " '随着繁荣变为萧条，由此产生的损失已快要超过了西班牙整个国家的能力，这一问题变成了欧洲问题，因为事关欧元存亡。',\n",
       " '西班牙的状况是一个更大问题的症状。',\n",
       " '国家层面的监管者总是试图最小化国内问题。',\n",
       " '他们的本能（也是他们官僚主义利益之所在）是捍卫“国宝”（national champion）银行在国外的表现。',\n",
       " '但他们对辨认国内问题的抵触更加严重。',\n",
       " '直到最近，西班牙当局才不再坚持该国房地产部门问题不是暂时性的。',\n",
       " '承认真相意味着多年来他们忽略了不可持续建设繁荣的风险，以至于如今整个国家都陷入了破产危险。',\n",
       " '在爱尔兰，最初的情况与西班牙并没有太大不同。',\n",
       " '当问题开始浮现时，时任财政部长宣称爱尔兰将实行“史上成本最小的银行救援”。',\n",
       " '不难预料，国家级监管者总是倾向于对国内问题视而不见，因此，清算资不抵债银行的成本应该由各国自行承担似乎是自然而然的事情。',\n",
       " '于是，即使是在欧元区，银行监管大体上仍维持各国各自为政的局面似乎也不无道理。',\n",
       " '新近成立的欧洲银行局所有用的权限比日常工作主要根据国内情况进行的国家级监管者大不了多少。',\n",
       " '但现实已经证明，这条道路走不通。',\n",
       " '问题也许肇始于国家层面，但是，由于货币联盟的存在，它们很快会威胁到整个欧元区银行系统的稳定性。',\n",
       " '在6月峰会上，欧洲领导人最终意识到修正这一情况、将欧元区银行监管职能移交给欧洲央行的重要性。',\n",
       " '作为货币联盟，金融一体化在欧元区非常牢固，这使得欧洲央行成了不二之选。',\n",
       " '此外，欧洲央行事实上早已承担起了维持欧元区银行系统稳定的职能。',\n",
       " '但是，迄今为止，欧洲央行仍必须在无法判断银行稳健程度的情况下向它们大规模贷款，因为所有信息都掌握在国家级监管者手中，而后者对此死抓不放，出了问题总是拼命掩盖，直到无法收场为止。',\n",
       " '让欧洲央行来行使此职能还有助于阻止正在潜滋暗长的去一体化过程，该过程尽管还没有进入公众视野，但已成为现实威胁。',\n",
       " '只消问问坐落于受困欧元区国家的任何大型国际银行集团总部便可推知一二。',\n",
       " '就拿总部设在意大利的某家一行来说吧，这家银行在德国设有一家重要分行。',\n",
       " '德国分行自然手握资金盈余（因为平均而言德国储蓄远高于投资）。',\n",
       " '总行希望使用这笔资金增强整个集团的流动性。',\n",
       " '但德国监管当局认为意大利存在风险，因此反对资金转移。',\n",
       " '母国（意大利）监管者的利益正好与此相反。',\n",
       " '它希望看到“内部资本市场”操作，越多越好。',\n",
       " '在这里，让欧洲央行来充当尊重这些对立利益的中立仲裁者才是合适的。',\n",
       " '但是，尽管让欧洲央行行使银行监管职能可???解决问题，但也会造成另一个问题：国家级监管当局是否需要继续对其不再行使监管职能的储蓄银行负责？',\n",
       " '经济（和政治）逻辑意味着很快欧元区又需要一个共同银行援助基金。',\n",
       " '这一需要尚未被正式承认。',\n",
       " '但此乃欧洲一体化进程中的典型做法：在某领域出台的不完整措施要求在相关领域出台后续措施。',\n",
       " '这一增量办法在过去效果甚丰； 事实上，今天的欧盟就是这样产生的。',\n",
       " '但金融危机使得决策者不再像过去那样有时间向选民解释为什么一项措施会带来另一项措施。',\n",
       " '他们需要大大加快动作不乏，这样才能拯救欧元。',\n",
       " '安倍经济学的新一轮承诺',\n",
       " '发自东京——日本自民党在今年12月14日的国会大选中取得了决定性的胜利，因为日本选民们一面倒地支持首相安倍晋三的宏观经济政策议程。',\n",
       " '虽然选民投票率还是相对较低——这在很大程度上源自于这类事务的某种技术性本质——但这场选举所传达出来的信息也非常清晰：大多数日本人都不愿意回到“安倍经济学”之前那种低迷经济轨道中去。',\n",
       " '当安倍经济学的第一支箭——财政刺激计划——在两年前射出之后，资产市场立刻作出了积极回应。',\n",
       " '而安倍经济学的第二支箭——货币宽松——则强化了这一效应。',\n",
       " '在过去两年中，日本股票市场的市值几乎翻了一倍，使消费者的财富得到了增长。',\n",
       " '此外，日元对美元贬值了近三分之一，从原来的约80日元兑1美元下降到近120日元，令日本的出口工业蓬勃发展。',\n",
       " '更令人鼓舞的则是劳动力市场的变化发展，跟资产市场不同的是，这反映出了结果，而非仅仅是预期。',\n",
       " '在这方面形势也十分喜人。',\n",
       " '劳动力市场得到了巩固，失业率维持在3. 5%，而职位和求职者的比率则高于1。',\n",
       " '无可否认，这其中还存在的一些挫折：日本的GDP在第二和第三季度出现了收缩。',\n",
       " '但这一由于四月份提升消费税（从5%提到8%）所导致的下滑状况不能归罪于安倍经济学。',\n",
       " '事实上，安倍只是在遵从由上届日本民主党政府所通过的一项法律而已。',\n",
       " '安倍经济学的头两支箭都旨在刺激需求——而且也极为有效。',\n",
       " '同时也需要提升消费税来维持这两支箭的顺利飞行，但不幸的是，此次提升的幅度过大，反而对这两项政策造成了阻碍。',\n",
       " '好消息是增税的影响只是暂时的，并将很快开始逐渐消失，而工业产量则会接近满负荷。',\n",
       " '当需求开始超过供给的时候，需求端的刺激政策将逐渐变得无效，是时候射出安倍经济学的第三支箭：强化增长的结构性改革。',\n",
       " '为了提升生产力增长并改善日本的经济竞争力，这类改革极有必要。',\n",
       " '而其中则凸显出四项要点：',\n",
       " '首要任务应当是消除——或至少要减少——遏制经济活力的一大堆政府管制。',\n",
       " '现行的系统是如此的繁琐复杂，以致在东京开设一所新医学院需要花费超过30年时间。',\n",
       " '同样，飞往羽田机场这个通往东京市区的便利转接点的航班也受到限制。',\n",
       " '这显然不是一个促进长期经济繁荣的做法。',\n",
       " '此外，日本政府应当加紧完成跨太平洋伙伴计划的谈判。 目前已经有十二个国家参与该项谈判，涵盖从墨西哥到美国再到越南的广大区域。',\n",
       " '这一伙伴计划极大提升日本的贸易前景，包括像在农业这样的敏感部门，因为像鲜花和蔬菜之类快速消费品的出口将因此而受益。',\n",
       " '日本领导人也必须扩大由于本国的快速老龄化而面临严重制约的劳动力队伍。',\n",
       " '在缺乏大规模移民，日本又不准备开放移民的情况下，一个相对简单的解决方案将是把更多的女性融入劳动力大军。',\n",
       " '如果日本的女性劳动力参与率能提升10%——这是一个完全可以实现的目标——那就将把整体的劳动力参与率提升近5%。',\n",
       " '最后安倍政府必须削减企业税率，使其与国际标准接轨。',\n",
       " '在国际间日渐激烈的外国投资争夺战中，通过驱使企业将自身的巨额现金储备投入到更具生产力的活动当中，减少企业税率将在事实上增加日本的税收收入。',\n",
       " '如今安倍政府已经获得了日本选民的新一轮授权，也必须兑现自己的承诺——这意味着坚定而全面地实施结构改革。',\n",
       " '这当然需要作出某些牺牲。',\n",
       " '事实上日本家庭已经在承受着因为消费税提升而带来的艰难状况。',\n",
       " '而安倍政府的下一步则是利用自身的政治资本去制服既得利益阶层——包括政界与商界。',\n",
       " '这意味着迫使企业放弃某些目前享有的特殊税务优惠。',\n",
       " '政治家则必须参与到纳税人识别体系中来。',\n",
       " '而政府官员们则要放弃路管制所赋予他们的某些权利。',\n",
       " '如果上述这些团体能和日本民众一道接受和承担合理的牺牲，安倍政府就能兑现自身的承诺并建立一个兴旺繁荣的经济。',\n",
       " '为了日本的整体利益——不要说一个需要新动力来源的世界经济——这个承诺值得去兑现。',\n",
       " '安倍经济学的迷失之箭',\n",
       " '东京—日本首相安倍晋三在2012年执政后不久就开始了振兴日本经济的政策，他引入了大规模财政刺激和激进的货币宽松计划。',\n",
       " '此后，日本决策者一直致力于启动安倍所谓的第三支“箭”：重要行业的艰巨改革计划以及破除增长的结构性壁垒。',\n",
       " '但专注于公共政策让“第四支箭”——私人部门——被冷落一旁，似乎遭到了忽略。',\n",
       " '这是不幸的，因为政府无法单枪匹马治好日本病。',\n",
       " '年生产率增长极其低迷，在过去20年中的大部分时间里还不到2%，这表明机会被错失，成本竞争力在下降。',\n",
       " '日本生产率大降是整个经济的普遍现象； 几乎所有部门的劳动和资本生产率都近乎停滞——即使是日本标志性的发达的制造业。',\n",
       " '比如，运输设备劳动生产率大约只有德国的一半。',\n",
       " '这一趋势意味着到2025年，日本年GDP增长率平均将只有1. 3%，连续第三个十年陷入停滞。',\n",
       " '这一结果恰逢不利的人口趋势——并将放大后者的效应——制约财政收入，增加全民医疗和退休金福利的成本。',\n",
       " '日本能否摆脱这一轨道取决于个体公司决定投资、改变工作场所政策、采用新技术以及尝试未尝试的业务模式。',\n",
       " '安倍的结构改革需要时间，还需要实施的政治意愿，但日本公司决不能坐等。',\n",
       " '它们能够也必须行动起来，不必等政府改变政策。',\n",
       " '在许多情形中，经济瓶颈并非监管，而是来自根深蒂固的生意经。',\n",
       " '麦肯锡全球研究所（McKinsey Global Institute）的新研究详细考察了日本发达的制造业、零售业、金融服务业和医疗业——发现这些领域无不存在巨大的未利用生产率潜力。',\n",
       " '首先，日本企业必须更加融入全球。',\n",
       " '向增长最快的海外市场出口显然是克服国内需求增长停滞的一个办法。',\n",
       " '但是，日本企业所需要的不仅仅是将产品卖到海外，还需要扩大海外经营，组织更广阔的国际人才网。',\n",
       " '日本公司拥有可怕的研发部门，但大部分需要重组以获得更好的回报和影响。',\n",
       " '这一过程必须首先理解客户想要什么并决心据此提供解决方案。',\n",
       " '管理严格的封闭研发业务必须变得更具流动性、更加开放，把客户和供应商也包括进来。',\n",
       " '日本公司还需要提高营销、定价和人才开发等领域的能力。',\n",
       " '尽管在这些领域表现出众的日本企业不少，但大部分仍严重缺乏这方面的能力。',\n",
       " '要想在全球市场上竞争，它们需要实现与传统优势领域同样的一致性。',\n",
       " '许多日本公司至今仍未实现纸面流程的数字化和更新过时的信息技术系统。',\n",
       " '其他公司可以跨越基本数字化，向新一代技术（如大数据分析）迈进，并从中获益。',\n",
       " '公司还可以用智能软件系统和机器人解决劳动力短缺的问题。',\n",
       " '制造商可以用物联网和3D打印等技术扩大或代替流水线。',\n",
       " '更广泛地说，日本公司必须以业绩和纪律为目标进行组织。',\n",
       " '随着政策变化解放市场力量，企业将面临更大的竞争。',\n",
       " '一些企业也许需要重组或退出无利可图的市场； 令一些企业也许必须进行兼并收购以实现规模经济。',\n",
       " '最后，股东和高管应该将业绩目标与激励挂钩。',\n",
       " '一些日本大公司以开始将基于传统年资的晋升模式改变为基于表现的薪酬结构。',\n",
       " '其他公司应该效仿。',\n",
       " '提拔更年轻、更多样化的人才能够用新思维为组织注入活力。',\n",
       " '如果日本私人部门奋起迎接挑战，可以推动经济走上更快的增长路径。',\n",
       " '一家公司的创新奖迫使竞争者更加投入竞争，从而提升整个行业。',\n",
       " '比如，20世纪五六十年代，丰田引入了效率更高的生产流程，并最终被整个汽车行业所采用。',\n",
       " '日本不应该满足于1. 3%的未来年GDP增长率，它可以到2025年实现大约3%的年增长率。',\n",
       " '这样做要求劳动力生产率翻一番以上，但仍是可实现的目标。',\n",
       " '一大半增长增量可以通过采取全球公司已在使用的最佳实践实现，而技术可以填补大部分余下的空缺。',\n",
       " '日本商界领袖需要将大局观和细节专注结合起来。',\n",
       " '他们需要创造创新性产品，渗透新市场，积极地投资于设备、技术和人才，同时仔细检视经营的每个方面以发现低效和浪费。',\n",
       " '传统生意经必须抛弃。',\n",
       " '但取得进步、刺激更快经济增长的空间很大。',\n",
       " '巨大的贸易流、新兴世界数十亿新消费者的崛起以及技术突破正在快速改变全球经济。',\n",
       " '日本可以将这一破坏潮转化为机遇，摆脱当前的轨迹。',\n",
       " '对俄罗斯的单恋',\n",
       " '新德里—日本首相安倍晋三孜孜不倦地讨好俄罗斯总统普京，在四年内和他进行了十多次会晤。',\n",
       " '这个月，他在东京和家乡山口县（以温泉闻名）接待了普京。',\n",
       " '但安倍的追求没有给日本带来什么好处，倒是给俄罗斯送去了很多利益。',\n",
       " '安倍对普京的外交姿态是他的战略大局的一部分：将日本定位为中国的制衡力量和亚洲的再平衡力量。 日本、俄罗斯、中国和印度组成了一个亚洲战略四边形。',\n",
       " '安倍已经建立了与印度的紧密关系，并将改善对俄关系——二战后日本从未与俄罗斯媾和——视为地区实力平衡的缺失的一角。',\n",
       " '但安倍与俄罗斯构建信任的努力不仅是为了制衡中国的野心。',\n",
       " '他还希望俄罗斯归还千岛群岛最南端拥有丰富资源的北方四岛。 1945年美国在广岛和长期投下原子弹后，苏联占领了这四个岛屿。',\n",
       " '作为交换，安倍提出向俄罗斯提供经济援助、投资被忽视的远东地区以及能源大单。',\n",
       " '但是，安倍面临着多重阻碍。',\n",
       " '首先，日本是以美国为首的对俄制裁的参与者。',\n",
       " '制裁自2014年3月俄罗斯入侵克里米亚后开始实施，这让俄罗斯与其传统对手中国越走越近； 而普京公开表示制裁是日俄缔结和平条约的障碍。',\n",
       " '作为对安倍的姿态的回应，普京一直奉行强硬的讨价还价。',\n",
       " '在这样的背景下，安倍在“温泉峰会”解决领土争议的希望破灭，而俄罗斯带着68份新商业协议回国，这一结果一点都不令人惊讶。',\n",
       " '新协议中不少是象征性的，但也有一些是实质性的，包括一份价值25亿美元、成立一个10亿美元双边投资基金的协议。',\n",
       " '在后一份协议中，日本和俄罗斯将建立一个“特殊框架”管理争议岛屿上的经济活动。',\n",
       " '但该计划已经遇到了麻烦。',\n",
       " '俄罗斯政府远东投资和出口局（Far East Investment and Export Agency）主管彼得·舍拉哈耶夫（Peter Shelakhaev）表示建立这一框架存在法律障碍，在南千岛群岛经营的日本企业必须向俄罗斯纳税。',\n",
       " '但是，如果日本照办，就等于承认了俄罗斯对这些岛屿的司法管辖权。',\n",
       " '因此，安倍想要的东西被拒绝了，而普京成功地缓解了俄罗斯的孤立。',\n",
       " '安倍是俄罗斯吞并克里米亚后第一位与普京举行峰会的G7领导人，眼下俄罗斯赢得了日本的经济合作。',\n",
       " '日本是唯一一个与俄罗斯存在领土纠纷的G7国家，并且它显然比克里姆林宫更渴望达成协议。',\n",
       " '但这正中俄罗斯下怀。',\n",
       " '日本立场有所软化，并且释放出信号可以接受局部归还争议岛屿，而俄罗斯的立场变得更加强硬。',\n",
       " '最新峰会后，安倍透露普京似乎否1956年日本与苏联签订的协议，该协议规定北方四岛中较小的两个将在日俄缔结和平条约后归还日本。',\n",
       " '今年是这一联合声明签署60周年，当时，这份声明被广泛视为是一个突破。',\n",
       " '如今，克里姆林宫表示它是否会兑现声明的条件是日本不加入针对俄罗斯的安全联盟。',\n",
       " '普京还表示担心，1960年日美安全条约范围将扩大到归还的两岛，从而美国也能够在那里部署军事设施。',\n",
       " '日本没有为俄罗斯解决担忧的立场。',\n",
       " '它不能选择跳出美国领导的制裁； 也不能让争议的北方四岛不受日美安全条约的管辖，特别是在如今日本一直在敦促美国就捍卫日本控制、中国主张主权的钓鱼岛做出明确承诺的时候。',\n",
       " '至于普京，他似乎对他的谈判立场相当满意。',\n",
       " '在温泉峰会上，他迟到了三小时，延续了他让外国领导人等他的习惯； 还拒绝了日本政府的礼物——为日本2012年送给他的日本秋田犬配一只雄性伴侣。',\n",
       " '如今，安倍为讨好普京而掷下的政治资本基本上是打了水漂。',\n",
       " '而日本的困境反而更加深化了。',\n",
       " '美国当选总统唐纳德·特朗普准备缓和对俄关系或许有利于安倍继续讨好普京； 但如果俄罗斯得到了美国，就再也不需要日本了。',\n",
       " '柏林共识?',\n",
       " '发自香港——最近的柏林之行令我回想起了自己的上一次到访，那是1967年的夏天，作为一个穷学生的我，在那堵还将继续分割和压抑整个德国社会20多年的高墙前震惊不已。',\n",
       " '如今在德国人民的艰苦努力以及为统一做出的牺牲之下，柏林早已生机勃勃，重现往日活力，此外新经济思维研究中心（Institute for New Economic Thinking）会议适当的议题设置对此也有贡献，而本人正是来参加这场会议的。',\n",
       " '会议的主题是“范式的失落”，超过300位经济学家，政治科学家，系统分析者和生态学者齐聚一堂，讨论当今日渐恶化的不平等状况、失业率升高，全球金融紊乱和气候变化等问题所引发的挑战以及不确定性，并重新思考与之对应的经济和政治理论。',\n",
       " '几乎所有与会代表都赞同一点，那就是新古典主义经济学的旧有范式已经失效，但在何种范式可以取而代之这一点上却未能达成共识。',\n",
       " '诺贝尔经济学奖得主阿马蒂亚·森（Amartya Sen）在会上指出，欧洲的危机源自于四个方面的失效——政治、经济，社会和智力层面。',\n",
       " '这场全球金融危机——始于2007年美国次级贷危机并最终扩大成为欧洲主权债务（及银行）危机——提出了一些我们无法回答的问题，而其原因则是知识的过度专业化和碎片化。',\n",
       " '同时也无可否认，我们的世界已经变得极端复杂，以致无法用任何简单且高度概括性的理论去解释在经济、技术，人口构成以及环境方面的所发生的复杂转变。',\n",
       " '尤其重要的是，新兴市场的崛起使西方的演绎和归纳逻辑遭到了挑战。',\n",
       " '演绎推理使我们在知道原理（法则）和起因的情况下可以预测出结果。',\n",
       " '而通过使用归纳法，我们则可以在知道前因后果的情况下推断出中间的原理。',\n",
       " '相比之下，东方的思考方式则是溯因式的，从实用主义出发去猜测下一步该如何行动。',\n",
       " '溯因推理注重实效，只关注结果，通过结果来猜想原理，再由此确定原因。',\n",
       " '以历史为例，社会-科学理论是由胜利者撰写的，同时也受到特定时代背景和当时（社会主要）矛盾的影响。',\n",
       " '自由市场理念演化自盎格鲁-撒克逊理论家（其中许多来自苏格兰），他们不断移居和殖民新的地域，让那些拥有财富的个人觉得消费是会无限增长的。',\n",
       " '而欧洲大陆的思想则主要服务于城市化以及对社会秩序的追求，因此强调对政治经济的体制性分析。',\n",
       " '因此，新古典主义经济学在19世纪的崛起在很大程度上都是受到了牛顿和笛卡尔流派物理学的影响，从定性分析转向对人类行为的定量分析，假设这些行为都是理性的同时不断对不确定性加以排除。',\n",
       " '这一“预先设定的均衡”理念——认为市场总能自我纠正——导致了政策瘫痪，直到大萧条来袭，而约翰·梅纳德·凯恩斯（John Maynard Keynes）则因此提出政府应当出手干涉失业问题，而产出与消费之间的差距问题也得到了重视。',\n",
       " '到了1970年代，新古典主义总体均衡学派通过一个假设“金融只是一张包装纸”的实体部门模型说服了凯恩斯主义者，并因此令他们对金融市场的不稳定作用一无所知。',\n",
       " '海曼·明斯基（Hyman Minsky）这样的经济学家尝试去纠正这一错误，但当时领导经济学界的是米尔顿·弗里德曼等自由市场和最小政府的鼓吹者，于是明斯基之辈也只能默默无闻了。',\n",
       " '但随后出现的科技，人口分布特征和全球化则令新古典经济学派迎来了意料之外的挑战。',\n",
       " '随着全球发达国家通过金融衍生品提供的杠杆来实现过度消费，世界70亿人口中有40亿开始进入中等收入状态，不但对全球资源造成了巨大需求，也催生了环境生态的可持续发展问题。',\n",
       " '我们需要新的思维来应对这些大规模且系统性的变革需要，同时也需要把中国和印度这样的大国整合进入现代世界。',\n",
       " '不但西方需要转换思维，东方也是如此。',\n",
       " '为此历史学家黄仁宇早在1987年就针对中国进行了论述：',\n",
       " '&#160;“随着世界进入现代时期，大多数承受着内部和外部压力的国家都必须对自身进行重建，用基于商业的一套法则来取代原???构建于农业经验之上的管治模式……但这是件知易行难的事。',\n",
       " '整个更新过程将影响到上下两个阶层，并不可避免地需要重新修复两者之间的体制联系。',\n",
       " '全面破坏将成为常态； 而且往往需要数十年来完成这项工作。',\n",
       " '利用微历史的框架，我们可以看到一个新的，多极化全球体系之中存在着许多不断产生互动的复杂结构，而日本通胀，欧洲债务甚至阿拉伯之春则可被视为这些结构内部某一阶段的系统性变革。',\n",
       " '我们正见证着各种同时进行中的全球收敛现象（各国之间收入，财富和知识差距的不断收窄）以及本地分化现象（各国内部收入，财富和知识差距的不断加大）。',\n",
       " '适应性系统在演化的过程中会不断在秩序和创新性之间左右摇摆。',\n",
       " '正如英国哲学家伯特兰·罗素（Bertrand Russell）所预见的那样：“安全与正义要求集权化的政府控制，而这个系统如果要达到有效的话就势必要扩张成为一个全球政府。',\n",
       " '相反，进步则要求实现一个与社会秩序相容的，最大范围的个人主动性。 ”',\n",
       " '一个被经济学家约瑟夫·熊彼得（Joseph Schumpeter）称之为“创造性破坏”的新浪潮已经涌现：即便中央银行奋力通过向市场注入大量流动性来保持稳定，对企业和家庭的信贷依然不断紧缩。 我们生活在一个对通胀和通缩的两大恐惧同时并存的时代；',\n",
       " '既有前所未有的繁荣，又有不断扩大的不平等； 一方面科技进步，另一方面则存在资源枯竭。',\n",
       " '与此同时，现有的政治系统既不愿意做出牺牲，却又承诺会提供好工作，优秀管治，可持续环境和社会和谐——这是一个自私自利的搭便车者梦想的天堂，但代价则是牺牲了自然环境以及子孙后代的福祉。',\n",
       " '我们不能通过印钞的方式永远推迟承担适应变革所带来的阵痛。',\n",
       " '只有当既得利益者愿意为非既得利益者做出牺牲的时候，可持续性才可能实现。',\n",
       " '针对发展中国国家自由市场改革的华盛顿共识已经在20年前达成。',\n",
       " '而柏林新经济思维研究中心会议则显示我们需要一个新的共识——为了团结统一而做出牺牲。',\n",
       " '欧洲当采用之。',\n",
       " '接受日本的用词',\n",
       " '东京—近几年来，访日游客数量迅速增长，去年创出1,340万人次新高，比2013年增加29%。',\n",
       " '日本似乎正在大踏步向重新成为亚洲文化中心的目标迈进。 一百年前，也就是印度诺贝尔奖诗人泰戈尔生活在东京的时候，日本曾经是亚洲文化中心。',\n",
       " '中国革命领袖孙中山和蒋介石以及其他诸多亚洲名人也纷纷来到日本。',\n",
       " '如今，所有来日旅客都会学会两个关键词：“多末”，意思是“你好”、“谢谢”或“很好”； 以及“斯米马赛”，它包括了domo的全部含义，还可以表达“对不起”和“劳驾”。',\n",
       " '普通日本人每天都要说无数次“斯米马赛”，有一点点事情或犯一点点错误就用这个词向朋友或陌生人致歉。',\n",
       " '但是，自二战以来，日本领导人真切地感受到，向其他国家表达悔恨之情并不那么简单。',\n",
       " '但这正是首相安倍晋三在即将到来的二战结束70周年纪念讲话中所要做的。',\n",
       " '该讲话将以诸多日本乃至全球著名二战史家的意见为基础，但更重要的是安倍本人、他的良知和他的用心，因为他明白，在这个高度敏感的话题上，他的用语意义重大。',\n",
       " '当然，安倍绝非首位面临如此挑战的日本领导人。',\n",
       " '在他以前，无数首相和内阁官方长官都对二战中所发生的事件表达了诚挚的忏悔。',\n",
       " '二十年前，时任首相、社会党党首村山富市承认“日本的殖民统治和侵略给许多国家的人民造成了严重伤害和苦难，”特别是亚洲国家。',\n",
       " '他接着表示“感到深深的忏悔”，向受害者致以“衷心的歉意”。',\n",
       " '十年后，小泉纯一郎首相重申了村山的歉词，并说二战结束以来日本一直在“用行动表达对战争的忏悔，”特别是发展援助和人道主义行动。',\n",
       " '小泉还承诺“日本是热爱和平的国家，将尽其所能致力于实现全人类的和平与繁荣。 ”',\n",
       " '尽管日本做出了这些直白的忏悔，但一些政府和人民仍不满足，给人们一种日本领导人的所作所为并没有让他们感到日本的悔意的印象。',\n",
       " '从某种程度上，这个棘手问题不难理解； 幸存者及其后代的痛苦依然剧烈。',\n",
       " '但在某些情形中，不愿抛弃历史恩怨是因为政治利益。',\n",
       " '事实上，声称安倍不同意此前的官方道歉（尽管他一再保证他同意此前的道歉），其背后便是政治动机； 同样，认为安倍试图推翻历史（尽管他从未否认日本的殖民侵略），其背后也是政治动机。',\n",
       " '此外，一些人将日本的整体形象刻画为一个冥顽不灵的国家——更有甚者，说日本铁了心要重走军国主义老路。',\n",
       " '这样的描述不可不谓厚颜无耻，因为七十年来日本一直是国际社会和平而具有建设性的成员。',\n",
       " '对于一些质问祖国究竟需要道歉多长时间的日本人来说，这一点很重要，有人甚至认为，70年后，关于该主题的一条“推特”应该足以让安倍认识到这一点。',\n",
       " '但是，首相仍决定就该主题发表强烈而诚挚的讲话。',\n",
       " '今年年初，安倍宣布他将在70周年讲话中表达日本对战争的忏悔，描述日本在支持和平方面所取得的进步，并表达日本可以在未来几十年对亚洲和世界其他地区的和平所做的贡献。',\n",
       " '事实上，第三部分引起了一些观察者的担忧：通过帮助亚太地区构建强大的安全结构，日本可能破坏某些行动方主张自身利益的能力。',\n",
       " '因此，它们用谣言攻击安倍的讲话，哪怕距离讲话时间还有几个月之久、演讲词都没有开始撰写。',\n",
       " '但是，亚洲的安全与繁荣当然事关所有人的利益。',\n",
       " '因此，即便安倍讲话的用词并非十分重要，重要的是他所表达的决心，以及他（以合适的谦逊态度）拿出怎样的行动。',\n",
       " '事实上，安倍似乎决心要在与日本友邦和盟国进行有效合作的基础上为和平做出真正的贡献。',\n",
       " '但如果亚洲要超越历史恩怨，日本战时侵略的受害者就必须承认2015年的日本不是1931、1941乃至1945年的日本，并且，如许多亚洲领导人在过去几年中所认识到的，原谅可以给所有人带来好处。',\n",
       " '1998年，韩国总统金大中对前日本首相小渊惠三的讲话予以积极回应。',\n",
       " '印度尼西亚、菲律宾、越南和其他国家也是如此，现在它们都欢迎日本与盟国携手保护地区安全的承诺。',\n",
       " '这些国家对和解的开放态度让日本重新铸造其地区和平与繁荣重要仲裁者的形象，更不用说活力日盛的文化枢纽了。',\n",
       " '该地区其他国家也应该效而仿之，按字面意思接受日本诚挚的道歉，与日本共同建设更美好的未来。',\n",
       " '在亚洲面临严重安全挑战的当下，这一立场无比紧要。',\n",
       " '小农迎来大机遇',\n",
       " '纽约——八国集团不久前在意大利拉奎拉峰会上启动的200亿美元小农援助计划或许是战胜饥饿和极端贫困的一次历史性的突破。',\n",
       " '如果这笔新资金能得到认真的管理，非洲的粮食生产将会大幅度增加。',\n",
       " '实际上，这项新计划和其他医疗、教育、基础设施计划一起，有可能成为迄今为止实现千年发展目标的最有效举措，千年发展目标国际协议规定到2015年将极端贫困、疾病和饥饿人口减少到原来的半数。',\n",
       " '2002至2006年，我为实现千年发展目标，在时任联合国秘书长科菲·安南管理下领导联合国千年发展项目。',\n",
       " '“小农”是这个项目的基础，“小农”一词是非洲、拉美和亚洲耕种土地不超过1公顷（合2. 5英亩）的农民家庭的代名词。',\n",
       " '颇具讽刺意味的是，尽管自身也生产粮食，但他们却隶属于世界最贫困、最饥饿家庭的行列。',\n",
       " '他们饥饿，是因为无力购买提高产量所需的高产种子、化肥、灌溉设备和其他工具。',\n",
       " '这导致农作物产量匮乏，根本不够维持生计。',\n",
       " '他们的贫困造成农业生产率低下，而农业生产率低下又反过来加剧了贫困。',\n",
       " '这形成了恶性循环，技术上被称为贫困陷阱。',\n",
       " '在两位世界著名科学家斯瓦米纳坦和佩德罗·桑切斯的领导下，联合国千年发展项目反饥饿工作组研究这种恶性循环应该如何打破。 工作组的结论是，如果以农业投入的方式向小农提供帮助，可能会大幅增加非洲粮食产量。',\n",
       " '千年计划建议大幅度增加对这方面的全球资助。 根据此项成果及相关的科学发现，安南在扩大非洲和捐助国合作规模的基础上，在2004年呼吁启动非洲绿色革命。',\n",
       " '我们很多人，特别是现任联合国秘书长潘基文，为实现这一目标付出了艰苦的努力。 潘秘书长反复强调过去两年中全球粮食、金融和能源危机引发的特别问题。',\n",
       " '八国集团声明反映了这些年的努力，其中当然离不开美国总统巴拉克·奥巴马、西班牙总理何塞·路易斯·萨帕特罗、澳大利亚总理陆克文、世界银行行长罗伯特·佐立克、欧盟专员路易·米歇尔、欧洲议会议员蒂伊斯·伯曼、还有其他人的领导作用。',\n",
       " '现在的关键是要收到实效。',\n",
       " '历史的教训显而易见。',\n",
       " '向小农家庭发放含有大量补贴的种子和化肥（有时甚至是免费发放）将产生持续性效果。',\n",
       " '不仅能在短期内提高粮食产量，而且能使农户利用高收入和更好的医疗积累各种财富：现金盈余、土壤养分、家畜、及其子女的医疗和教育。',\n",
       " '资产的积累反过来会启动小额融资公司等当地信贷市场。',\n",
       " '农户可以依靠自有资金或资信改善来以贷款的方式购买农用物资。',\n",
       " '目前已经就援助小农的必要性达成???一致，但在实际操作中还存在问题。',\n",
       " '现在主要的风险或许是“官僚援助体系”竭力争夺这200亿美元的经费，想把尽可能多的部分用于会议、专家咨询、管理费用、报告撰写以及组织更多的会议。',\n",
       " '捐助国的“伙伴关系”自身又可能代价昂贵，而且除推迟实际行动外起不到任何作用。',\n",
       " '如果捐助国政府真的看重结果，就应该从30家或者更多的独立官僚援助机构中把款项收回，集中交给一两家机构管理。 最顺理成章的管理机构是华盛顿世界银行和罗马联合国国际农业发展基金。',\n",
       " '这样这一两家机构就会拥有数十亿美元的扶贫账户。',\n",
       " '饥荒地区，特别是非洲的政府，则需提交国家行动计划，提供他们打算如何利用捐助款项为贫困农民提供高产种子、化肥、灌溉、农机、仓库和咨询建议的细节。',\n",
       " '将由独立的专家小组对国家计划的科学性和管理的连贯性进行评估。',\n",
       " '如果计划通过了检验，支持计划的资金将迅速拨付到位。',\n",
       " '而后，将对每项国家计划进行监督、审计和评估。',\n",
       " '这种方法直接、有效、负责、有着不错的科学基础。',\n",
       " '最近两个成功的捐助案例就是使用的这种方法：其一是全球疫苗免疫联盟成功为儿童提供免疫服务，还有就是全球艾滋病、肺结核和疟疾基金为战胜上述致命疾病的国家计划提供了支持。',\n",
       " '过去10年来这两家机构都挽救了数百万人的生命，并为更高效、更科学合理的新的发展援助方法铺平了道路。',\n",
       " '很多联合国和富裕国家援助机构为此展开争夺，这并不值得大惊小怪。',\n",
       " '但这种争夺常常是为了争权夺利，而不是以最有效的方法向贫困人口提供援助。',\n",
       " '奥巴马、陆克文、萨帕特罗和其他有远见的领导人因此可以通过兑现在八国峰会上的承诺、坚持要求实际援助效果带来巨大的变化。',\n",
       " '必须绕过官僚机构为由世界最贫困的农民家庭所耕种的土地提供援助。',\n",
       " '黑白人种问题',\n",
       " '纽约——7月16日下午两名男子看似正试图闯入位于马萨诸塞州坎布里奇高尚住宅区的一栋豪宅。',\n",
       " '接到电话报警后，一名警察立即赶到了现场。',\n",
       " '他看到一名黑人男子在房间里，就要求他出来。',\n",
       " '那个人拒绝了。',\n",
       " '警察要求他说明身份。',\n",
       " '该男子仍然拒绝走出房门，但说自己是一位哈佛教授，出示了证件，并警告警察不要打扰他。',\n",
       " '他说美国黑人遭到歧视，并要求那位白人警察报名并出示警官证。',\n",
       " '此时该警察的几名同事赶到，他们以妨害治安为由逮捕了这位教授。',\n",
       " '我们现在知道这位教授在司机的帮助下闯入了自家的房门，因为门被卡住了。',\n",
       " '这次事件中不同寻常的并不是警察的暴虐。',\n",
       " '多数美国人都知道如果跟警察顶嘴，他们很快就会变得气急败坏。',\n",
       " '这位教授是黑人的事实可能有也可能没有导致警察更快地掏出手铐。',\n",
       " '这一点也没有什么不同寻常。',\n",
       " '这个案件的特殊性在于亨利·路易斯·盖茨是美国最著名的教授之一，著作等身，文章充栋，并多次在电视上露面。',\n",
       " '他称得上是位显贵，在学术和媒体圈中一呼百应，还与巴拉克·奥巴马总统私交甚笃。',\n",
       " '他因此警告詹姆斯·克劳利中士，这位坎布里奇的老资格警官不要招惹他。',\n",
       " '美国的阶级和种族问题错综复杂。',\n",
       " '在这个案件中，不可能将它们区分清楚。',\n",
       " '盖茨非常了解美国糟糕的种族关系史，甚至可以说是这方面的专家，因此他本能地认为自己成了种族歧视的受害者。',\n",
       " '从他的话中还可以看出他因未能获得一位哈佛教授兼媒体名人应有的尊重而耿耿于怀。',\n",
       " '他在网上公布的一次采访中对自己的女儿说：“[克劳利]当时应该对我说，‘对不起，先生，祝您好运。 我喜欢您的[电视]系列片——回头见！',\n",
       " '’说完后立即离开。 ”',\n",
       " '唉，克劳利中士从来没有听说过盖茨教授的大名。',\n",
       " '作为一名兄弟都在警方供职的当地男性，一名体育爱好者，和一名业余篮球教练，克劳利和盖茨根本不属于同一个的社交圈。',\n",
       " '实际上，对盖茨的指控已被及时取消，如果不是奥巴马总统在因连续几周争取通过医疗法案未果而感到厌倦和沮丧时去替“老友”盖茨说话，将警方斥之为“愚不可及”的话，这个案件可能已经烟消云散了。',\n",
       " '然后他和盖茨都谈到了这次事件的“教训”。',\n",
       " '盖茨甚至可能正在筹划拍摄一部有关粗暴执法的电视纪录片。',\n",
       " '如果我们还没有吸取这样的教训，那么这件事的教训之一就是告诉我们尽管选出了一位黑人总统，但种族敏感问题多么容易在美国生活中浮出水面。',\n",
       " '黑人的愤怒、白人的负疚、还有黑人和白人的恐惧是如此错综复杂，以致于多数美国人选择逃避种族话题。',\n",
       " '这片领域里有着太多的地雷。',\n",
       " '奥巴马最大的成就之一是他通过自己智慧巧妙的措辞，对绝大多数人讳莫如深的种族问题进行了严肃的探讨。',\n",
       " '如果同样的事情发生在哈莱姆区，或另外一个主要由黑人构成的穷人聚居区内一个无名小卒的身上，那么也许根本就不会有人知道。 这次事件发生在坎布里奇教授身上才吸引了人们的注意力。',\n",
       " '但这个案件的确也存在着对有关种族的全国讨论产生负面影响的危险性。',\n",
       " '因为把实际上不那么重要的问题过分夸大，盖茨可能受到将比这恶劣很多的滥用职权事件琐碎化的指责。',\n",
       " '事实上，我们根本不知道这个案件是否涉及到种族范畴。',\n",
       " '克劳利自始至终从未提到过盖茨的肤色，也没有出现过任何暴力侵害行为。',\n",
       " '只是教授和警察双方都对不尊重的暗示表现得过于敏感。',\n",
       " '一位不愿受到打扰的教授的怒火并非探讨无数贫困的无名人士所处困境的最佳方式，这些无名的小人物太容易被我们所忽略。',\n",
       " '萨尔科奇的宝莱坞新娘',\n",
       " '??? 巴黎&#45;&#45;自从法国总统萨尔科奇在欧洲迪斯尼的浪漫之旅期间公开承认与前超级模特、流行音乐歌手布吕尼的恋情而把自己从法国最受追逐的钻石王老五名单中除下后，他就沾染上了麻烦。',\n",
       " '??? 他的民意指数第一次下降到50%以下。',\n",
       " '上年纪的法国人并不欣赏国家领导人公开展示恋情。',\n",
       " '在外国，几名埃及议员们对于法国国家元首同其女友同床共枕非常激动，乃至在国会上公开表示不满。',\n",
       " '??? 同样，印度也对如何处理萨尔科奇即将作为国家贵宾参加该国1月26日的共和国日庆祝活动的礼宾问题为难。',\n",
       " '第一女友是否应当像第一夫人一样有自己的车队呢？',\n",
       " '极右翼印度教团体反对情人节，认为这是腐朽堕落的西方节日。 他们警告说，如果萨尔科奇带着女朋友来访，他们就会上街欢迎。',\n",
       " '??? 这一争端已经威胁给大受赞扬的世界两大民主国家的峰会蒙上阴影。',\n",
       " '推动法国经济的的军火、核电站以及空中客车等大笔利润丰厚的交易取决于这一访问，因此，对于这次成功的印度之行，法国利益攸关。',\n",
       " '他大声训斥说：“完事后你们也许就会知道了。',\n",
       " '”有传言说两人已经订下2月8日或者9日结婚。 还有人说萨尔科奇在回避结婚问题的时候已经在爱丽舍宫秘密结婚而让媒体扑空。',\n",
       " '??? 如果确实如此，萨尔科奇就错过了一辈子才有一次的浪漫机会。',\n",
       " '如果二人以埃及祖玛和约旦佩特拉为背景在镜头前任人拍照，那么，想像一下在世界上最为浪漫的地点印度泰姬陵前将会有多么热闹。',\n",
       " '而且，鉴于法国目前对于印度的宝莱坞电影着迷，一场浪漫的印度婚礼将会再合适不过了。',\n",
       " '布吕尼自身的生活道路也非常像宝莱坞明星那样从模特转到演员。',\n",
       " '一个漂亮的褐色头发歌唱女郎对于宝莱坞化妆而言是再好不过的了。',\n",
       " '??? 印度政府如果见到第一女友成为第一夫人而不感到如释重负就无足轻重了。 印度最大的报纸之一印度快报怕有人不懂而指出，“女朋友不是妻子或配偶。',\n",
       " '”一旦结婚，所有有关法国代表团礼宾问题的担心都将会烟消雾散。',\n",
       " '??? 除了宝莱坞刚刚出道的女演员会在银幕上有一些露骨的色情折腾表演外，印度还是一个极为保守的社会。',\n",
       " '离婚是丢人现眼的事情(萨尔科奇现在已经两次离婚了)。',\n",
       " '而且，尽管有钱有势的人周围有不少情人，但是她们并不公开出现在这些人的周围。',\n",
       " '公共场合下即使夫妻之间亲吻拥抱也是为社会所不容。',\n",
       " '??? 在这一方面，印度倒像是萨尔科奇所想要决裂的法国，而不是当前的法国。',\n",
       " '在许多法国人看来，大多数印度人情愿不放下对萨尔科奇前任们的“伪善”指责(也就是，直到法国前总统密特朗葬礼时人们才知道他有一名情妇和私生女儿)。',\n",
       " '??? 萨尔科奇要比所有的人都应该知道总统职位份量的很大一部分来自于庄重和场合。',\n",
       " '在国事活动中，外表应当是遮人耳目的。',\n",
       " '萨尔科奇平时具有良好的媒体本能，他抱怨说他和平常人并没有什么两样。 这时候，他就危险地把总统个人和总统职务混为一谈了。',\n",
       " '??? 大多数法国人只能梦见在印度举办一场充满异国情调的婚礼。',\n",
       " '萨尔科奇则可以让这一梦想成为现实。',\n",
       " '如果他果真像他所说的那样对布吕尼爱得死去活来而且想要大操大办与她结婚，那么为何不利用即将到来的印度之行并让这一婚礼永志不忘呢？',\n",
       " '他可以让新娘座在一头精心打扮的大象上迎接，而新娘则身著印度鲜艳服装，珠光宝气。',\n",
       " '被人称为喜欢名牌服装的总统的萨尔科奇可以穿上他所想要的所有金饰并且让新娘戴上更多的钻石。',\n",
       " '??? 照相机会闪个不停，印度人会抱以微笑，而法国将会受到做梦也想不到的宝莱坞式的热闹场景。',\n",
       " '而且，如果没有赶上婚礼，还有婚宴呢。',\n",
       " '重生的共同农业政策',\n",
       " '瓦格宁根，荷兰——诞生于1957年的共同农业政策（CAP）如今已年逾五旬，欧盟委员会正提议对它这位中年后代进行所谓的体检。',\n",
       " '不过，表面的修补碍难满足欧盟未来的需要，共同农业政策必须转世重生。',\n",
       " '与其重生有关的工作预定即刻开始，完成的计划在2013年将可付诸实施，但我们还需对其进行一番深入得多的再思考。',\n",
       " '共同农业政策原本旨在为欧盟最初的六个成员国提供可靠的食物来源，当时这六国是食品进口国并试图实现一定的自给自足。',\n",
       " '所有公民都应获得优质、健康而又廉价的食品；',\n",
       " '提升的农业生产率将惠及农村地区，并使农民在欧盟不断增长的财富中获得相称的一份。',\n",
       " '实现这些目标所需的手段得到发展，食品安全也得以实现。',\n",
       " '很快，共同农业政策就被视为欧洲计划中的皇冠上的宝石。',\n",
       " '随着欧盟的演化与扩张，食品体系也变得更加复杂，这一体系不但包含了生产、加工、供应链组织以及批发和零售配销等环节，而且每一环节又涉及诸如健康、环保等新的问题；',\n",
       " '对土地的使用也受到更为认真的监控。',\n",
       " '1991年由荷兰政府政策科学委员会进行的一次名为',\n",
       " '这些数据还只是针对彼时只有15个成员的欧盟，如今欧盟已扩大为27国，因此其潜在可能变得更为巨大。',\n",
       " '一份荷兰的土地使用分析已经表明，通过在最好的可使用土地上运用最佳的技术与生态手段，食物生产可以显著增加。',\n",
       " '因此，所需农民数量也已经显著减少这一点不再令人吃惊。',\n",
       " '一个简化版的共同农业政策将鼓励更为清洁、更加多产并且高效的农业。',\n",
       " '对于欧盟在世界上的地位而言，此举还有一个额外益处，那就是一旦发展中国家的农民得到保证，将获得来自欧洲的公平待遇后，世界贸易组织陷于中止的多哈谈判就能重新启动。',\n",
       " '而且只要更新后的政策就位，共同农业政策在欧洲政治与社会整合过程中所扮演的发动机角色就能恢复。',\n",
       " '不过此类更新不能听任全球市场力量的摆布，因为其结果可能并不一定有利于欧洲农业与社会。',\n",
       " '如果市场“运作失当”，农民就会陷入贫困，并导致欧洲大片地区受到忽视。',\n",
       " '这是决策者们根据以下五条重要原则改革共同农业政策时必须严肃加以考虑的、足够真切的危险。',\n",
       " '1、欧盟需要一个增强欧洲农业竞争力的知识与创新政策。',\n",
       " '这一政策曾在荷兰取得成功，对该国农业综合企业的发展与实力贡献良多。',\n",
       " '荷兰农业综合企业21个分支中的10个（包括园艺育种、观赏植物、种用马铃薯与小牛肉）位列对该国经济与贸易收支平衡贡献最大的产业之中。',\n",
       " '欧盟作为整体而言，导向刺激科学卓越成就与欧洲知识体系内更高一致性的研究项目的政策将极大地增强农业竞争力并有助于食品安全和可持续发展。',\n",
       " '2、欧洲也需要一个关于土地使用的重组政策。',\n",
       " '在欧洲层面已有许多结构性改进项目得到资助，但农业生产与土地使用却不在其中。',\n",
       " '农业主体结构的发展将补全欧洲主体生态结构。',\n",
       " '重新造林与自然生态系统的修复也应当被纳入土地使用政策之中。',\n",
       " '3、关于欧洲食品体系的政策应对生产、加工、分配、物流以及零售诸环节进行联合考虑。',\n",
       " '消费模式与偏好也是这类体系不可或缺的一部分。',\n",
       " '欧洲科学基金会所做的初步研究 ——“对欧洲食品体系的展望”将为欧盟范围的政策创制提供帮助。',\n",
       " '4、在一个高速城市化的世界中，都市型农业能凭借少量的土地供应高质量的农产品。',\n",
       " '这为如何满足对健康食品不断增长的需求，同时又对环境的副作???最小这一问题提供了一个答案。',\n",
       " '5、新的共同农业政策应当包含一个保护欧洲地形的方案。',\n",
       " '但并非各处都应保留文化遗产，也不该忽视其成本。',\n",
       " '同时它还不应该成为一个趋向全力聚焦于贫瘠土地的防御性政策。',\n",
       " '上述五条重要原则涉及到激烈的选择，但它们可能只需欧洲纳税人付出更少的资金，而非更多。',\n",
       " '它们将对更为清洁、更加多产并且高效的农业与土地使用做出真正的贡献，并能兼顾社会的需求。',\n",
       " '突破饥饿危机',\n",
       " '纽约—当今世界饥饿危机的严重程度前所未有，已是当务之急。',\n",
       " '近十亿人在忍受着长期饥饿的煎熬—这样的人口很可能比两年前增加了一亿。',\n",
       " '西班牙正在对抗饥饿方面起着全球性的领导作用。 它将邀请世界各国领导人于一月底汇聚马德里，将言语付诸行动。',\n",
       " '在西班牙的领导和联合国秘书长潘基文的支持下，几个捐助国政府正协商着将他们的财政资源集中起来以便让世界上最贫困的农民能够种植更多的粮食，从而摆脱贫穷的陷阱。',\n",
       " '捐助国的支援所带来的好处是不言而喻的。',\n",
       " '非洲、海地和其他贫困地区目前都在没有高产种子和化肥的情况下种植作物。',\n",
       " '其结果就是粮食产量（例如玉米）比在有更好的农业投入的情况下大约要少三分之一。',\n",
       " '相较于大量施用化肥的中国农民每公顷四吨的产量，非洲农民每公顷的粮食产量只有约一吨。',\n",
       " '非洲农民知道他们需要化肥，只是买不起。',\n",
       " '有了捐助国的帮助，他们就能用上化肥了。',\n",
       " '到那时，这些农民不仅能养家糊口，还能开始赚取市场收入并为未来而储蓄。',\n",
       " '有了几年的积蓄，这些农民最终能够成为具有银行信用的人，或者有足够的现金自行购买必需的农业投入。',\n",
       " '现今的一个广泛共识是捐助融资对非洲小农户（拥有两公顷或以下土地，或者是贫穷的畜牧业者）的必要性和急迫性。',\n",
       " '联合国秘书长领导的一个指导小组去年得出的结论是非洲农业每年需要约八十亿的捐助融资—大约是现有规模的四倍—而重中之重是改良的种子、化肥、灌溉系统和延伸培训。',\n",
       " '除了对小农场的直接援助外，捐助方还应该在必需的研发方面给予更多的帮助，从而开发更多种类的新高产种子，特别是用以培育能够耐受暂时的洪涝、过量的氮、盐质土壤、害虫和其他可持续粮食生产所面临的挑战的作物。',\n",
       " '以当今的技术帮助穷人，同时投资于未来更好的技术是最佳的分工。',\n",
       " '有了如国际水稻研究所和国际玉米及小麦研究所这些提供高产种子和创新农业战略，并共同引发了亚洲“绿色革命”的研究中心，这种投资的回报极为可观。',\n",
       " '这些中心并非人尽皆知，但它们的确是无名英雄。',\n",
       " '它们的科学突破帮助我们养活了全世界的人口，这样的机构应该更多一些。',\n",
       " '数十个低收入、粮食赤字的国家（可能有四十到五十个）已经宣布了旨在增加小农场粮食产量的紧急计划，但却由于缺少捐助方的资金支持而力不从心。',\n",
       " '这些国家向世界银行请求融资，而该行在2008年也不遗余力地通过其新的“全球粮食危机响应计划” (GFCRP) 提供了帮助。',\n",
       " '但世行资金在满足这些国家的紧急需求方面也是捉襟见肘，并不得不对援助资金进行配额限制，使得其数额相对于原本所需的可靠而有效资金量而言只是杯水车薪。',\n",
       " '同时，近十亿人则在饥饿中挣扎。',\n",
       " '许多捐助国都各自宣称正准备增加对小型农业的资金支持，但现在正寻求一种合适的实施机制。',\n",
       " '目前的援助架构尚有欠缺。',\n",
       " '二十余家双边和多边的农业援助机构则高度分散，其规模不论个体还是集体都尚显不足。',\n",
       " '尽管许多专业人士付出了巨大的努力，但对饥饿危机的响应仍然远远不足。',\n",
       " '2008年的种植季节来了又去，贫穷的小农户们在此期间所获得的额外援助仍是少之又少。',\n",
       " '非洲国家无尽地，也几乎无果地寻求着购买化肥和良种所需的小额资金。',\n",
       " '作为西班牙政府此项计划的顾问委员会成员，我和我的同事们建议捐助方将他们的资金集中到同一个被我们称为“融资协调机制(FCM)”的国际账户。',\n",
       " '这些集中起来的资金能让贫穷国家的农民获得急需的化肥、良种和小型灌溉设备。',\n",
       " '穷国能从一个统一账户而不是数十个各自为政且高度分散的捐助方获得即时和可预见的农业投入融资。',\n",
       " '通过将财政资源集中到一个统一的捐助方FCM，可以保持较低的援助计划行政成本，援助资金流的持续性也能得到保证，而穷国也不必为了获得援助而进行二十五次谈判。',\n",
       " '按部就班的时代一去不返。',\n",
       " '捐助方承诺到2010年将援助资金增加一倍，但现在仍是任重道远。',\n",
       " '的确，在过去的二十年里，他们实际上削减了农业计划的援助资金，直到现在才开始回到正轨。',\n",
       " '与此同时，十亿人却每天生活在饥饿的阴影之中。',\n",
       " '我需要一种突破，它具有示范效应，公开、明晰并具有说服力； 它能激发公众的身心投入，并彰显成功。',\n",
       " '当世界上最富裕和最贫穷的国家在一月底齐心协力寻求全球饥饿危机的解决之道时，我们就能在马德里创造历史。',\n",
       " '十亿最贫穷人口的生命皆系于此。',\n",
       " '全球卫生状况的突破性机会',\n",
       " '纽约—每年都有数百万人因可防可治的疾病而身亡，特别是在穷国。',\n",
       " '在很多情形中，救命药可以以低廉的成本大量生产，但售价却让需要者望而却步。',\n",
       " '更有许多人的死仅仅是因为缺少治疗或疫苗，而造成这种情况的原因是用于为穷人治病的世界宝贵研究人才和有限资源太少。',\n",
       " '如此状况反映了经济学和法律方面急需得到修正的失灵。',\n",
       " '好消息是现在改变的机会已现，主要是通过一项由世界卫生组织牵头的国际计划，该计划将开始修正造成廉价药品开发和难以获得问题的知识产权制度漏洞。',\n",
       " '如今，药品之所以难以获得，主要是拜两大问题所赐。',\n",
       " '其中之一是药品成本太高； 或者，更正确地说，是药品要价太高，而生产成本只是要价的一小部分。',\n",
       " '第二个问题是药品开发的着眼点是利润最大化，而不是社会效益最大化，这使得药品开发倾向于创造对人类福利至关重要的品种。',\n",
       " '而穷人根本没有多少钱可花，因此，在现有安排下，制药公司根本没什么激励研究折磨穷人的疾病。',\n",
       " '事情本不必如此。',\n",
       " '制药公司指出，要价高是必要的，这是为了给研发提供资金。',\n",
       " '但是，在美国，大部分卫生相关研发活动的实际出资者是政府——或是通过公共支持（美国国家卫生研究院、国家科学基金会等）直接提供资金，或是通过药品公共采购（如医疗保险和医疗补助计划）间接提供资金。',\n",
       " '不能获得政府融资的部分也不是传统市场； 许多个人的处方药支出都能获得保险赔付。',\n",
       " '政府之所以为卫生研究提供融资，是因为改善药品是一种公共品。',\n",
       " '由此带来的知识进步能防止传染病、减轻常见病的经济和人身伤害，从而让每个人都受益。',\n",
       " '要获得高效，就必须在获得知识进步后尽可能快地共享之。',\n",
       " '托马斯·杰斐逊曾将知识比作蜡烛：用一根蜡烛点燃另一根蜡烛，并不会减弱前一根蜡烛的光辉。',\n",
       " '相反，这样能使所有东西都变得更亮。',\n",
       " '但是，在美国以及世界大部分地区，药品价格依然高得离谱，知识的传播则受到了严格的制约。',\n",
       " '这是因为我们所创造的专利制度给了创新者对其所创之物的暂时垄断权，这促使创新者对知识敝帚自珍，唯恐助了竞争者一臂之力。',\n",
       " '这一制度使得创新有利可图，从而确实为某些类型的研究提供了激励； 但制药公司也因此得以坐地起价，它们获得的激励与社会回报不一定相容。',\n",
       " '在卫生领域，致力于研究“仿造”药可能比开发一种另辟蹊径的疗法更加有利可图。',\n",
       " '专利制度甚至可能对创新起到反作用，因为研究的最重要输入变量是领先的思想，而专利制度却在鼓励彼此保密。',\n",
       " '要价高企和研究??向偏差问题的解决之道是用政府奖励基金代替现有模式。',\n",
       " '在奖励制度下，创新者将因新知识而受到奖赏，但不再获得新知识使用的垄断权。',\n",
       " '这样一来，竞争性市场的力量将能保证只要药品被开发出来，就能以尽可能低的价格（而不是大大膨胀的垄断价）为患者所获。',\n",
       " '幸运的是，一些美国立法者已对这一方法产生了浓厚兴趣。',\n",
       " '参议员桑德斯（Bernie Sanders）提出、并获得国会通过的艾滋病奖励基金法（Prize Fund for HIV/AIDS Act）只是一个开始。',\n",
       " '桑德斯的法案包括了一个旨在鼓励开源研究的重要部分，将促使现有研究模式从保密转向共享。',\n",
       " '但是，从全球角度看，我们的创新制度还需要做出大得多的变化。',\n",
       " '世界卫生组织旨在促进国际层面的广泛改革的努力非常关键。',\n",
       " '今年春天，世界卫生组织公布了一份报告，其中提出了一项类似于桑德斯法案的解决之道——只不过世卫组织的方案是全球水平的。',\n",
       " '有一点很重要，这份题为《用研发满足发展中国家的卫生需要》（“Research and Development to Meet Health Needs in Developing Countries）给出了一个完整的方法，包括来自政府的指令性发展中国家卫生研究资金分配； 优先卫生项目和实施的国际合作；',\n",
       " '以及监测最紧迫需求的全球观察组。 5月底，国际社会将由机会在世界卫生组织大会上开始实施这些想法，这将是世界公共卫生的希望时刻。',\n",
       " '改革创新制度并不仅仅是一个经济问题。',\n",
       " '在很多情况下，它是个事关生死的大问题。',\n",
       " '因此，将研发激励与药品价格脱钩、鼓励更多的科学知识分享至关重要。',\n",
       " '对美国来说，桑德斯法案是一次重大进步。',\n",
       " '对全世界来说，世卫组织的建议可谓修正长期存在的卫生不平等性恶疾的千载难逢的良机（从更广的角度讲，也是设立与全球化时代相适应的全球公共品治理新模式的良机）。',\n",
       " '我们决不能坐视这个机会从手中白白溜走。',\n",
       " '心不在焉的杀戮者',\n",
       " '作为一个物种，人类存在很大的自我控制问题。',\n",
       " '我们人类现在全世界所有的地方如此大肆捕鱼、打猎、采伐和种植作物，乃至我们实际上在把其他物种从地球上驱赶出去。',\n",
       " '我们极度想要从大自然中索取所有我们可以索取的东西，留给其他生物种类的东西少之又少。',\n",
       " '在1992年世界各国政府第一次保证解决人为全球气候变暖的时候，他们同时还誓言阻止由人类引发的其他物种的灭绝。',\n",
       " '在里约热内卢通过的生物多样性公约确认“生物多样性是人类的共同关切问题。',\n",
       " '”签署国们同意通过挽救物种及其栖息地来保全生物多样性，并且以可持续的方式使用生物资源(例如森林)。',\n",
       " '2002年，该公约的签署国们更进一步，承诺在2010年以前“大幅度削减目前的生物多样性损失”。',\n",
       " '不幸的是，就像其他如此众多的国际公约一样，生物多样性公约基本上不为人所知，没有得到宣传促进，也没有得到落实。',\n",
       " '这一疏忽是人类的悲剧。',\n",
       " '只需要很少的现金投入，甚至在根本上不需要花钱，我们就本能够保全自然，并且进而保护人类自身生命和生计的基础。',\n",
       " '我们并非是出于必须才杀戮其他物种，而是我们过于疏忽，乃至不改弦更张。',\n",
       " '让我们来看一看几个臭名昭著的例子。',\n",
       " '像西班牙、葡萄牙、澳大利亚以及新西兰等富国拥有从事所谓“水底拖网”的捕鱼船队。',\n",
       " '水底拖网在海洋底部拖走很重的鱼网，从而在这一过程中摧毁大量没有探明并且受到威胁的海洋物种。',\n",
       " '因为水底拖网是捕捞某些深海鱼种的“低成本”办法，因此某些复杂而又独特的生态、最为主要的是被称为海底山的地下火山被切断。',\n",
       " '其中的一个物种叫做橙连鳍鲑，被用于商业捕捞不过大约25年的时间，但是已经被捕捞得处于崩溃的边缘。',\n",
       " '同样在世界许多地方，热带雨林遭到砍伐来开辟牧场和粮食作物。',\n",
       " '结果就是栖息地的大量损失和物种的毁灭，所产生的微小的经济利益是以巨大的社会成本为代价的。',\n",
       " '在一片雨林被砍伐后，土壤通常马上就丧失了养分， 无法供养作物或者让家畜食用有营养的草地。',\n",
       " '结果，新的牧场或农田马上被放弃，没有机会再退耕还林，恢复其独特的生态系统。',\n",
       " '由于这些活动的代价高昂、收益低下，停止这些活动是容易的。',\n",
       " '水底拖网应当干脆被取缔； 在捕鱼产业向其他活动转型期间予以补偿是容易做到而且也是费用不大的。',\n",
       " '另一方面，砍伐森林最好通过经济激励的方式辅之以管治限制予以制止。',\n",
       " '简单地限制开发土地的做法可能无法奏效，因为农业家庭和社区将会面临极大的躲避法律限制的诱惑。',\n",
       " '在另一方面，财政刺激可能会成功，因为砍伐森林来创造牧场土地并不有利可图，从而使得农民为了保护土地而放弃付款。',\n",
       " '许多雨林国家在最近几年中联合起来提议富有国家建立雨林储存基金，用来向贫困的小农场主支付少量金额来保护森林。',\n",
       " '一个设计合理的基金将会放缓或者停止森林采伐，保护生物多样性，并且减少燃烧砍伐后的森林所造成的二氧化碳排放。',\n",
       " '与此同时，小农场主将会得到稳定的收入，他们可以用这笔钱作小额投资用以改善家庭的财富、教育和健康。',\n",
       " '除了取缔水底拖网并且建立为避免森林砍伐的全球性基金外，我们应当指定一个全球性的海洋保护区域网络，在这些区域中禁止捕鱼、划船、污染、挖泥、钻探以及其他有害的活动。',\n",
       " '这些区域不仅可以重新恢复物种，而且还可以提供生态效益向邻近的未保护区域扩散。',\n",
       " '我们还需要定期的科学进程来向世界展示物种繁荣和灭绝的证据，正如有关气候变化的进程一样。',\n",
       " '政客们并不十分听取个别科学家的意见，但是当几百个科学家异口同声的时候他们就不得不???取了。',\n",
       " '最后，各国应该在不迟于2010年谈判一项新的框架来减缓人为引起的气候变化。',\n",
       " '毫无疑问，气候变化是物种生存最大的威胁之一。',\n",
       " '当地球变暖、 降雨和风暴模式大幅度改变之际，许多物种都会处于不再支持其存活的气候区域内。',\n",
       " '某些物种可以迁徙，但是，除非我们采取决定性的行动来阻止气候变化，其他的物种(例如北极熊)就有可能遭到灭绝。',\n",
       " '在2010年以前实现这些目标是可能的。',\n",
       " '它们在经济上是可以承担的，而且在各个情况下都最终会带来巨大的净效益。',\n",
       " '最为重要的是，这将会让我们实现全球性的许诺。',\n",
       " '人类在心不在焉之际会摧毁数以白万计的其他物种并威胁到我们自己的未来，要相信这一点是过于痛苦的。',\n",
       " '多劳应多得',\n",
       " '伯克利—在美国，生产和递送我们所消费的商品只需要十分之三的工人。',\n",
       " '我们提炼、种植、设计、建造、制作、打磨和运送的所有东西——一直到从饭店厨房泡一杯咖啡并送给餐桌前的顾客——是由大约30%的美国劳动力做的。',\n",
       " '我们其他人把时间花在规划制造什么、决定把我们造出来的东西摆在哪里、履行个人服务、彼此谈话、追踪正在制造什么上，以便我们能够发现我们下一步需要做什么。',\n",
       " '而尽管我们显然拥有制造比我们需要的多得多的东西的能力，但我们似乎并不会富得发慌。',\n",
       " '现时代的一大悖论正是工人和中产阶级家庭在前无古人的丰裕时代仍然要为生活奔波。',\n",
       " '1930年，当凯恩斯的著名预言预测人类的“经济问题，即生存斗争”有可能“在一百年内得到解决，或至少看到解决办法。',\n",
       " '”',\n",
       " '然而没有什么迹象表明美国工人和中产阶级比35年前生活得更好。',\n",
       " '更奇怪的是，生产率增长似乎没有如你预期的那样大幅加速； 事实上，根据旧金山联邦储备银行经济研究部的约翰·费纳尔多（John Fernald）和王兵（音）的研究，生产率增长出现了减速。',\n",
       " '增长前景甚至更加糟糕，因为创新活动撞上了南墙。',\n",
       " '一个协调就业市场变化与我们的生活经验和这些统计数字的办法是注意到我们生产的许多东西与我们过去所制造的东西区别很大。',\n",
       " '在人类历史的大部分时间里，我们生产出来的东西无法轻易地在未经允许的情况下共享或使用。',\n",
       " '我们以前制造的东西是经济学家所谓的“竞争性”或“排他性”商品。',\n",
       " '?“竞争性”是指两个人无法在同时使用同一个产品。',\n",
       " '“排他性”是指产品的所有者可以轻易地阻止其他人使用这个产品。',\n",
       " '这两个特征让控制生产和分配的人拥有了巨大的议价权，成为基于私人产权的市场经济的理想之选。',\n",
       " '钱自然而然地流向了提供效用和价值的地方——并且这些资金流可以轻易地在国民账户中进行追踪。',\n",
       " '但我们在信息时代所生产的许多东西既不具有竞争性，也不具有排他性——这改变了整个局面。',\n",
       " '信息时代商品的制造很难施加激励； 它们的分配也难以货币化；',\n",
       " '我们也缺乏工具在国民账户中轻易地追踪它们。 结果是人们愿意为给定服务所支付的数量与国民统计所测量的增长之间的差异越来越大。',\n",
       " '换句话说，我们生产和消费的数量比经济指标所显示的规模大得多——而许多这些产品的额制造者没有获得充分的报偿。',\n",
       " '这就带来了一系列独特的问题。',\n",
       " '为了确保今天和明天的工人能够获得信息时代的好处，我们就必须重新设计经济制度以刺激这些新型商品的创造。',\n",
       " '除了找到记录这一新型财富的方法，我们还需要找到将让产品需求有利于创造者收入的渠道。',\n",
       " '只有找到能准确衡量我们所制造的商品的额价值的方法，我们才能维持中产阶级社会，而不至于出现由技术大亨和他们的服务业农奴组成的社会。',\n",
       " '学术精神的落幕',\n",
       " '伦敦——我敢打赌我比现在活着的人当过更多所大学的校长。',\n",
       " '这部分因为在担任香港总督期间，我被任命为这座城市所有大学的校长。',\n",
       " '我抗议说应当让大学自主选择行政领导。',\n",
       " '但大学却不允许我轻易辞职。',\n",
       " '因此有5年时间，我享受着为数以万计学子授予学位的体验，并亲眼目睹了这样的仪式对学子和他们的家庭意味着什么。',\n",
       " '1997年回到英国时，我被任命为纽卡斯尔大学的校长。',\n",
       " '之后在2003年，我再次被毕业生推选为世界最伟大的学府之一，牛津大学的校长。',\n",
       " '因此我对大学意味着什么、以及对校内教学、研究或学习有着强烈的观点也就不足为奇了。',\n",
       " '大学应当是一切社会的自由堡垒。',\n",
       " '它们主要的研究和教学功能不应受到政府的干涉； 它们应当能够控制自身的学术治理举措。',\n",
       " '如果上述条件得不到满足，我不相信一所学校能够成为一所世界级的学府。',\n",
       " '大学的功能是制造思想碰撞，与其他学者一道检验研究成果，以及对学生传授新的知识。',\n",
       " '言论自由因此对大学而言至关重要，它能让学者保持人类共同情感，并捍卫支撑任何自由社会的共同的理解和宽容。',\n",
       " '这当然导致独裁政府将大学视为危险，并试图遏制大学对疑难问题的提出和解答。',\n",
       " '但如果任何否定学术自由的行为都是对大学意义的打击，那么今天最具有讽刺意义的却是某些对这些价值最令人担忧的攻击来源于大学内部。',\n",
       " '在美国和英国，某些学生和老师现在试图限制争鸣和辩论。',\n",
       " '他们争辩说人们根本不应该听到他们强烈反对的想法。',\n",
       " '此外，他们还提出应当将未能通过今天政治正确性测试者的名字（虽非才能）从历史上抹去。',\n",
       " '托马斯·杰斐逊和塞西尔·罗兹就包括在这些名字当中。',\n",
       " '如果对丘吉尔和华盛顿进行同样的测试，他们的成绩将会如何？',\n",
       " '在某些显然不太有文化的校园常见的可怕行话（所谓的“无平台化”）里，某些人说话的机会被彻底剥夺。',\n",
       " '有些人呼吁建立安全空间，确保违反“道德”和“恰当”认知的任何行为都不会被学生接触到。',\n",
       " '这反应并必将孕育一种有害的牺牲政治——即划定与别人相对立的自我身份（和自我利益）。',\n",
       " '50年前当我还是学生时，我主要的老师是一位著名的马克思主义历史学家兼前共产党。',\n",
       " '英国安全部门非常怀疑他。',\n",
       " '他是一位伟大的历史学家和教师，但现在我却可能在外界鼓励下认为我的“安全空间”被他侵犯了。',\n",
       " '事实上，他在很大程度上拓宽了我的视野，让我以开放的态度面对挑战自己想法的讨论，更有能力区分辩论和争吵，也更能独立思考事情的对错。',\n",
       " '当然种族仇恨、性别敌意或政治暴力煽动等某些想法在任何一个自由社会都是被深恶痛绝的。',\n",
       " '自由的存在需要（在法治条件下由民主辩论自由决定的）某种限度。',\n",
       " '应当相信大学有能力加以自控。',\n",
       " '但不容许辩论、探讨和特定学术领域的存在是永远不能被容忍的。',\n",
       " '正如伟大的政治哲学家卡尔·波普教导我们的那样，我们唯一不能容忍的特质应当是偏狭。',\n",
       " '尤其在大学里更应当这样。',\n",
       " '而某些美国和英国的学者和学生本身就在破坏自由； 荒谬的是，他们有自由这样做。',\n",
       " '此外，中国和香港大学的自主权和自由正面临威胁，这些威胁并非来自于学校内部，而是来自于独裁政府。',\n",
       " '在香港，城市基本法和中英50年条约共同规定的大学自主权及言论自由本身正在遭受威胁。',\n",
       " '理由似乎是因为学生强烈支持2014年的民主抗议活动，他们所就读的大学应当受到惩罚。',\n",
       " '于是香港政府匆匆忙忙地挑起事端，显然是在北京政府的命令之下。',\n",
       " '事实上，通过在香港大街上绑架一名英国公民（和其他四名香港居民）的行为，中国政府最近才向外界表明他们对中英关系“黄金年代”条约义务（受到英国大臣们广为宣传）的看法。',\n",
       " '这5人出版的书籍揭露了中国领导人某些不可见人的秘密。',\n",
       " '在大陆，中国共产党对大学发动了继1989年天安门广场屠杀事件后最大规模的镇压。',\n",
       " '中国大学不允许讨论所谓西方价值观，只允许教授马克思主义。',\n",
       " '难道没有人告诉习近平主席及其政治局同事卡尔·马克思从哪里来？',\n",
       " '现在的问题恰恰是他们对马克思知之甚少，但却对列宁了解颇多。',\n",
       " '西方人应密切关注中国大学正在发生的事件，以及它们对支撑学术、教学和学院的实际价值是什么？',\n",
       " '我们要学会像学生那样进行对比和比较。',\n",
       " '由政府决定什么能安全学习和探讨的大学是否符合你的愿望？',\n",
       " '你心目中的大学是否将“安全空间”理念——即在冒犯他人的情况下停止讨论——视为学术环境中的矛盾修饰法？',\n",
       " '西方学生应时时想起他们在香港和中国的同伴，这些人仍在争取他们所熟视无睹——甚至往往随意滥用的自由。',\n",
       " '科研领域新一轮人才流失',\n",
       " '迪拜——2013年12月，诺贝尔物理学奖得主彼得·希格斯在接受卫报采访时表示，如果他今天在学术界求职，“我恐怕不会被认为是最高效的。',\n",
       " '”因为在1964年发表突破之作后迄今为止仅发表过不到十篇论文，希格斯认为今天没有哪家大学会雇佣他。',\n",
       " '“出版或者毁灭”是当今学者熟悉的概念。',\n",
       " '他们必须越来越多地在同行评议杂志上发表作品，才能攀登职业阶梯、保住工作岗位和为所在机构争取研究经费。',\n",
       " '但那些研究重点迥异且与今天能造就或毁灭学术/科研生涯的专业期刊鲜有联系的科学家和其他学者（比方说中东学者）又该怎么办？',\n",
       " '在著名期刊拥有高出版率的学者和科研机构效率得分较高，进而在职业生涯发展和研究经费支持方面得到更多奖励。',\n",
       " '但可悲的是，他们所发表的作品对所在研究领域能否产生可以度量的影响却往往不那么被外界关注。',\n",
       " '他们所面临的激励机制意味着数量往往比质量重要。',\n",
       " '学术期刊决定科研机构被动研究的学科领域排名，从而导致科研机构仅仅雇佣并保留那些高效率的学者。',\n",
       " '由此导致了更深层次的双重问题：学术期刊的影响力已经过于庞大，而且它们过分鼓励实证研究。',\n",
       " '在第一个问题上，期刊正逐步取代科研机构成为学术成果质量的评判者。',\n",
       " '几乎任何领域想在“A级”研究机构求职的学者都必须在几本被公认为入门通道的A级杂志上发表研究成果。',\n",
       " '上述期刊的编辑委员会越来越偏爱实证主义理论研究作品——也就是说以实证数据分析为基础的研究。',\n",
       " '定性研究——如民族志、参与性调查和案例研究等——往往被定性为只适合B级或C级期刊发表。',\n",
       " '相比那些从事定性研究的学者，从事实证研究的学者优势非常显著。 因为他们可以使用高效软件和功能强大的电脑来快速测试假设，并计算数据集中不同变量的影响。',\n",
       " '此类工作也可以降低成本，因为单个数据集可以产出多篇期刊文章。',\n",
       " '可以肯定，科学实践随技术演进、抑或学术界使用更丰富的数据集和更先进的软件都属于正常现象。',\n",
       " '但这种定量方法的采用不应成为评估科学卓越性和决定职业轨迹的唯一重要的衡量指标。',\n",
       " '毕竟知识是以不同方式取得的，而经验实证主义仅仅是广义认识论宝库中的一项法则。',\n",
       " '今天的实证主义趋势对发展中国家而言问题尤为严重，因为发展中国家不仅数据集稀缺，而且往往质量较差。',\n",
       " '因此，在发展中国家工作的科学家们面临着囚徒困境：或者研究有着丰富数据的富裕国家问题，或者甘冒职业生涯之险而进行无缘进入A级期刊的定性研究。',\n",
       " '从欧洲和北美等数据丰富国家迁往中东等数据贫乏国家的学者往往面临着这样的问题。',\n",
       " '正如阿布扎比我所在科研机构的研究人员所知的那样，采集调研数据用于进行定性研究尚属可行； 但从零开始搜集理论建设研究所需的丰富数据却是极其困难的。',\n",
       " '在今年召开的科学技术指标国际???议上，一位研究非洲土壤的法国学者报告在他所研究的领域，仅有5%的出版作品来自非洲研究者。',\n",
       " '但当他深入研究自己的研究成果时，却发现他对非洲土壤的认识有50%来自非洲研究者，而这些研究人员的成果没有也不可能在国际学术期刊上发表。',\n",
       " '英语不是通用语的国家在科研方面处境尤其不利，这绝非因为他们学术科研水平不高，而是因为科学界由英语期刊来扮演发号施令的角色。',\n",
       " '非英语学术期刊在科学界根本得不到同样的关注。',\n",
       " '结果导致许多国家可以进行的研究课题范围有限，因此必须拼尽全力才能将科研人才留在本国。',\n",
       " '中东地区的情况尤其如此，各国政府正努力实现经济多样化，从而使国民经济适应性得到提高。',\n",
       " '因为英语实证研究期刊进一步增强了对科学家能否取得职业成功渠道的控制，发展中国家必须大量投资于国内数据基础设施，才能使国内研究人员处在更具竞争力的科研地位上。',\n",
       " '但即使——或者尤其——如果发展中国家确实进行这样的投资，将给科研领域造成非常严重的后果。',\n",
       " '因为统治全球科学的学术期刊（多数）来自美国，科学家不用实际迁移就会成为新人才流失的组成部分，科学家的研究重点、问题和方法将会牺牲其他选择而转向目前占主导地位的实证主义认识研究。',\n",
       " '污染者必须负责',\n",
       " '发自纽约——当英国石油公司及其搭档瑞士越洋钻探公司于2010年在墨西哥湾引发“深水地平线”漏油事故时，美国政府不但要求英国石油公司承担油污清理费用，赔偿受害者损失，还要为导致此次灾难的违规行为接受刑事处罚。',\n",
       " '英国石油公司已经为该事故调拨了200多亿美元用于善后补救工作和交纳罚金。',\n",
       " '根据上周达成的协议，英国石油公司将支付美国历史上最大的一 笔刑事罚款——45亿美元。',\n",
       " '同样的环境事故清理善后标准也应运用到那些在贫困国家经营的国际企业身上。 这些大公司的势力相对当地政府来说通常极为巨大，以致许多公司都是在豁免权下活动，可以对环境造成严重破坏却极少或无需承担责任。',\n",
       " '随着我们进入一个可持续发展的新时代，豁免权应该转向责任制。',\n",
       " '不管是在富国还是穷国，污染者必须付出代价。',\n",
       " '大公司应该为它们的行为承担相应的责任。',\n",
       " '尼日利亚就曾经是企业逍遥于环境污染惩罚之外的例子。',\n",
       " '几十年来，包括壳牌、埃克森美孚和雪弗龙在内的各大石油公司一直在尼日尔三角洲开采石油。 尼日尔三角洲是一个拥有淡水沼泽森林、红树林、低地雨林和沿海屏障岛屿的脆弱生态环境。',\n",
       " '这片肥沃富饶的栖息地支撑着非凡的生物多样性——或者说在石油公司来破坏之前是这样的——而当地3000多万居民的健康和生计都依赖于该生态系统。',\n",
       " '二十年前，世界自然保护联盟把尼日尔三角洲分类为一个拥有极丰富海洋及沿海动植物多样性的地区——不同树种、鱼类、鸟类和哺乳类与其他形式的生命共处一地——因此将其列为极高的优先保护等级。',\n",
       " '然而该联盟也指出，由于这个地区极少或未被保护，其生物多样性正处于巨大的威胁之中。',\n",
       " '在尼日尔三角洲经营的跨国石油企业几十年来一直在该地区泄漏石油及燃烧天然气，根本不考虑自然环境和因其行为而变得贫困甚至遭受毒害的人类社区。',\n",
       " '一份研究估计宣称过去50年中这些公司累积泄漏的石油约为1000万桶——这是英国石油公司漏油量的2倍。',\n",
       " '该数据并不准确：过去的50年中有好几千次石油溢出事故——通常这些事故都未被详细记录，其石油泄漏量常常被掩盖甚至被公司及政府故意忽略。',\n",
       " '确实，正当英国石油公司遭受新的刑事处罚时，埃克森美孚又通报了一起位于尼日尔三角洲的石油管道泄漏事件。',\n",
       " '三角洲的环境破坏只是一长串事件中的一部分：腐败的公司与腐败的政府官员相互勾结以牟取私利。',\n",
       " '公司通常贿赂政府官员以获取石油合同，它们谎报产量，偷税漏税，逃避由其经营活动引起的环境损害的责任。',\n",
       " '由于一直掠夺三角洲自然财富的跨国企业几十年来的贿赂，尼日利亚的政府官员变得惊人的富有。',\n",
       " '壳牌——尼日尔三角洲最大的国外营运商——就屡次因其恶名昭彰的行为及不愿承担责任的态度而遭到外界批评。',\n",
       " '与此同时，当地居民却一直处于贫困状态，还被由不安全的空气、被污染的饮用水和食物链污染引起的疾病所困扰。',\n",
       " '当地违法行为已经引起了黑帮火拼以及层出不穷的管道偷油案件，这又进一步导致了大量石油泄漏事故并导致了造成大量人员死亡——包括许多无辜受殃者——的频繁爆炸事件。',\n",
       " '在殖民时期，从殖民领地攫取财富是帝国主义的官方意图。',\n",
       " '而在后殖民时期，这种意图有了更好的伪装。',\n",
       " '当石油公司在尼日利亚或其他国家恣意妄为时，它们会受到母国政权保护。',\n",
       " '美国及欧洲国家警告那些石油原产国：别找石油公司的麻烦。',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26196da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'en':en,'cn':cn}\n",
    "dataframe = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15dd83e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>cn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1929 or 1989?</td>\n",
       "      <td>1929年还是1989年?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PARIS – As the economic crisis deepens and wid...</td>\n",
       "      <td>巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>At the start of the crisis, many people likene...</td>\n",
       "      <td>一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Today, the mood is much grimmer, with referenc...</td>\n",
       "      <td>如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The tendency is either excessive restraint (Eu...</td>\n",
       "      <td>目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Europe is being cautious in the name of avoidi...</td>\n",
       "      <td>欧洲在避免债务和捍卫欧元的名义下正变得谨慎，而美国已经在许多方面行动起来，以利用这一理想的时...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>For geo-strategists, however, the year that na...</td>\n",
       "      <td>然而，作为地域战略学家，无论是从政治意义还是从经济意义上，让我自然想到的年份是1989年。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Of course, the fall of the house of Lehman Bro...</td>\n",
       "      <td>当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indeed, on the surface it seems to be its perf...</td>\n",
       "      <td>事实上，从表面上看，两者似乎是完全是相反的：一个是象征着压抑和人为分裂的柏林墙的倒塌，而另一...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Yet 2008-2009, like 1989, may very well corres...</td>\n",
       "      <td>然而，和1989年一样，2008-2009年很可能也能被视为一个划时代的改变，其带来的发人深...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                                      1929 or 1989?   \n",
       "1  PARIS – As the economic crisis deepens and wid...   \n",
       "2  At the start of the crisis, many people likene...   \n",
       "3  Today, the mood is much grimmer, with referenc...   \n",
       "4  The tendency is either excessive restraint (Eu...   \n",
       "5  Europe is being cautious in the name of avoidi...   \n",
       "6  For geo-strategists, however, the year that na...   \n",
       "7  Of course, the fall of the house of Lehman Bro...   \n",
       "8  Indeed, on the surface it seems to be its perf...   \n",
       "9  Yet 2008-2009, like 1989, may very well corres...   \n",
       "\n",
       "                                                  cn  \n",
       "0                                      1929年还是1989年?  \n",
       "1  巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正...  \n",
       "2  一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为...  \n",
       "3  如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政...  \n",
       "4                  目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。  \n",
       "5  欧洲在避免债务和捍卫欧元的名义下正变得谨慎，而美国已经在许多方面行动起来，以利用这一理想的时...  \n",
       "6      然而，作为地域战略学家，无论是从政治意义还是从经济意义上，让我自然想到的年份是1989年。  \n",
       "7                         当然，雷曼兄弟公司的倒闭和柏林墙的倒塌没有任何关系。  \n",
       "8  事实上，从表面上看，两者似乎是完全是相反的：一个是象征着压抑和人为分裂的柏林墙的倒塌，而另一...  \n",
       "9  然而，和1989年一样，2008-2009年很可能也能被视为一个划时代的改变，其带来的发人深...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c78595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_text as tf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4704406a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "[[b'PARIS', b'\\xe2\\x80\\x93', b'As', b'the', b'economic', b'crisis', b'deepens', b'and', b'widens,', b'the', b'world', b'has', b'been', b'searching', b'for', b'historical', b'analogies', b'to', b'help', b'us', b'understand', b'what', b'has', b'been', b'happening.']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize([\"PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97444911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe5\\xb7\\xb4\\xe9\\xbb\\x8e', b'-', b'\\xe9\\x9a\\x8f\\xe7\\x9d\\x80', b'\\xe7\\xbb\\x8f\\xe6\\xb5\\x8e', b'\\xe5\\x8d\\xb1\\xe6\\x9c\\xba', b'\\xe4\\xb8\\x8d\\xe6\\x96\\xad', b'\\xe5\\x8a\\xa0\\xe6\\xb7\\xb1', b'\\xe5\\x92\\x8c', b'\\xe8\\x94\\x93\\xe5\\xbb\\xb6', b'\\xef\\xbc\\x8c', b'\\xe6\\x95\\xb4', b'\\xe4\\xb8\\xaa', b'\\xe4\\xb8\\x96\\xe7\\x95\\x8c', b'\\xe4\\xb8\\x80\\xe7\\x9b\\xb4', b'\\xe5\\x9c\\xa8', b'\\xe5\\xaf\\xbb\\xe6\\x89\\xbe', b'\\xe5\\x8e\\x86\\xe5\\x8f\\xb2', b'\\xe4\\xb8\\x8a', b'\\xe7\\x9a\\x84', b'\\xe7\\xb1\\xbb\\xe4\\xbc\\xbc', b'\\xe4\\xba\\x8b\\xe4\\xbb\\xb6', b'\\xe5\\xb8\\x8c\\xe6\\x9c\\x9b', b'\\xe6\\x9c\\x89\\xe5\\x8a\\xa9\\xe4\\xba\\x8e', b'\\xe6\\x88\\x91\\xe4\\xbb\\xac', b'\\xe4\\xba\\x86\\xe8\\xa7\\xa3', b'\\xe7\\x9b\\xae\\xe5\\x89\\x8d', b'\\xe6\\xad\\xa3\\xe5\\x9c\\xa8', b'\\xe5\\x8f\\x91\\xe7\\x94\\x9f', b'\\xe7\\x9a\\x84', b'\\xe6\\x83\\x85\\xe5\\x86\\xb5', b'\\xe3\\x80\\x82']]\n"
     ]
    }
   ],
   "source": [
    "MODEL_HANDLE = \"https://tfhub.dev/google/zh_segmentation/1\"\n",
    "segmenter = tf_text.HubModuleTokenizer(MODEL_HANDLE)\n",
    "tokens = segmenter.tokenize([\"巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\"])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e4456e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['巴黎', '-', '随着', '经济', '危机', '不断', '加深', '和', '蔓延', '，', '整', '个', '世界', '一直', '在', '寻找', '历史', '上', '的', '类似', '事件', '希望', '有助于', '我们', '了解', '目前', '正在', '发生', '的', '情况', '。']]\n"
     ]
    }
   ],
   "source": [
    "def decode_list(x):\n",
    "    if type(x) is list:\n",
    "        return list(map(decode_list, x))\n",
    "    return x.decode(\"UTF-8\")\n",
    "\n",
    "def decode_utf8_tensor(x):\n",
    "    return list(map(decode_list, x.to_list()))\n",
    "\n",
    "print(decode_utf8_tensor(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dcb90cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus((' '.join(e) for e in en), target_vocab_size=2**13)\n",
    "tokenizer_cn = tfds.features.text.SubwordTextEncoder.build_from_corpus(( c for c in cn), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fc46185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[124, 10, 448, 8248, 8153, 8201, 8248, 8153, 8201, 2]\n",
      "我是你爸爸。\n"
     ]
    }
   ],
   "source": [
    "sample_str = '我是你爸爸。'\n",
    "tokenized_str = tokenizer_cn.encode(sample_str)\n",
    "print(tokenized_str)\n",
    "original_str = tokenizer_cn.decode(tokenized_str)\n",
    "print(original_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e39d95b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1043, 1040, 1047, 1047, 6, 1058, 1050, 1053, 1047, 1039, 983, 971, 1055, 1040, 1049, 1054, 1050, 1053, 1041, 1047, 1050, 20, 989]\n",
      "hello world, tensorflow 2\n"
     ]
    }
   ],
   "source": [
    "sample_str = 'hello world, tensorflow 2'\n",
    "tokenized_str = tokenizer_en.encode(sample_str)\n",
    "print(tokenized_str)\n",
    "original_str = tokenizer_en.decode(tokenized_str)\n",
    "print(original_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15769cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = [tokenizer_cn.encode(line)for line in cn]\n",
    "en= [tokenizer_en.encode(' '.join(line))for line in en]\n",
    "\n",
    "en_ = []\n",
    "for i in en :\n",
    "    en_.append([tokenizer_en.vocab_size]+list(i)+[tokenizer_en.vocab_size+1])\n",
    "cn_ = []\n",
    "for i in cn :\n",
    "    cn_.append([tokenizer_cn.vocab_size]+list(i)+[tokenizer_cn.vocab_size+1])\n",
    "    \n",
    "en_text=tf.keras.preprocessing.sequence.pad_sequences(en_, maxlen=40, dtype='int32', padding='post',value=0.0)\n",
    "cn_text=tf.keras.preprocessing.sequence.pad_sequences(cn_, maxlen=40, dtype='int32', padding='post',value=0.0)\n",
    "\n",
    "en_batch = tf.data.Dataset.from_tensor_slices(en_text)\n",
    "cn_batch = tf.data.Dataset.from_tensor_slices(cn_text)\n",
    "\n",
    "en_batch  = en_batch.padded_batch(64)\n",
    "cn_batch = cn_batch.padded_batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e883786",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef encode(lang1, lang2):\\n    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\\n        lang1.numpy()) + [tokenizer_pt.vocab_size+1]\\n    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\\n        lang2.numpy()) + [tokenizer_en.vocab_size+1]\\n    return lang1, lang2\\n\\nMAX_LENGTH=100\\ndef filter_long_sent(x, y, max_length=MAX_LENGTH):\\n    return tf.logical_and(tf.size(x) <= max_length,\\n                         tf.size(y) <= max_length)\\n                         \\ndef tf_encode(cn, en):\\n    return tf.py_function(encode, [cn, en], [tf.int64, tf.int64])\\n    \\nBUFFER_SIZE = 20000\\nBATCH_SIZE = 64\\n\\n# 使用.map()运行相关图操作\\ntrain_dataset = train_examples.map(tf_encode)\\n# 过滤过长的数据\\ntrain_dataset = train_dataset.filter(filter_long_sent)\\n# 使用缓存数据加速读入\\ntrain_dataset = train_dataset.cache()\\n# 打乱并获取批数据\\ntrain_dataset = train_dataset.padded_batch(\\nBATCH_SIZE, padded_shapes=([40], [40]))  # 填充为最大长度-90\\n# 设置预取数据\\ntrain_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\\n\\n# 验证集数据\\nval_dataset = val_examples.map(tf_encode)\\nval_dataset = val_dataset.filter(filter_long_sent).padded_batch(\\nBATCH_SIZE, padded_shapes=([40], [40]))\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def encode(lang1, lang2):\n",
    "    lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(\n",
    "        lang1.numpy()) + [tokenizer_pt.vocab_size+1]\n",
    "    lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
    "        lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
    "    return lang1, lang2\n",
    "\n",
    "MAX_LENGTH=100\n",
    "def filter_long_sent(x, y, max_length=MAX_LENGTH):\n",
    "    return tf.logical_and(tf.size(x) <= max_length,\n",
    "                         tf.size(y) <= max_length)\n",
    "                         \n",
    "def tf_encode(cn, en):\n",
    "    return tf.py_function(encode, [cn, en], [tf.int64, tf.int64])\n",
    "    \n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 使用.map()运行相关图操作\n",
    "train_dataset = train_examples.map(tf_encode)\n",
    "# 过滤过长的数据\n",
    "train_dataset = train_dataset.filter(filter_long_sent)\n",
    "# 使用缓存数据加速读入\n",
    "train_dataset = train_dataset.cache()\n",
    "# 打乱并获取批数据\n",
    "train_dataset = train_dataset.padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))  # 填充为最大长度-90\n",
    "# 设置预取数据\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# 验证集数据\n",
    "val_dataset = val_examples.map(tf_encode)\n",
    "val_dataset = val_dataset.filter(filter_long_sent).padded_batch(\n",
    "BATCH_SIZE, padded_shapes=([40], [40]))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ca8aa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    # 这里的i等价与上面公式中的2i和2i+1\n",
    "    angle_rates = 1 / np.power(10000, (2*(i // 2))/ np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8960b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                           np.arange(d_model)[np.newaxis,:],\n",
    "                           d_model)\n",
    "    # 第2i项使用sin\n",
    "    sines = np.sin(angle_rads[:, 0::2])\n",
    "    # 第2i+1项使用cos\n",
    "    cones = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = np.concatenate([sines, cones], axis=-1)\n",
    "    pos_encoding = pos_encoding[np.newaxis, ...]\n",
    "    \n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd7cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print(pos_encoding.shape)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show() # 在这里左右边分别为原来2i 和 2i+1的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54375e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 1, 1, 5), dtype=float32, numpy=\n",
       "array([[[[0., 0., 1., 1., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_padding_mark(seq):\n",
    "    # 获取为0的padding项\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    \n",
    "    # 扩充维度以便用于attention矩阵\n",
    "    return seq[:, np.newaxis, np.newaxis, :] # (batch_size,1,1,seq_len)\n",
    "\n",
    "# mark 测试\n",
    "create_padding_mark([[1,2,0,0,3],[3,4,5,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdcc08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mark(size):\n",
    "    # 1 - 对角线和取下三角的全部对角线（-1->全部）\n",
    "    # 这样就可以构造出每个时刻未预测token的掩码\n",
    "    mark = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mark  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7a0cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "temp = create_look_ahead_mark(3)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80adf26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    # query key 相乘获取匹配关系\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "    \n",
    "    # 使用dk进行缩放\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    \n",
    "    # 掩码\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)\n",
    "        \n",
    "    # 通过softmax获取attention权重\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    \n",
    "    # attention 乘上value\n",
    "    output = tf.matmul(attention_weights, v) # （.., seq_len_v, depth）\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40bddedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_att = scaled_dot_product_attention(\n",
    "    q, k, v, None)\n",
    "    print('attention weight:')\n",
    "    print(temp_att)\n",
    "    print('output:')\n",
    "    print(temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0658d2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weight:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "output:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 显示为numpy类型\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 3)\n",
    "# 关注第2个key, 返回对应的value\n",
    "temp_q = tf.constant([[0,10,0]], dtype=tf.float32)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f7373ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weight:\n",
      "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
      "output:\n",
      "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 关注重复的key(第3、4个), 返回对应的value（平均）\n",
    "temp_q = tf.constant([[0,0,10]], dtype=tf.float32)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9e392c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weight:\n",
      "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
      "output:\n",
      "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 关注第1、2个key, 返回对应的value（平均）\n",
    "temp_q = tf.constant([[10,10,0]], dtype=tf.float32)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6143b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention weight:\n",
      "tf.Tensor(\n",
      "[[0.  0.  0.5 0.5]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
      "output:\n",
      "tf.Tensor(\n",
      "[[550.    5.5]\n",
      " [ 10.    0. ]\n",
      " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 依次放入每个query\n",
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b68810d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造mutil head attention层\n",
    "class MutilHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MutilHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # d_model 必须可以正确分为各个头\n",
    "        assert d_model % num_heads == 0\n",
    "        # 分头后的维度\n",
    "        self.depth = d_model // num_heads\n",
    "        \n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        # 分头, 将头个数的维度 放到 seq_len 前面\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        \n",
    "        # 分头前的前向网络，获取q、k、v语义\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "        \n",
    "        # 分头\n",
    "        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        # 通过缩放点积注意力层\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "        # 把多头维度后移\n",
    "        scaled_attention = tf.transpose(scaled_attention, [0, 2, 1, 3]) # (batch_size, seq_len_v, num_heads, depth)\n",
    "\n",
    "        # 合并多头\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))\n",
    "        \n",
    "        # 全连接重塑\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97eb483d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512) (1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "temp_mha = MutilHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))\n",
    "output, att = temp_mha(y, k=y, q=y, mask=None)\n",
    "print(output.shape, att.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "659fe414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, diff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(diff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "231b60c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_fnn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_fnn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02e0ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-6, **kwargs):\n",
    "        self.eps = epsilon\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma', shape=input_shape[-1:],\n",
    "                                     initializer=tf.ones_initializer(), trainable=True)\n",
    "        self.beta = self.add_weight(name='beta', shape=input_shape[-1:],\n",
    "                                    initializer=tf.zeros_initializer(), trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x):\n",
    "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54ce614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, ddf, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha = MutilHeadAttention(d_model, n_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, ddf)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, inputs, training, mask):\n",
    "        # 多头注意力网络\n",
    "        att_output, _ = self.mha(inputs, inputs, inputs, mask)\n",
    "        att_output = self.dropout1(att_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + att_output)  # (batch_size, input_seq_len, d_model)\n",
    "        # 前向网络\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4878cb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 43, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "sample_encoder_layer_output = sample_encoder_layer(\n",
    "tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "sample_encoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa41d737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, drop_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.mha1 = MutilHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MutilHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "        \n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dropout1 = layers.Dropout(drop_rate)\n",
    "        self.dropout2 = layers.Dropout(drop_rate)\n",
    "        self.dropout3 = layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self,inputs, encode_out, training, \n",
    "             look_ahead_mask, padding_mask):\n",
    "        # masked muti-head attention\n",
    "        att1, att_weight1 = self.mha1(inputs, inputs, inputs,look_ahead_mask)\n",
    "        att1 = self.dropout1(att1, training=training)\n",
    "        out1 = self.layernorm1(inputs + att1)\n",
    "        # muti-head attention\n",
    "        att2, att_weight2 = self.mha2(encode_out, encode_out, inputs, padding_mask)\n",
    "        att2 = self.dropout2(att2, training=training)\n",
    "        out2 = self.layernorm2(out1 + att2)\n",
    "        \n",
    "        ffn_out = self.ffn(out2)\n",
    "        ffn_out = self.dropout3(ffn_out, training=training)\n",
    "        out3 = self.layernorm3(out2 + ffn_out)\n",
    "        \n",
    "        return out3, att_weight1, att_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d90c2378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 50, 512])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
    "tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
    "    False, None, None)\n",
    "sample_decoder_layer_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0afb4f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                input_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.encode_layer = [EncoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                            for _ in range(n_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "    def call(self, inputs, training, mark):\n",
    "        \n",
    "        seq_len = inputs.shape[1]\n",
    "        word_emb = self.embedding(inputs)\n",
    "        word_emb *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        emb = word_emb + self.pos_embedding[:,:seq_len,:]\n",
    "        x = self.dropout(emb, training=training)\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.encode_layer[i](x, training, mark)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17a7db2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 120, 512])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_encoder = Encoder(2, 512, 8, 1024, 5000, 200)\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 120)),\n",
    "                                      False, None)\n",
    "sample_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "70a9a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdb\n",
    "# pdb.set_trace()\n",
    "class Decoder(layers.Layer):\n",
    "    def __init__(self, n_layers, d_model, n_heads, ddf,\n",
    "                target_vocab_size, max_seq_len, drop_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = positional_encoding(max_seq_len, d_model)\n",
    "        \n",
    "        self.decoder_layers= [DecoderLayer(d_model, n_heads, ddf, drop_rate)\n",
    "                             for _ in range(n_layers)]\n",
    "        \n",
    "        self.dropout = layers.Dropout(drop_rate)\n",
    "        \n",
    "    def call(self, inputs, encoder_out,training,\n",
    "             look_ahead_mark, padding_mark):\n",
    "    \n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        attention_weights = {}\n",
    "        h = self.embedding(inputs)\n",
    "        h *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        h += self.pos_embedding[:,:seq_len,:]\n",
    "        \n",
    "        h = self.dropout(h, training=training)\n",
    "#         print('--------------------\\n',h, h.shape)\n",
    "        # 叠加解码层\n",
    "        for i in range(self.n_layers):\n",
    "            h, att_w1, att_w2 = self.decoder_layers[i](h, encoder_out,\n",
    "                                                   training, look_ahead_mark,\n",
    "                                                   padding_mark)\n",
    "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att_w1\n",
    "            attention_weights['decoder_layer{}_att_w2'.format(i+1)] = att_w2\n",
    "        \n",
    "        return h, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfa5622f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 100, 512]), TensorShape([64, 8, 100, 100]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_decoder = Decoder(2, 512,8,1024,5000, 200)\n",
    "sample_decoder_output, attn = sample_decoder(tf.random.uniform((64, 100)),\n",
    "                                            sample_encoder_output, False,\n",
    "                                            None, None)\n",
    "sample_decoder_output.shape, attn['decoder_layer1_att_w1'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "621141bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, n_layers, d_model, n_heads, diff,\n",
    "                input_vocab_size, target_vocab_size,\n",
    "                max_seq_len, drop_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads,diff,\n",
    "                              input_vocab_size, max_seq_len, drop_rate)\n",
    "        \n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, diff,\n",
    "                              target_vocab_size, max_seq_len, drop_rate)\n",
    "        \n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    def call(self, inputs, targets, training, encode_padding_mask, \n",
    "            look_ahead_mask, decode_padding_mask):\n",
    "        \n",
    "        encode_out = self.encoder(inputs, training, encode_padding_mask)\n",
    "        print(encode_out.shape)\n",
    "        decode_out, att_weights = self.decoder(targets, encode_out, training, \n",
    "                                               look_ahead_mask, decode_padding_mask)\n",
    "        print(decode_out.shape)\n",
    "        final_out = self.final_layer(decode_out)\n",
    "        \n",
    "        return final_out, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ca7ee89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 62, 512)\n",
      "(64, 26, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([64, 26, 8000])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_transformer = Transformer(\n",
    "n_layers=2, d_model=512, n_heads=8, diff=1024,\n",
    "input_vocab_size=8500, target_vocab_size=8000, max_seq_len=120\n",
    ")\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
    "                              encode_padding_mask=None,\n",
    "                               look_ahead_mask=None,\n",
    "                               decode_padding_mask=None,\n",
    "                              )\n",
    "fn_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bb56fb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 8\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 16\n",
    "\n",
    "input_vocab_size = tokenizer_cn.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "max_seq_len = 40\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c542abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdcb3200",
   "metadata": {},
   "outputs": [],
   "source": [
    "learing_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learing_rate, beta_1=0.9, \n",
    "                                    beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a245de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'learning rate')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 测试\n",
    "temp_learing_rate = CustomSchedule(d_model)\n",
    "plt.plot(temp_learing_rate(tf.range(40000, dtype=tf.float32)))\n",
    "plt.xlabel('train step')\n",
    "plt.ylabel('learning rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a4ea9f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                           reduction='none')\n",
    "\n",
    "def loss_fun(y_ture, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_ture, 0))  # 为0掩码标1\n",
    "    loss_ = loss_object(y_ture, y_pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9715f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85321a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size,\n",
    "                          max_seq_len, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86acb5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建掩码\n",
    "def create_mask(inputs,targets):\n",
    "    encode_padding_mask = create_padding_mark(inputs)\n",
    "    # 这个掩码用于掩输入解码层第二层的编码层输出\n",
    "    decode_padding_mask = create_padding_mark(inputs)\n",
    "    \n",
    "    # look_ahead 掩码， 掩掉未预测的词\n",
    "    look_ahead_mask = create_look_ahead_mark(tf.shape(targets)[1])\n",
    "    # 解码层第一层得到padding掩码\n",
    "    decode_targets_padding_mask = create_padding_mark(targets)\n",
    "    \n",
    "    # 合并解码层第一层掩码\n",
    "    combine_mask = tf.maximum(decode_targets_padding_mask, look_ahead_mask)\n",
    "    \n",
    "    return encode_padding_mask, combine_mask, decode_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d554b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = './checkpoint/train'\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                          optimizer=optimizer)\n",
    "# ckpt管理器\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('last checkpoit restore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a662ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    tar_inp = targets[:,:-1]\n",
    "    tar_real = targets[:,1:]\n",
    "    # 构造掩码\n",
    "    encode_padding_mask, combined_mask, decode_padding_mask = create_mask(inputs, tar_inp)\n",
    "    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inputs, tar_inp,\n",
    "                                    True,\n",
    "                                    encode_padding_mask,\n",
    "                                    combined_mask,\n",
    "                                    decode_padding_mask)\n",
    "        loss = loss_fun(tar_real, predictions)\n",
    "    # 求梯度\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "    # 反向传播\n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "    \n",
    "    # 记录loss和准确率\n",
    "    train_loss(loss)\n",
    "    train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b3e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "(64, 40, 128)\n",
      "(64, 39, 128)\n",
      "epoch 1, batch 0, loss:6.9924, acc:0.0000\n",
      "epoch 1, batch 500, loss:4.4671, acc:0.1526\n",
      "epoch 1, batch 1000, loss:3.4572, acc:0.2368\n",
      "epoch 1, batch 1500, loss:3.0348, acc:0.2795\n",
      "epoch 1, batch 2000, loss:2.7730, acc:0.3132\n",
      "epoch 1, batch 2500, loss:2.5906, acc:0.3398\n",
      "epoch 1, batch 3000, loss:2.4434, acc:0.3638\n",
      "epoch 1, batch 3500, loss:2.3355, acc:0.3831\n",
      "(48, 40, 128)\n",
      "(48, 39, 128)\n",
      "epoch 1, loss:2.3241, acc:0.3852\n",
      "time in 1 epoch:382.43542885780334 secs\n",
      "\n",
      "epoch 2, batch 0, loss:1.5069, acc:0.5220\n",
      "epoch 2, batch 500, loss:1.5749, acc:0.5149\n",
      "epoch 2, batch 1000, loss:1.5483, acc:0.5264\n",
      "epoch 2, batch 1500, loss:1.5232, acc:0.5339\n",
      "epoch 2, batch 2000, loss:1.5024, acc:0.5400\n",
      "epoch 2, batch 2500, loss:1.4876, acc:0.5441\n",
      "epoch 2, batch 3000, loss:1.4692, acc:0.5488\n",
      "epoch 2, batch 3500, loss:1.4571, acc:0.5523\n",
      "epoch 2, save model at ./checkpoint/train\\ckpt-1\n",
      "epoch 2, loss:1.4554, acc:0.5528\n",
      "time in 1 epoch:345.80479669570923 secs\n",
      "\n",
      "epoch 3, batch 0, loss:1.2732, acc:0.5905\n",
      "epoch 3, batch 500, loss:1.3299, acc:0.5778\n",
      "epoch 3, batch 1000, loss:1.3244, acc:0.5838\n",
      "epoch 3, batch 1500, loss:1.3195, acc:0.5862\n",
      "epoch 3, batch 2000, loss:1.3150, acc:0.5882\n",
      "epoch 3, batch 2500, loss:1.3134, acc:0.5890\n",
      "epoch 3, batch 3000, loss:1.3068, acc:0.5907\n",
      "epoch 3, batch 3500, loss:1.3033, acc:0.5919\n",
      "epoch 3, loss:1.3027, acc:0.5921\n",
      "time in 1 epoch:345.2712826728821 secs\n",
      "\n",
      "epoch 4, batch 0, loss:1.1968, acc:0.6102\n",
      "epoch 4, batch 500, loss:1.2494, acc:0.5997\n",
      "epoch 4, batch 1000, loss:1.2466, acc:0.6047\n",
      "epoch 4, batch 1500, loss:1.2456, acc:0.6061\n",
      "epoch 4, batch 2000, loss:1.2445, acc:0.6072\n",
      "epoch 4, batch 2500, loss:1.2455, acc:0.6073\n",
      "epoch 4, batch 3000, loss:1.2418, acc:0.6083\n",
      "epoch 4, batch 3500, loss:1.2414, acc:0.6086\n",
      "epoch 4, save model at ./checkpoint/train\\ckpt-2\n",
      "epoch 4, loss:1.2411, acc:0.6088\n",
      "time in 1 epoch:346.04415488243103 secs\n",
      "\n",
      "epoch 5, batch 0, loss:1.1452, acc:0.6218\n",
      "epoch 5, batch 500, loss:1.2064, acc:0.6111\n",
      "epoch 5, batch 1000, loss:1.2053, acc:0.6161\n",
      "epoch 5, batch 1500, loss:1.2050, acc:0.6171\n",
      "epoch 5, batch 2000, loss:1.2050, acc:0.6180\n",
      "epoch 5, batch 2500, loss:1.2072, acc:0.6178\n",
      "epoch 5, batch 3000, loss:1.2046, acc:0.6184\n",
      "epoch 5, batch 3500, loss:1.2048, acc:0.6186\n",
      "epoch 5, loss:1.2046, acc:0.6188\n",
      "time in 1 epoch:345.31314063072205 secs\n",
      "\n",
      "epoch 6, batch 0, loss:1.1389, acc:0.6294\n",
      "epoch 6, batch 500, loss:1.1776, acc:0.6190\n",
      "epoch 6, batch 1000, loss:1.1770, acc:0.6239\n",
      "epoch 6, batch 1500, loss:1.1778, acc:0.6249\n",
      "epoch 6, batch 2000, loss:1.1785, acc:0.6254\n",
      "epoch 6, batch 2500, loss:1.1810, acc:0.6251\n",
      "epoch 6, batch 3000, loss:1.1790, acc:0.6257\n",
      "epoch 6, batch 3500, loss:1.1797, acc:0.6257\n",
      "epoch 6, save model at ./checkpoint/train\\ckpt-3\n",
      "epoch 6, loss:1.1795, acc:0.6258\n",
      "time in 1 epoch:347.5900478363037 secs\n",
      "\n",
      "epoch 7, batch 0, loss:1.1179, acc:0.6286\n",
      "epoch 7, batch 500, loss:1.1578, acc:0.6245\n",
      "epoch 7, batch 1000, loss:1.1570, acc:0.6294\n",
      "epoch 7, batch 1500, loss:1.1576, acc:0.6305\n",
      "epoch 7, batch 2000, loss:1.1586, acc:0.6310\n",
      "epoch 7, batch 2500, loss:1.1614, acc:0.6307\n",
      "epoch 7, batch 3000, loss:1.1600, acc:0.6310\n",
      "epoch 7, batch 3500, loss:1.1610, acc:0.6310\n",
      "epoch 7, loss:1.1609, acc:0.6311\n",
      "time in 1 epoch:345.4657335281372 secs\n",
      "\n",
      "epoch 8, batch 0, loss:1.0967, acc:0.6410\n",
      "epoch 8, batch 500, loss:1.1409, acc:0.6296\n",
      "epoch 8, batch 1000, loss:1.1410, acc:0.6341\n",
      "epoch 8, batch 1500, loss:1.1421, acc:0.6348\n",
      "epoch 8, batch 2000, loss:1.1434, acc:0.6353\n",
      "epoch 8, batch 2500, loss:1.1464, acc:0.6348\n",
      "epoch 8, batch 3000, loss:1.1452, acc:0.6351\n",
      "epoch 8, batch 3500, loss:1.1463, acc:0.6350\n",
      "epoch 8, save model at ./checkpoint/train\\ckpt-4\n",
      "epoch 8, loss:1.1462, acc:0.6351\n",
      "time in 1 epoch:345.6392676830292 secs\n",
      "\n",
      "epoch 9, batch 0, loss:1.0813, acc:0.6462\n",
      "epoch 9, batch 500, loss:1.1285, acc:0.6329\n",
      "epoch 9, batch 1000, loss:1.1289, acc:0.6375\n",
      "epoch 9, batch 1500, loss:1.1299, acc:0.6383\n",
      "epoch 9, batch 2000, loss:1.1312, acc:0.6388\n",
      "epoch 9, batch 2500, loss:1.1342, acc:0.6384\n",
      "epoch 9, batch 3000, loss:1.1332, acc:0.6387\n",
      "epoch 9, batch 3500, loss:1.1344, acc:0.6386\n",
      "epoch 9, loss:1.1343, acc:0.6387\n",
      "time in 1 epoch:346.9707818031311 secs\n",
      "\n",
      "epoch 10, batch 0, loss:1.0645, acc:0.6502\n",
      "epoch 10, batch 500, loss:1.1180, acc:0.6359\n",
      "epoch 10, batch 1000, loss:1.1178, acc:0.6408\n",
      "epoch 10, batch 1500, loss:1.1191, acc:0.6414\n",
      "epoch 10, batch 2000, loss:1.1206, acc:0.6418\n",
      "epoch 10, batch 2500, loss:1.1239, acc:0.6413\n",
      "epoch 10, batch 3000, loss:1.1230, acc:0.6415\n",
      "epoch 10, batch 3500, loss:1.1243, acc:0.6413\n",
      "epoch 10, save model at ./checkpoint/train\\ckpt-5\n",
      "epoch 10, loss:1.1242, acc:0.6414\n",
      "time in 1 epoch:347.73466181755066 secs\n",
      "\n",
      "epoch 11, batch 0, loss:1.0593, acc:0.6518\n",
      "epoch 11, batch 500, loss:1.1086, acc:0.6386\n",
      "epoch 11, batch 1000, loss:1.1087, acc:0.6433\n",
      "epoch 11, batch 1500, loss:1.1101, acc:0.6440\n",
      "epoch 11, batch 2000, loss:1.1116, acc:0.6444\n",
      "epoch 11, batch 2500, loss:1.1148, acc:0.6438\n",
      "epoch 11, batch 3000, loss:1.1140, acc:0.6440\n",
      "epoch 11, batch 3500, loss:1.1155, acc:0.6438\n",
      "epoch 11, loss:1.1154, acc:0.6439\n",
      "time in 1 epoch:347.1103324890137 secs\n",
      "\n",
      "epoch 12, batch 0, loss:1.0561, acc:0.6530\n",
      "epoch 12, batch 500, loss:1.1018, acc:0.6408\n",
      "epoch 12, batch 1000, loss:1.1016, acc:0.6454\n",
      "epoch 12, batch 1500, loss:1.1030, acc:0.6461\n",
      "epoch 12, batch 2000, loss:1.1046, acc:0.6465\n",
      "epoch 12, batch 2500, loss:1.1078, acc:0.6460\n",
      "epoch 12, batch 3000, loss:1.1069, acc:0.6462\n",
      "epoch 12, batch 3500, loss:1.1083, acc:0.6459\n",
      "epoch 12, save model at ./checkpoint/train\\ckpt-6\n",
      "epoch 12, loss:1.1082, acc:0.6460\n",
      "time in 1 epoch:347.14623641967773 secs\n",
      "\n",
      "epoch 13, batch 0, loss:1.0469, acc:0.6595\n",
      "epoch 13, batch 500, loss:1.0947, acc:0.6428\n",
      "epoch 13, batch 1000, loss:1.0947, acc:0.6474\n",
      "epoch 13, batch 1500, loss:1.0961, acc:0.6480\n",
      "epoch 13, batch 2000, loss:1.0977, acc:0.6484\n",
      "epoch 13, batch 2500, loss:1.1010, acc:0.6479\n",
      "epoch 13, batch 3000, loss:1.1003, acc:0.6480\n",
      "epoch 13, batch 3500, loss:1.1019, acc:0.6478\n",
      "epoch 13, loss:1.1018, acc:0.6479\n",
      "time in 1 epoch:347.38662004470825 secs\n",
      "\n",
      "epoch 14, batch 0, loss:1.0445, acc:0.6510\n",
      "epoch 14, batch 500, loss:1.0892, acc:0.6443\n",
      "epoch 14, batch 1000, loss:1.0890, acc:0.6490\n",
      "epoch 14, batch 1500, loss:1.0902, acc:0.6496\n",
      "epoch 14, batch 2000, loss:1.0919, acc:0.6501\n",
      "epoch 14, batch 2500, loss:1.0951, acc:0.6495\n",
      "epoch 14, batch 3000, loss:1.0944, acc:0.6497\n",
      "epoch 14, batch 3500, loss:1.0959, acc:0.6494\n",
      "epoch 14, save model at ./checkpoint/train\\ckpt-7\n",
      "epoch 14, loss:1.0959, acc:0.6495\n",
      "time in 1 epoch:346.4191541671753 secs\n",
      "\n",
      "epoch 15, batch 0, loss:1.0439, acc:0.6526\n",
      "epoch 15, batch 500, loss:1.0835, acc:0.6456\n",
      "epoch 15, batch 1000, loss:1.0833, acc:0.6504\n",
      "epoch 15, batch 1500, loss:1.0847, acc:0.6510\n",
      "epoch 15, batch 2000, loss:1.0863, acc:0.6515\n",
      "epoch 15, batch 2500, loss:1.0896, acc:0.6511\n",
      "epoch 15, batch 3000, loss:1.0891, acc:0.6511\n",
      "epoch 15, batch 3500, loss:1.0907, acc:0.6508\n",
      "epoch 15, loss:1.0907, acc:0.6509\n",
      "time in 1 epoch:345.9913263320923 secs\n",
      "\n",
      "epoch 16, batch 0, loss:1.0336, acc:0.6567\n",
      "epoch 16, batch 500, loss:1.0783, acc:0.6472\n",
      "epoch 16, batch 1000, loss:1.0781, acc:0.6521\n",
      "epoch 16, batch 1500, loss:1.0796, acc:0.6526\n",
      "epoch 16, batch 2000, loss:1.0813, acc:0.6530\n",
      "epoch 16, batch 2500, loss:1.0846, acc:0.6525\n",
      "epoch 16, batch 3000, loss:1.0841, acc:0.6526\n",
      "epoch 16, batch 3500, loss:1.0857, acc:0.6523\n",
      "epoch 16, save model at ./checkpoint/train\\ckpt-8\n",
      "epoch 16, loss:1.0856, acc:0.6524\n",
      "time in 1 epoch:358.02658796310425 secs\n",
      "\n",
      "epoch 17, batch 0, loss:1.0340, acc:0.6538\n",
      "epoch 17, batch 500, loss:1.0738, acc:0.6484\n",
      "epoch 17, batch 1000, loss:1.0737, acc:0.6532\n",
      "epoch 17, batch 1500, loss:1.0752, acc:0.6538\n",
      "epoch 17, batch 2000, loss:1.0772, acc:0.6541\n",
      "epoch 17, batch 2500, loss:1.0806, acc:0.6536\n",
      "epoch 17, batch 3000, loss:1.0801, acc:0.6536\n",
      "epoch 17, batch 3500, loss:1.0816, acc:0.6534\n",
      "epoch 17, loss:1.0815, acc:0.6535\n",
      "time in 1 epoch:346.2037560939789 secs\n",
      "\n",
      "epoch 18, batch 0, loss:1.0389, acc:0.6490\n",
      "epoch 18, batch 500, loss:1.0700, acc:0.6496\n",
      "epoch 18, batch 1000, loss:1.0696, acc:0.6544\n",
      "epoch 18, batch 1500, loss:1.0712, acc:0.6549\n",
      "epoch 18, batch 2000, loss:1.0731, acc:0.6553\n",
      "epoch 18, batch 2500, loss:1.0764, acc:0.6547\n",
      "epoch 18, batch 3000, loss:1.0759, acc:0.6549\n",
      "epoch 18, batch 3500, loss:1.0775, acc:0.6546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, save model at ./checkpoint/train\\ckpt-9\n",
      "epoch 18, loss:1.0774, acc:0.6547\n",
      "time in 1 epoch:346.8669831752777 secs\n",
      "\n",
      "epoch 19, batch 0, loss:1.0426, acc:0.6518\n",
      "epoch 19, batch 500, loss:1.0661, acc:0.6506\n",
      "epoch 19, batch 1000, loss:1.0659, acc:0.6556\n",
      "epoch 19, batch 1500, loss:1.0675, acc:0.6561\n",
      "epoch 19, batch 2000, loss:1.0694, acc:0.6564\n",
      "epoch 19, batch 2500, loss:1.0728, acc:0.6559\n",
      "epoch 19, batch 3000, loss:1.0723, acc:0.6560\n",
      "epoch 19, batch 3500, loss:1.0740, acc:0.6557\n",
      "epoch 19, loss:1.0739, acc:0.6558\n",
      "time in 1 epoch:348.04583501815796 secs\n",
      "\n",
      "epoch 20, batch 0, loss:1.0289, acc:0.6583\n",
      "epoch 20, batch 500, loss:1.0630, acc:0.6516\n",
      "epoch 20, batch 1000, loss:1.0627, acc:0.6566\n",
      "epoch 20, batch 1500, loss:1.0643, acc:0.6571\n",
      "epoch 20, batch 2000, loss:1.0661, acc:0.6574\n",
      "epoch 20, batch 2500, loss:1.0694, acc:0.6569\n",
      "epoch 20, batch 3000, loss:1.0690, acc:0.6570\n",
      "epoch 20, batch 3500, loss:1.0706, acc:0.6567\n",
      "epoch 20, save model at ./checkpoint/train\\ckpt-10\n",
      "epoch 20, loss:1.0705, acc:0.6568\n",
      "time in 1 epoch:346.22070717811584 secs\n",
      "\n",
      "epoch 21, batch 0, loss:1.0251, acc:0.6607\n",
      "epoch 21, batch 500, loss:1.0592, acc:0.6526\n",
      "epoch 21, batch 1000, loss:1.0589, acc:0.6576\n",
      "epoch 21, batch 1500, loss:1.0605, acc:0.6581\n",
      "epoch 21, batch 2000, loss:1.0625, acc:0.6584\n",
      "epoch 21, batch 2500, loss:1.0658, acc:0.6578\n",
      "epoch 21, batch 3000, loss:1.0655, acc:0.6579\n",
      "epoch 21, batch 3500, loss:1.0672, acc:0.6576\n",
      "epoch 21, loss:1.0672, acc:0.6576\n",
      "time in 1 epoch:345.45279455184937 secs\n",
      "\n",
      "epoch 22, batch 0, loss:1.0184, acc:0.6595\n",
      "epoch 22, batch 500, loss:1.0561, acc:0.6538\n",
      "epoch 22, batch 1000, loss:1.0559, acc:0.6585\n",
      "epoch 22, batch 1500, loss:1.0576, acc:0.6590\n",
      "epoch 22, batch 2000, loss:1.0597, acc:0.6592\n",
      "epoch 22, batch 2500, loss:1.0630, acc:0.6587\n",
      "epoch 22, batch 3000, loss:1.0626, acc:0.6587\n",
      "epoch 22, batch 3500, loss:1.0642, acc:0.6584\n",
      "epoch 22, save model at ./checkpoint/train\\ckpt-11\n",
      "epoch 22, loss:1.0642, acc:0.6585\n",
      "time in 1 epoch:346.4740071296692 secs\n",
      "\n",
      "epoch 23, batch 0, loss:1.0127, acc:0.6635\n",
      "epoch 23, batch 500, loss:1.0536, acc:0.6542\n",
      "epoch 23, batch 1000, loss:1.0534, acc:0.6591\n",
      "epoch 23, batch 1500, loss:1.0550, acc:0.6596\n",
      "epoch 23, batch 2000, loss:1.0569, acc:0.6599\n",
      "epoch 23, batch 2500, loss:1.0604, acc:0.6594\n",
      "epoch 23, batch 3000, loss:1.0600, acc:0.6594\n",
      "epoch 23, batch 3500, loss:1.0616, acc:0.6591\n",
      "epoch 23, loss:1.0616, acc:0.6592\n",
      "time in 1 epoch:346.231693983078 secs\n",
      "\n",
      "epoch 24, batch 0, loss:1.0035, acc:0.6643\n",
      "epoch 24, batch 500, loss:1.0508, acc:0.6554\n",
      "epoch 24, batch 1000, loss:1.0509, acc:0.6599\n",
      "epoch 24, batch 1500, loss:1.0525, acc:0.6604\n",
      "epoch 24, batch 2000, loss:1.0544, acc:0.6607\n",
      "epoch 24, batch 2500, loss:1.0579, acc:0.6602\n",
      "epoch 24, batch 3000, loss:1.0575, acc:0.6602\n",
      "epoch 24, batch 3500, loss:1.0590, acc:0.6599\n",
      "epoch 24, save model at ./checkpoint/train\\ckpt-12\n",
      "epoch 24, loss:1.0590, acc:0.6600\n",
      "time in 1 epoch:347.047993183136 secs\n",
      "\n",
      "epoch 25, batch 0, loss:1.0035, acc:0.6695\n",
      "epoch 25, batch 500, loss:1.0481, acc:0.6560\n",
      "epoch 25, batch 1000, loss:1.0480, acc:0.6606\n",
      "epoch 25, batch 1500, loss:1.0496, acc:0.6613\n",
      "epoch 25, batch 2000, loss:1.0515, acc:0.6616\n",
      "epoch 25, batch 2500, loss:1.0550, acc:0.6610\n",
      "epoch 25, batch 3000, loss:1.0546, acc:0.6610\n",
      "epoch 25, batch 3500, loss:1.0562, acc:0.6607\n",
      "epoch 25, loss:1.0562, acc:0.6608\n",
      "time in 1 epoch:354.61420941352844 secs\n",
      "\n",
      "epoch 26, batch 0, loss:0.9921, acc:0.6695\n",
      "epoch 26, batch 500, loss:1.0455, acc:0.6568\n",
      "epoch 26, batch 1000, loss:1.0457, acc:0.6614\n",
      "epoch 26, batch 1500, loss:1.0473, acc:0.6620\n",
      "epoch 26, batch 2000, loss:1.0493, acc:0.6622\n",
      "epoch 26, batch 2500, loss:1.0526, acc:0.6616\n",
      "epoch 26, batch 3000, loss:1.0525, acc:0.6615\n",
      "epoch 26, batch 3500, loss:1.0542, acc:0.6612\n",
      "epoch 26, save model at ./checkpoint/train\\ckpt-13\n",
      "epoch 26, loss:1.0542, acc:0.6613\n",
      "time in 1 epoch:351.68035793304443 secs\n",
      "\n",
      "epoch 27, batch 0, loss:1.0021, acc:0.6695\n",
      "epoch 27, batch 500, loss:1.0436, acc:0.6572\n",
      "epoch 27, batch 1000, loss:1.0433, acc:0.6620\n",
      "epoch 27, batch 1500, loss:1.0450, acc:0.6625\n",
      "epoch 27, batch 2000, loss:1.0470, acc:0.6628\n",
      "epoch 27, batch 2500, loss:1.0505, acc:0.6621\n",
      "epoch 27, batch 3000, loss:1.0502, acc:0.6622\n",
      "epoch 27, batch 3500, loss:1.0518, acc:0.6618\n",
      "epoch 27, loss:1.0518, acc:0.6619\n",
      "time in 1 epoch:346.09807109832764 secs\n",
      "\n",
      "epoch 28, batch 0, loss:1.0016, acc:0.6587\n",
      "epoch 28, batch 500, loss:1.0415, acc:0.6580\n",
      "epoch 28, batch 1000, loss:1.0413, acc:0.6628\n",
      "epoch 28, batch 1500, loss:1.0429, acc:0.6632\n",
      "epoch 28, batch 2000, loss:1.0450, acc:0.6634\n",
      "epoch 28, batch 2500, loss:1.0486, acc:0.6629\n",
      "epoch 28, batch 3000, loss:1.0482, acc:0.6629\n",
      "epoch 28, batch 3500, loss:1.0500, acc:0.6626\n",
      "epoch 28, save model at ./checkpoint/train\\ckpt-14\n",
      "epoch 28, loss:1.0499, acc:0.6627\n",
      "time in 1 epoch:346.9008629322052 secs\n",
      "\n",
      "epoch 29, batch 0, loss:1.0086, acc:0.6647\n",
      "epoch 29, batch 500, loss:1.0394, acc:0.6583\n",
      "epoch 29, batch 1000, loss:1.0393, acc:0.6632\n",
      "epoch 29, batch 1500, loss:1.0410, acc:0.6638\n",
      "epoch 29, batch 2000, loss:1.0431, acc:0.6640\n",
      "epoch 29, batch 2500, loss:1.0465, acc:0.6634\n",
      "epoch 29, batch 3000, loss:1.0463, acc:0.6634\n",
      "epoch 29, batch 3500, loss:1.0479, acc:0.6631\n",
      "epoch 29, loss:1.0479, acc:0.6632\n",
      "time in 1 epoch:346.56180024147034 secs\n",
      "\n",
      "epoch 30, batch 0, loss:0.9861, acc:0.6659\n",
      "epoch 30, batch 500, loss:1.0377, acc:0.6591\n",
      "epoch 30, batch 1000, loss:1.0377, acc:0.6636\n",
      "epoch 30, batch 1500, loss:1.0395, acc:0.6641\n",
      "epoch 30, batch 2000, loss:1.0414, acc:0.6643\n",
      "epoch 30, batch 2500, loss:1.0447, acc:0.6638\n",
      "epoch 30, batch 3000, loss:1.0444, acc:0.6639\n",
      "epoch 30, batch 3500, loss:1.0460, acc:0.6636\n",
      "epoch 30, save model at ./checkpoint/train\\ckpt-15\n",
      "epoch 30, loss:1.0460, acc:0.6637\n",
      "time in 1 epoch:351.5215301513672 secs\n",
      "\n",
      "epoch 31, batch 0, loss:1.0137, acc:0.6607\n",
      "epoch 31, batch 500, loss:1.0358, acc:0.6598\n",
      "epoch 31, batch 1000, loss:1.0356, acc:0.6644\n",
      "epoch 31, batch 1500, loss:1.0374, acc:0.6648\n",
      "epoch 31, batch 2000, loss:1.0392, acc:0.6652\n",
      "epoch 31, batch 2500, loss:1.0427, acc:0.6645\n",
      "epoch 31, batch 3000, loss:1.0424, acc:0.6646\n",
      "epoch 31, batch 3500, loss:1.0441, acc:0.6642\n",
      "epoch 31, loss:1.0440, acc:0.6643\n",
      "time in 1 epoch:358.07216811180115 secs\n",
      "\n",
      "epoch 32, batch 0, loss:0.9807, acc:0.6731\n",
      "epoch 32, batch 500, loss:1.0341, acc:0.6600\n",
      "epoch 32, batch 1000, loss:1.0342, acc:0.6647\n",
      "epoch 32, batch 1500, loss:1.0358, acc:0.6652\n",
      "epoch 32, batch 2000, loss:1.0379, acc:0.6654\n",
      "epoch 32, batch 2500, loss:1.0412, acc:0.6649\n",
      "epoch 32, batch 3000, loss:1.0410, acc:0.6649\n",
      "epoch 32, batch 3500, loss:1.0427, acc:0.6646\n",
      "epoch 32, save model at ./checkpoint/train\\ckpt-16\n",
      "epoch 32, loss:1.0426, acc:0.6647\n",
      "time in 1 epoch:405.8947353363037 secs\n",
      "\n",
      "epoch 33, batch 0, loss:0.9833, acc:0.6751\n",
      "epoch 33, batch 500, loss:1.0327, acc:0.6604\n",
      "epoch 33, batch 1000, loss:1.0324, acc:0.6650\n",
      "epoch 33, batch 1500, loss:1.0340, acc:0.6656\n",
      "epoch 33, batch 2000, loss:1.0361, acc:0.6659\n",
      "epoch 33, batch 2500, loss:1.0395, acc:0.6653\n",
      "epoch 33, batch 3000, loss:1.0394, acc:0.6653\n",
      "epoch 33, batch 3500, loss:1.0410, acc:0.6650\n",
      "epoch 33, loss:1.0409, acc:0.6651\n",
      "time in 1 epoch:378.103502035141 secs\n",
      "\n",
      "epoch 34, batch 0, loss:0.9924, acc:0.6683\n",
      "epoch 34, batch 500, loss:1.0305, acc:0.6611\n",
      "epoch 34, batch 1000, loss:1.0306, acc:0.6659\n",
      "epoch 34, batch 1500, loss:1.0323, acc:0.6664\n",
      "epoch 34, batch 2000, loss:1.0343, acc:0.6666\n",
      "epoch 34, batch 2500, loss:1.0379, acc:0.6660\n",
      "epoch 34, batch 3000, loss:1.0377, acc:0.6660\n",
      "epoch 34, batch 3500, loss:1.0393, acc:0.6657\n",
      "epoch 34, save model at ./checkpoint/train\\ckpt-17\n",
      "epoch 34, loss:1.0393, acc:0.6658\n",
      "time in 1 epoch:398.2215881347656 secs\n",
      "\n",
      "epoch 35, batch 0, loss:0.9907, acc:0.6611\n",
      "epoch 35, batch 500, loss:1.0291, acc:0.6613\n",
      "epoch 35, batch 1000, loss:1.0291, acc:0.6660\n",
      "epoch 35, batch 1500, loss:1.0308, acc:0.6665\n",
      "epoch 35, batch 2000, loss:1.0329, acc:0.6668\n",
      "epoch 35, batch 2500, loss:1.0362, acc:0.6663\n",
      "epoch 35, batch 3000, loss:1.0361, acc:0.6663\n",
      "epoch 35, batch 3500, loss:1.0378, acc:0.6660\n",
      "epoch 35, loss:1.0377, acc:0.6661\n",
      "time in 1 epoch:435.83675718307495 secs\n",
      "\n",
      "epoch 36, batch 0, loss:0.9811, acc:0.6779\n",
      "epoch 36, batch 500, loss:1.0280, acc:0.6618\n",
      "epoch 36, batch 1000, loss:1.0274, acc:0.6666\n",
      "epoch 36, batch 1500, loss:1.0293, acc:0.6670\n",
      "epoch 36, batch 2000, loss:1.0314, acc:0.6673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 36, batch 2500, loss:1.0348, acc:0.6667\n",
      "epoch 36, batch 3000, loss:1.0347, acc:0.6667\n",
      "epoch 36, batch 3500, loss:1.0364, acc:0.6664\n",
      "epoch 36, save model at ./checkpoint/train\\ckpt-18\n",
      "epoch 36, loss:1.0363, acc:0.6665\n",
      "time in 1 epoch:460.4980204105377 secs\n",
      "\n",
      "epoch 37, batch 0, loss:0.9858, acc:0.6735\n",
      "epoch 37, batch 500, loss:1.0267, acc:0.6622\n",
      "epoch 37, batch 1000, loss:1.0264, acc:0.6671\n",
      "epoch 37, batch 1500, loss:1.0282, acc:0.6676\n",
      "epoch 37, batch 2000, loss:1.0301, acc:0.6677\n",
      "epoch 37, batch 2500, loss:1.0336, acc:0.6672\n",
      "epoch 37, batch 3000, loss:1.0334, acc:0.6672\n",
      "epoch 37, batch 3500, loss:1.0350, acc:0.6669\n",
      "epoch 37, loss:1.0350, acc:0.6669\n",
      "time in 1 epoch:385.66047072410583 secs\n",
      "\n",
      "epoch 38, batch 0, loss:0.9792, acc:0.6679\n",
      "epoch 38, batch 500, loss:1.0249, acc:0.6629\n",
      "epoch 38, batch 1000, loss:1.0248, acc:0.6673\n",
      "epoch 38, batch 1500, loss:1.0265, acc:0.6678\n",
      "epoch 38, batch 2000, loss:1.0286, acc:0.6681\n",
      "epoch 38, batch 2500, loss:1.0320, acc:0.6675\n",
      "epoch 38, batch 3000, loss:1.0319, acc:0.6675\n",
      "epoch 38, batch 3500, loss:1.0336, acc:0.6672\n",
      "epoch 38, save model at ./checkpoint/train\\ckpt-19\n",
      "epoch 38, loss:1.0336, acc:0.6673\n",
      "time in 1 epoch:375.92278385162354 secs\n",
      "\n",
      "epoch 39, batch 0, loss:0.9917, acc:0.6723\n",
      "epoch 39, batch 500, loss:1.0242, acc:0.6628\n",
      "epoch 39, batch 1000, loss:1.0237, acc:0.6677\n",
      "epoch 39, batch 1500, loss:1.0253, acc:0.6683\n",
      "epoch 39, batch 2000, loss:1.0273, acc:0.6685\n",
      "epoch 39, batch 2500, loss:1.0307, acc:0.6679\n",
      "epoch 39, batch 3000, loss:1.0306, acc:0.6678\n",
      "epoch 39, batch 3500, loss:1.0322, acc:0.6675\n",
      "epoch 39, loss:1.0322, acc:0.6676\n",
      "time in 1 epoch:364.3140425682068 secs\n",
      "\n",
      "epoch 40, batch 0, loss:0.9852, acc:0.6667\n",
      "epoch 40, batch 500, loss:1.0222, acc:0.6637\n",
      "epoch 40, batch 1000, loss:1.0219, acc:0.6682\n",
      "epoch 40, batch 1500, loss:1.0237, acc:0.6687\n",
      "epoch 40, batch 2000, loss:1.0260, acc:0.6689\n",
      "epoch 40, batch 2500, loss:1.0295, acc:0.6683\n",
      "epoch 40, batch 3000, loss:1.0294, acc:0.6683\n",
      "epoch 40, batch 3500, loss:1.0311, acc:0.6680\n",
      "epoch 40, save model at ./checkpoint/train\\ckpt-20\n",
      "epoch 40, loss:1.0311, acc:0.6680\n",
      "time in 1 epoch:366.14740014076233 secs\n",
      "\n",
      "epoch 41, batch 0, loss:0.9766, acc:0.6763\n",
      "epoch 41, batch 500, loss:1.0216, acc:0.6637\n",
      "epoch 41, batch 1000, loss:1.0210, acc:0.6686\n",
      "epoch 41, batch 1500, loss:1.0229, acc:0.6690\n",
      "epoch 41, batch 2000, loss:1.0249, acc:0.6692\n",
      "epoch 41, batch 2500, loss:1.0284, acc:0.6686\n",
      "epoch 41, batch 3000, loss:1.0282, acc:0.6685\n",
      "epoch 41, batch 3500, loss:1.0299, acc:0.6682\n",
      "epoch 41, loss:1.0298, acc:0.6683\n",
      "time in 1 epoch:361.0502724647522 secs\n",
      "\n",
      "epoch 42, batch 0, loss:0.9804, acc:0.6627\n",
      "epoch 42, batch 500, loss:1.0199, acc:0.6640\n",
      "epoch 42, batch 1000, loss:1.0197, acc:0.6688\n",
      "epoch 42, batch 1500, loss:1.0214, acc:0.6693\n",
      "epoch 42, batch 2000, loss:1.0235, acc:0.6695\n",
      "epoch 42, batch 2500, loss:1.0270, acc:0.6689\n",
      "epoch 42, batch 3000, loss:1.0270, acc:0.6689\n",
      "epoch 42, batch 3500, loss:1.0287, acc:0.6686\n",
      "epoch 42, save model at ./checkpoint/train\\ckpt-21\n",
      "epoch 42, loss:1.0287, acc:0.6687\n",
      "time in 1 epoch:362.88204526901245 secs\n",
      "\n",
      "epoch 43, batch 0, loss:0.9818, acc:0.6739\n",
      "epoch 43, batch 500, loss:1.0187, acc:0.6641\n",
      "epoch 43, batch 1000, loss:1.0186, acc:0.6690\n",
      "epoch 43, batch 1500, loss:1.0202, acc:0.6696\n",
      "epoch 43, batch 2000, loss:1.0224, acc:0.6698\n",
      "epoch 43, batch 2500, loss:1.0258, acc:0.6693\n",
      "epoch 43, batch 3000, loss:1.0257, acc:0.6693\n",
      "epoch 43, batch 3500, loss:1.0274, acc:0.6690\n",
      "epoch 43, loss:1.0273, acc:0.6691\n",
      "time in 1 epoch:362.9420063495636 secs\n",
      "\n",
      "epoch 44, batch 0, loss:0.9724, acc:0.6739\n",
      "epoch 44, batch 500, loss:1.0179, acc:0.6645\n",
      "epoch 44, batch 1000, loss:1.0176, acc:0.6693\n",
      "epoch 44, batch 1500, loss:1.0194, acc:0.6698\n",
      "epoch 44, batch 2000, loss:1.0215, acc:0.6700\n",
      "epoch 44, batch 2500, loss:1.0249, acc:0.6695\n",
      "epoch 44, batch 3000, loss:1.0247, acc:0.6695\n",
      "epoch 44, batch 3500, loss:1.0264, acc:0.6692\n",
      "epoch 44, save model at ./checkpoint/train\\ckpt-22\n",
      "epoch 44, loss:1.0264, acc:0.6693\n",
      "time in 1 epoch:369.55927205085754 secs\n",
      "\n",
      "epoch 45, batch 0, loss:0.9642, acc:0.6819\n",
      "epoch 45, batch 500, loss:1.0171, acc:0.6647\n",
      "epoch 45, batch 1000, loss:1.0168, acc:0.6696\n",
      "epoch 45, batch 1500, loss:1.0184, acc:0.6702\n",
      "epoch 45, batch 2000, loss:1.0205, acc:0.6704\n",
      "epoch 45, batch 2500, loss:1.0239, acc:0.6698\n",
      "epoch 45, batch 3000, loss:1.0238, acc:0.6698\n",
      "epoch 45, batch 3500, loss:1.0256, acc:0.6695\n",
      "epoch 45, loss:1.0255, acc:0.6696\n",
      "time in 1 epoch:362.4715270996094 secs\n",
      "\n",
      "epoch 46, batch 0, loss:0.9810, acc:0.6751\n",
      "epoch 46, batch 500, loss:1.0162, acc:0.6651\n",
      "epoch 46, batch 1000, loss:1.0158, acc:0.6698\n",
      "epoch 46, batch 1500, loss:1.0175, acc:0.6704\n",
      "epoch 46, batch 2000, loss:1.0197, acc:0.6706\n",
      "epoch 46, batch 2500, loss:1.0230, acc:0.6701\n",
      "epoch 46, batch 3000, loss:1.0228, acc:0.6701\n",
      "epoch 46, batch 3500, loss:1.0245, acc:0.6698\n",
      "epoch 46, save model at ./checkpoint/train\\ckpt-23\n",
      "epoch 46, loss:1.0244, acc:0.6699\n",
      "time in 1 epoch:365.58511686325073 secs\n",
      "\n",
      "epoch 47, batch 0, loss:0.9786, acc:0.6715\n",
      "epoch 47, batch 500, loss:1.0152, acc:0.6652\n",
      "epoch 47, batch 1000, loss:1.0143, acc:0.6702\n",
      "epoch 47, batch 1500, loss:1.0162, acc:0.6708\n",
      "epoch 47, batch 2000, loss:1.0183, acc:0.6710\n",
      "epoch 47, batch 2500, loss:1.0218, acc:0.6704\n",
      "epoch 47, batch 3000, loss:1.0217, acc:0.6704\n",
      "epoch 47, batch 3500, loss:1.0234, acc:0.6701\n",
      "epoch 47, loss:1.0233, acc:0.6702\n",
      "time in 1 epoch:365.30786633491516 secs\n",
      "\n",
      "epoch 48, batch 0, loss:0.9660, acc:0.6763\n",
      "epoch 48, batch 500, loss:1.0135, acc:0.6660\n",
      "epoch 48, batch 1000, loss:1.0137, acc:0.6705\n",
      "epoch 48, batch 1500, loss:1.0154, acc:0.6710\n",
      "epoch 48, batch 2000, loss:1.0174, acc:0.6713\n",
      "epoch 48, batch 2500, loss:1.0208, acc:0.6707\n",
      "epoch 48, batch 3000, loss:1.0206, acc:0.6707\n",
      "epoch 48, batch 3500, loss:1.0223, acc:0.6704\n",
      "epoch 48, save model at ./checkpoint/train\\ckpt-24\n",
      "epoch 48, loss:1.0223, acc:0.6704\n",
      "time in 1 epoch:366.37879180908203 secs\n",
      "\n",
      "epoch 49, batch 0, loss:0.9609, acc:0.6835\n",
      "epoch 49, batch 500, loss:1.0139, acc:0.6657\n",
      "epoch 49, batch 1000, loss:1.0133, acc:0.6705\n",
      "epoch 49, batch 1500, loss:1.0149, acc:0.6711\n",
      "epoch 49, batch 2000, loss:1.0168, acc:0.6713\n",
      "epoch 49, batch 2500, loss:1.0202, acc:0.6708\n",
      "epoch 49, batch 3000, loss:1.0201, acc:0.6708\n",
      "epoch 49, batch 3500, loss:1.0217, acc:0.6705\n",
      "epoch 49, loss:1.0216, acc:0.6706\n",
      "time in 1 epoch:346.9158799648285 secs\n",
      "\n",
      "epoch 50, batch 0, loss:0.9709, acc:0.6683\n",
      "epoch 50, batch 500, loss:1.0124, acc:0.6660\n",
      "epoch 50, batch 1000, loss:1.0118, acc:0.6711\n",
      "epoch 50, batch 1500, loss:1.0134, acc:0.6717\n",
      "epoch 50, batch 2000, loss:1.0156, acc:0.6718\n",
      "epoch 50, batch 2500, loss:1.0190, acc:0.6712\n",
      "epoch 50, batch 3000, loss:1.0188, acc:0.6712\n",
      "epoch 50, batch 3500, loss:1.0205, acc:0.6709\n",
      "epoch 50, save model at ./checkpoint/train\\ckpt-25\n",
      "epoch 50, loss:1.0205, acc:0.6710\n",
      "time in 1 epoch:349.0630805492401 secs\n",
      "\n",
      "epoch 51, batch 0, loss:0.9664, acc:0.6735\n",
      "epoch 51, batch 500, loss:1.0117, acc:0.6665\n",
      "epoch 51, batch 1000, loss:1.0111, acc:0.6713\n",
      "epoch 51, batch 1500, loss:1.0127, acc:0.6719\n",
      "epoch 51, batch 2000, loss:1.0148, acc:0.6721\n",
      "epoch 51, batch 2500, loss:1.0180, acc:0.6716\n",
      "epoch 51, batch 3000, loss:1.0179, acc:0.6716\n",
      "epoch 51, batch 3500, loss:1.0196, acc:0.6713\n",
      "epoch 51, loss:1.0196, acc:0.6713\n",
      "time in 1 epoch:353.15419006347656 secs\n",
      "\n",
      "epoch 52, batch 0, loss:0.9697, acc:0.6775\n",
      "epoch 52, batch 500, loss:1.0101, acc:0.6672\n",
      "epoch 52, batch 1000, loss:1.0101, acc:0.6718\n",
      "epoch 52, batch 1500, loss:1.0116, acc:0.6724\n",
      "epoch 52, batch 2000, loss:1.0137, acc:0.6725\n",
      "epoch 52, batch 2500, loss:1.0171, acc:0.6719\n",
      "epoch 52, batch 3000, loss:1.0170, acc:0.6719\n",
      "epoch 52, batch 3500, loss:1.0186, acc:0.6715\n",
      "epoch 52, save model at ./checkpoint/train\\ckpt-26\n",
      "epoch 52, loss:1.0186, acc:0.6716\n",
      "time in 1 epoch:353.106262922287 secs\n",
      "\n",
      "epoch 53, batch 0, loss:0.9795, acc:0.6775\n",
      "epoch 53, batch 500, loss:1.0097, acc:0.6674\n",
      "epoch 53, batch 1000, loss:1.0091, acc:0.6720\n",
      "epoch 53, batch 1500, loss:1.0107, acc:0.6725\n",
      "epoch 53, batch 2000, loss:1.0128, acc:0.6727\n",
      "epoch 53, batch 2500, loss:1.0161, acc:0.6721\n",
      "epoch 53, batch 3000, loss:1.0160, acc:0.6721\n",
      "epoch 53, batch 3500, loss:1.0177, acc:0.6718\n",
      "epoch 53, loss:1.0177, acc:0.6718\n",
      "time in 1 epoch:351.9463939666748 secs\n",
      "\n",
      "epoch 54, batch 0, loss:0.9724, acc:0.6795\n",
      "epoch 54, batch 500, loss:1.0089, acc:0.6673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 54, batch 1000, loss:1.0081, acc:0.6721\n",
      "epoch 54, batch 1500, loss:1.0099, acc:0.6725\n",
      "epoch 54, batch 2000, loss:1.0121, acc:0.6727\n",
      "epoch 54, batch 2500, loss:1.0154, acc:0.6722\n",
      "epoch 54, batch 3000, loss:1.0154, acc:0.6722\n",
      "epoch 54, batch 3500, loss:1.0170, acc:0.6719\n",
      "epoch 54, save model at ./checkpoint/train\\ckpt-27\n",
      "epoch 54, loss:1.0170, acc:0.6720\n",
      "time in 1 epoch:352.35829162597656 secs\n",
      "\n",
      "epoch 55, batch 0, loss:0.9660, acc:0.6827\n",
      "epoch 55, batch 500, loss:1.0082, acc:0.6675\n",
      "epoch 55, batch 1000, loss:1.0080, acc:0.6721\n",
      "epoch 55, batch 1500, loss:1.0096, acc:0.6727\n",
      "epoch 55, batch 2000, loss:1.0116, acc:0.6729\n",
      "epoch 55, batch 2500, loss:1.0149, acc:0.6723\n",
      "epoch 55, batch 3000, loss:1.0149, acc:0.6723\n",
      "epoch 55, batch 3500, loss:1.0165, acc:0.6720\n",
      "epoch 55, loss:1.0164, acc:0.6721\n",
      "time in 1 epoch:353.41247177124023 secs\n",
      "\n",
      "epoch 56, batch 0, loss:0.9674, acc:0.6791\n",
      "epoch 56, batch 500, loss:1.0072, acc:0.6677\n",
      "epoch 56, batch 1000, loss:1.0069, acc:0.6726\n",
      "epoch 56, batch 1500, loss:1.0086, acc:0.6730\n",
      "epoch 56, batch 2000, loss:1.0108, acc:0.6732\n",
      "epoch 56, batch 2500, loss:1.0141, acc:0.6726\n",
      "epoch 56, batch 3000, loss:1.0138, acc:0.6726\n",
      "epoch 56, batch 3500, loss:1.0155, acc:0.6723\n",
      "epoch 56, save model at ./checkpoint/train\\ckpt-28\n",
      "epoch 56, loss:1.0155, acc:0.6724\n",
      "time in 1 epoch:354.3569440841675 secs\n",
      "\n",
      "epoch 57, batch 0, loss:0.9623, acc:0.6799\n",
      "epoch 57, batch 500, loss:1.0062, acc:0.6680\n",
      "epoch 57, batch 1000, loss:1.0061, acc:0.6728\n",
      "epoch 57, batch 1500, loss:1.0078, acc:0.6733\n",
      "epoch 57, batch 2000, loss:1.0099, acc:0.6735\n",
      "epoch 57, batch 2500, loss:1.0133, acc:0.6728\n",
      "epoch 57, batch 3000, loss:1.0131, acc:0.6729\n",
      "epoch 57, batch 3500, loss:1.0148, acc:0.6726\n",
      "epoch 57, loss:1.0148, acc:0.6727\n",
      "time in 1 epoch:351.0707643032074 secs\n",
      "\n",
      "epoch 58, batch 0, loss:0.9753, acc:0.6755\n",
      "epoch 58, batch 500, loss:1.0058, acc:0.6681\n",
      "epoch 58, batch 1000, loss:1.0055, acc:0.6729\n",
      "epoch 58, batch 1500, loss:1.0070, acc:0.6736\n",
      "epoch 58, batch 2000, loss:1.0091, acc:0.6737\n",
      "epoch 58, batch 2500, loss:1.0125, acc:0.6731\n",
      "epoch 58, batch 3000, loss:1.0124, acc:0.6732\n",
      "epoch 58, batch 3500, loss:1.0141, acc:0.6728\n",
      "epoch 58, save model at ./checkpoint/train\\ckpt-29\n",
      "epoch 58, loss:1.0141, acc:0.6729\n",
      "time in 1 epoch:353.9669601917267 secs\n",
      "\n",
      "epoch 59, batch 0, loss:0.9828, acc:0.6727\n",
      "epoch 59, batch 500, loss:1.0050, acc:0.6684\n",
      "epoch 59, batch 1000, loss:1.0047, acc:0.6731\n",
      "epoch 59, batch 1500, loss:1.0064, acc:0.6737\n",
      "epoch 59, batch 2000, loss:1.0084, acc:0.6738\n",
      "epoch 59, batch 2500, loss:1.0117, acc:0.6733\n",
      "epoch 59, batch 3000, loss:1.0116, acc:0.6733\n",
      "epoch 59, batch 3500, loss:1.0133, acc:0.6730\n",
      "epoch 59, loss:1.0133, acc:0.6731\n",
      "time in 1 epoch:352.09502625465393 secs\n",
      "\n",
      "epoch 60, batch 0, loss:0.9586, acc:0.6779\n",
      "epoch 60, batch 500, loss:1.0037, acc:0.6688\n",
      "epoch 60, batch 1000, loss:1.0038, acc:0.6736\n",
      "epoch 60, batch 1500, loss:1.0057, acc:0.6741\n",
      "epoch 60, batch 2000, loss:1.0077, acc:0.6743\n",
      "epoch 60, batch 2500, loss:1.0111, acc:0.6737\n",
      "epoch 60, batch 3000, loss:1.0110, acc:0.6736\n",
      "epoch 60, batch 3500, loss:1.0126, acc:0.6733\n",
      "epoch 60, save model at ./checkpoint/train\\ckpt-30\n",
      "epoch 60, loss:1.0126, acc:0.6734\n",
      "time in 1 epoch:353.5041961669922 secs\n",
      "\n",
      "epoch 61, batch 0, loss:0.9470, acc:0.6799\n",
      "epoch 61, batch 500, loss:1.0035, acc:0.6690\n",
      "epoch 61, batch 1000, loss:1.0032, acc:0.6738\n",
      "epoch 61, batch 1500, loss:1.0050, acc:0.6741\n",
      "epoch 61, batch 2000, loss:1.0071, acc:0.6744\n",
      "epoch 61, batch 2500, loss:1.0105, acc:0.6738\n",
      "epoch 61, batch 3000, loss:1.0104, acc:0.6738\n",
      "epoch 61, batch 3500, loss:1.0121, acc:0.6734\n",
      "epoch 61, loss:1.0120, acc:0.6735\n",
      "time in 1 epoch:352.85396552085876 secs\n",
      "\n",
      "epoch 62, batch 0, loss:0.9668, acc:0.6715\n",
      "epoch 62, batch 500, loss:1.0026, acc:0.6689\n",
      "epoch 62, batch 1000, loss:1.0024, acc:0.6737\n",
      "epoch 62, batch 1500, loss:1.0042, acc:0.6742\n",
      "epoch 62, batch 2000, loss:1.0063, acc:0.6745\n",
      "epoch 62, batch 2500, loss:1.0096, acc:0.6739\n",
      "epoch 62, batch 3000, loss:1.0095, acc:0.6739\n",
      "epoch 62, batch 3500, loss:1.0112, acc:0.6736\n",
      "epoch 62, save model at ./checkpoint/train\\ckpt-31\n",
      "epoch 62, loss:1.0112, acc:0.6737\n",
      "time in 1 epoch:354.45867228507996 secs\n",
      "\n",
      "epoch 63, batch 0, loss:0.9634, acc:0.6723\n",
      "epoch 63, batch 500, loss:1.0020, acc:0.6693\n",
      "epoch 63, batch 1000, loss:1.0020, acc:0.6739\n",
      "epoch 63, batch 1500, loss:1.0038, acc:0.6745\n",
      "epoch 63, batch 2000, loss:1.0057, acc:0.6748\n",
      "epoch 63, batch 2500, loss:1.0092, acc:0.6742\n",
      "epoch 63, batch 3000, loss:1.0091, acc:0.6742\n",
      "epoch 63, batch 3500, loss:1.0108, acc:0.6738\n",
      "epoch 63, loss:1.0108, acc:0.6739\n",
      "time in 1 epoch:353.6318848133087 secs\n",
      "\n",
      "epoch 64, batch 0, loss:0.9652, acc:0.6727\n",
      "epoch 64, batch 500, loss:1.0013, acc:0.6697\n",
      "epoch 64, batch 1000, loss:1.0008, acc:0.6744\n",
      "epoch 64, batch 1500, loss:1.0029, acc:0.6747\n",
      "epoch 64, batch 2000, loss:1.0050, acc:0.6749\n",
      "epoch 64, batch 2500, loss:1.0082, acc:0.6743\n",
      "epoch 64, batch 3000, loss:1.0082, acc:0.6743\n",
      "epoch 64, batch 3500, loss:1.0098, acc:0.6740\n",
      "epoch 64, save model at ./checkpoint/train\\ckpt-32\n",
      "epoch 64, loss:1.0098, acc:0.6740\n",
      "time in 1 epoch:352.5677309036255 secs\n",
      "\n",
      "epoch 65, batch 0, loss:0.9686, acc:0.6699\n",
      "epoch 65, batch 500, loss:1.0011, acc:0.6696\n",
      "epoch 65, batch 1000, loss:1.0005, acc:0.6744\n",
      "epoch 65, batch 1500, loss:1.0023, acc:0.6749\n",
      "epoch 65, batch 2000, loss:1.0043, acc:0.6751\n",
      "epoch 65, batch 2500, loss:1.0076, acc:0.6745\n",
      "epoch 65, batch 3000, loss:1.0075, acc:0.6745\n",
      "epoch 65, batch 3500, loss:1.0092, acc:0.6742\n",
      "epoch 65, loss:1.0091, acc:0.6743\n",
      "time in 1 epoch:346.43317317962646 secs\n",
      "\n",
      "epoch 66, batch 0, loss:0.9528, acc:0.6835\n",
      "epoch 66, batch 500, loss:1.0000, acc:0.6699\n",
      "epoch 66, batch 1000, loss:0.9996, acc:0.6748\n",
      "epoch 66, batch 1500, loss:1.0016, acc:0.6752\n",
      "epoch 66, batch 2000, loss:1.0036, acc:0.6755\n",
      "epoch 66, batch 2500, loss:1.0071, acc:0.6748\n",
      "epoch 66, batch 3000, loss:1.0069, acc:0.6748\n",
      "epoch 66, batch 3500, loss:1.0087, acc:0.6744\n",
      "epoch 66, save model at ./checkpoint/train\\ckpt-33\n",
      "epoch 66, loss:1.0086, acc:0.6745\n",
      "time in 1 epoch:347.79347562789917 secs\n",
      "\n",
      "epoch 67, batch 0, loss:0.9680, acc:0.6759\n",
      "epoch 67, batch 500, loss:0.9996, acc:0.6699\n",
      "epoch 67, batch 1000, loss:0.9993, acc:0.6746\n",
      "epoch 67, batch 1500, loss:1.0012, acc:0.6752\n",
      "epoch 67, batch 2000, loss:1.0033, acc:0.6754\n",
      "epoch 67, batch 2500, loss:1.0066, acc:0.6749\n",
      "epoch 67, batch 3000, loss:1.0065, acc:0.6749\n",
      "epoch 67, batch 3500, loss:1.0082, acc:0.6745\n",
      "epoch 67, loss:1.0081, acc:0.6746\n",
      "time in 1 epoch:346.8619966506958 secs\n",
      "\n",
      "epoch 68, batch 0, loss:0.9571, acc:0.6843\n",
      "epoch 68, batch 500, loss:0.9995, acc:0.6702\n",
      "epoch 68, batch 1000, loss:0.9989, acc:0.6749\n",
      "epoch 68, batch 1500, loss:1.0007, acc:0.6754\n",
      "epoch 68, batch 2000, loss:1.0026, acc:0.6758\n",
      "epoch 68, batch 2500, loss:1.0060, acc:0.6751\n",
      "epoch 68, batch 3000, loss:1.0058, acc:0.6751\n",
      "epoch 68, batch 3500, loss:1.0075, acc:0.6748\n",
      "epoch 68, save model at ./checkpoint/train\\ckpt-34\n",
      "epoch 68, loss:1.0075, acc:0.6749\n",
      "time in 1 epoch:347.61797428131104 secs\n",
      "\n",
      "epoch 69, batch 0, loss:0.9621, acc:0.6767\n",
      "epoch 69, batch 500, loss:0.9991, acc:0.6703\n",
      "epoch 69, batch 1000, loss:0.9983, acc:0.6751\n",
      "epoch 69, batch 1500, loss:1.0001, acc:0.6755\n",
      "epoch 69, batch 2000, loss:1.0021, acc:0.6756\n",
      "epoch 69, batch 2500, loss:1.0054, acc:0.6751\n",
      "epoch 69, batch 3000, loss:1.0053, acc:0.6751\n",
      "epoch 69, batch 3500, loss:1.0070, acc:0.6748\n",
      "epoch 69, loss:1.0070, acc:0.6749\n",
      "time in 1 epoch:352.8270378112793 secs\n",
      "\n",
      "epoch 70, batch 0, loss:0.9502, acc:0.6771\n",
      "epoch 70, batch 500, loss:0.9981, acc:0.6705\n",
      "epoch 70, batch 1000, loss:0.9976, acc:0.6753\n",
      "epoch 70, batch 1500, loss:0.9993, acc:0.6758\n",
      "epoch 70, batch 2000, loss:1.0013, acc:0.6761\n",
      "epoch 70, batch 2500, loss:1.0046, acc:0.6755\n",
      "epoch 70, batch 3000, loss:1.0046, acc:0.6754\n",
      "epoch 70, batch 3500, loss:1.0062, acc:0.6751\n",
      "epoch 70, save model at ./checkpoint/train\\ckpt-35\n",
      "epoch 70, loss:1.0062, acc:0.6752\n",
      "time in 1 epoch:354.321040391922 secs\n",
      "\n",
      "epoch 71, batch 0, loss:0.9570, acc:0.6851\n",
      "epoch 71, batch 500, loss:0.9972, acc:0.6708\n",
      "epoch 71, batch 1000, loss:0.9967, acc:0.6756\n",
      "epoch 71, batch 1500, loss:0.9987, acc:0.6760\n",
      "epoch 71, batch 2000, loss:1.0007, acc:0.6763\n",
      "epoch 71, batch 2500, loss:1.0042, acc:0.6756\n",
      "epoch 71, batch 3000, loss:1.0041, acc:0.6756\n",
      "epoch 71, batch 3500, loss:1.0058, acc:0.6753\n",
      "epoch 71, loss:1.0058, acc:0.6753\n",
      "time in 1 epoch:344.28289699554443 secs\n",
      "\n",
      "epoch 72, batch 0, loss:0.9556, acc:0.6767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 72, batch 500, loss:0.9968, acc:0.6707\n",
      "epoch 72, batch 1000, loss:0.9965, acc:0.6754\n",
      "epoch 72, batch 1500, loss:0.9981, acc:0.6760\n",
      "epoch 72, batch 2000, loss:1.0003, acc:0.6763\n",
      "epoch 72, batch 2500, loss:1.0036, acc:0.6756\n",
      "epoch 72, batch 3000, loss:1.0036, acc:0.6756\n",
      "epoch 72, batch 3500, loss:1.0053, acc:0.6753\n",
      "epoch 72, save model at ./checkpoint/train\\ckpt-36\n",
      "epoch 72, loss:1.0053, acc:0.6754\n",
      "time in 1 epoch:345.6691882610321 secs\n",
      "\n",
      "epoch 73, batch 0, loss:0.9556, acc:0.6771\n",
      "epoch 73, batch 500, loss:0.9965, acc:0.6710\n",
      "epoch 73, batch 1000, loss:0.9962, acc:0.6757\n",
      "epoch 73, batch 1500, loss:0.9978, acc:0.6763\n",
      "epoch 73, batch 2000, loss:0.9998, acc:0.6765\n",
      "epoch 73, batch 2500, loss:1.0030, acc:0.6759\n",
      "epoch 73, batch 3000, loss:1.0030, acc:0.6759\n",
      "epoch 73, batch 3500, loss:1.0047, acc:0.6755\n",
      "epoch 73, loss:1.0047, acc:0.6756\n",
      "time in 1 epoch:345.81579542160034 secs\n",
      "\n",
      "epoch 74, batch 0, loss:0.9538, acc:0.6727\n",
      "epoch 74, batch 500, loss:0.9957, acc:0.6711\n",
      "epoch 74, batch 1000, loss:0.9955, acc:0.6759\n",
      "epoch 74, batch 1500, loss:0.9971, acc:0.6764\n",
      "epoch 74, batch 2000, loss:0.9991, acc:0.6766\n",
      "epoch 74, batch 2500, loss:1.0025, acc:0.6760\n",
      "epoch 74, batch 3000, loss:1.0024, acc:0.6760\n",
      "epoch 74, batch 3500, loss:1.0040, acc:0.6757\n",
      "epoch 74, save model at ./checkpoint/train\\ckpt-37\n",
      "epoch 74, loss:1.0040, acc:0.6758\n",
      "time in 1 epoch:347.75760078430176 secs\n",
      "\n",
      "epoch 75, batch 0, loss:0.9447, acc:0.6803\n",
      "epoch 75, batch 500, loss:0.9954, acc:0.6714\n",
      "epoch 75, batch 1000, loss:0.9948, acc:0.6763\n",
      "epoch 75, batch 1500, loss:0.9966, acc:0.6767\n",
      "epoch 75, batch 2000, loss:0.9986, acc:0.6769\n",
      "epoch 75, batch 2500, loss:1.0020, acc:0.6763\n",
      "epoch 75, batch 3000, loss:1.0019, acc:0.6763\n",
      "epoch 75, batch 3500, loss:1.0037, acc:0.6759\n",
      "epoch 75, loss:1.0036, acc:0.6760\n",
      "time in 1 epoch:347.6389458179474 secs\n",
      "\n",
      "epoch 76, batch 0, loss:0.9665, acc:0.6863\n",
      "epoch 76, batch 500, loss:0.9950, acc:0.6712\n",
      "epoch 76, batch 1000, loss:0.9947, acc:0.6761\n",
      "epoch 76, batch 1500, loss:0.9962, acc:0.6766\n",
      "epoch 76, batch 2000, loss:0.9982, acc:0.6768\n",
      "epoch 76, batch 2500, loss:1.0017, acc:0.6762\n",
      "epoch 76, batch 3000, loss:1.0016, acc:0.6762\n",
      "epoch 76, batch 3500, loss:1.0032, acc:0.6759\n",
      "epoch 76, save model at ./checkpoint/train\\ckpt-38\n",
      "epoch 76, loss:1.0032, acc:0.6760\n",
      "time in 1 epoch:346.22068452835083 secs\n",
      "\n",
      "epoch 77, batch 0, loss:0.9482, acc:0.6803\n",
      "epoch 77, batch 500, loss:0.9941, acc:0.6717\n",
      "epoch 77, batch 1000, loss:0.9941, acc:0.6761\n",
      "epoch 77, batch 1500, loss:0.9958, acc:0.6767\n",
      "epoch 77, batch 2000, loss:0.9977, acc:0.6770\n",
      "epoch 77, batch 2500, loss:1.0010, acc:0.6764\n",
      "epoch 77, batch 3000, loss:1.0010, acc:0.6764\n",
      "epoch 77, batch 3500, loss:1.0027, acc:0.6761\n",
      "epoch 77, loss:1.0026, acc:0.6762\n",
      "time in 1 epoch:348.1146728992462 secs\n",
      "\n",
      "epoch 78, batch 0, loss:0.9607, acc:0.6771\n",
      "epoch 78, batch 500, loss:0.9938, acc:0.6716\n",
      "epoch 78, batch 1000, loss:0.9935, acc:0.6764\n",
      "epoch 78, batch 1500, loss:0.9953, acc:0.6768\n",
      "epoch 78, batch 2000, loss:0.9971, acc:0.6771\n",
      "epoch 78, batch 2500, loss:1.0005, acc:0.6764\n",
      "epoch 78, batch 3000, loss:1.0005, acc:0.6764\n",
      "epoch 78, batch 3500, loss:1.0022, acc:0.6761\n",
      "epoch 78, save model at ./checkpoint/train\\ckpt-39\n",
      "epoch 78, loss:1.0022, acc:0.6762\n",
      "time in 1 epoch:347.0484700202942 secs\n",
      "\n",
      "epoch 79, batch 0, loss:0.9640, acc:0.6795\n",
      "epoch 79, batch 500, loss:0.9935, acc:0.6717\n",
      "epoch 79, batch 1000, loss:0.9930, acc:0.6765\n",
      "epoch 79, batch 1500, loss:0.9946, acc:0.6771\n",
      "epoch 79, batch 2000, loss:0.9967, acc:0.6773\n",
      "epoch 79, batch 2500, loss:1.0000, acc:0.6767\n",
      "epoch 79, batch 3000, loss:0.9999, acc:0.6767\n",
      "epoch 79, batch 3500, loss:1.0016, acc:0.6763\n",
      "epoch 79, loss:1.0016, acc:0.6764\n",
      "time in 1 epoch:351.82674407958984 secs\n",
      "\n",
      "epoch 80, batch 0, loss:0.9509, acc:0.6799\n",
      "epoch 80, batch 500, loss:0.9930, acc:0.6718\n",
      "epoch 80, batch 1000, loss:0.9925, acc:0.6766\n",
      "epoch 80, batch 1500, loss:0.9942, acc:0.6772\n",
      "epoch 80, batch 2000, loss:0.9961, acc:0.6773\n",
      "epoch 80, batch 2500, loss:0.9995, acc:0.6767\n",
      "epoch 80, batch 3000, loss:0.9995, acc:0.6767\n",
      "epoch 80, batch 3500, loss:1.0012, acc:0.6764\n",
      "epoch 80, save model at ./checkpoint/train\\ckpt-40\n",
      "epoch 80, loss:1.0011, acc:0.6765\n",
      "time in 1 epoch:353.9410274028778 secs\n",
      "\n",
      "epoch 81, batch 0, loss:0.9629, acc:0.6763\n",
      "epoch 81, batch 500, loss:0.9924, acc:0.6721\n",
      "epoch 81, batch 1000, loss:0.9919, acc:0.6770\n",
      "epoch 81, batch 1500, loss:0.9938, acc:0.6775\n",
      "epoch 81, batch 2000, loss:0.9958, acc:0.6777\n",
      "epoch 81, batch 2500, loss:0.9992, acc:0.6771\n",
      "epoch 81, batch 3000, loss:0.9991, acc:0.6771\n",
      "epoch 81, batch 3500, loss:1.0008, acc:0.6767\n",
      "epoch 81, loss:1.0007, acc:0.6768\n",
      "time in 1 epoch:352.99957609176636 secs\n",
      "\n",
      "epoch 82, batch 0, loss:0.9561, acc:0.6807\n",
      "epoch 82, batch 500, loss:0.9916, acc:0.6724\n",
      "epoch 82, batch 1000, loss:0.9913, acc:0.6772\n",
      "epoch 82, batch 1500, loss:0.9932, acc:0.6776\n",
      "epoch 82, batch 2000, loss:0.9954, acc:0.6777\n",
      "epoch 82, batch 2500, loss:0.9987, acc:0.6771\n",
      "epoch 82, batch 3000, loss:0.9986, acc:0.6771\n",
      "epoch 82, batch 3500, loss:1.0002, acc:0.6768\n",
      "epoch 82, save model at ./checkpoint/train\\ckpt-41\n",
      "epoch 82, loss:1.0002, acc:0.6769\n",
      "time in 1 epoch:353.842321395874 secs\n",
      "\n",
      "epoch 83, batch 0, loss:0.9565, acc:0.6779\n",
      "epoch 83, batch 500, loss:0.9915, acc:0.6723\n",
      "epoch 83, batch 1000, loss:0.9910, acc:0.6772\n",
      "epoch 83, batch 1500, loss:0.9927, acc:0.6778\n",
      "epoch 83, batch 2000, loss:0.9947, acc:0.6780\n",
      "epoch 83, batch 2500, loss:0.9981, acc:0.6773\n",
      "epoch 83, batch 3000, loss:0.9980, acc:0.6773\n",
      "epoch 83, batch 3500, loss:0.9997, acc:0.6770\n",
      "epoch 83, loss:0.9997, acc:0.6771\n",
      "time in 1 epoch:346.1329472064972 secs\n",
      "\n",
      "epoch 84, batch 0, loss:0.9576, acc:0.6811\n",
      "epoch 84, batch 500, loss:0.9912, acc:0.6724\n",
      "epoch 84, batch 1000, loss:0.9907, acc:0.6772\n",
      "epoch 84, batch 1500, loss:0.9926, acc:0.6776\n",
      "epoch 84, batch 2000, loss:0.9947, acc:0.6779\n",
      "epoch 84, batch 2500, loss:0.9979, acc:0.6773\n",
      "epoch 84, batch 3000, loss:0.9979, acc:0.6773\n",
      "epoch 84, batch 3500, loss:0.9995, acc:0.6770\n",
      "epoch 84, save model at ./checkpoint/train\\ckpt-42\n",
      "epoch 84, loss:0.9995, acc:0.6771\n",
      "time in 1 epoch:347.7655792236328 secs\n",
      "\n",
      "epoch 85, batch 0, loss:0.9559, acc:0.6839\n",
      "epoch 85, batch 500, loss:0.9911, acc:0.6723\n",
      "epoch 85, batch 1000, loss:0.9906, acc:0.6771\n",
      "epoch 85, batch 1500, loss:0.9922, acc:0.6777\n",
      "epoch 85, batch 2000, loss:0.9943, acc:0.6779\n",
      "epoch 85, batch 2500, loss:0.9976, acc:0.6774\n",
      "epoch 85, batch 3000, loss:0.9974, acc:0.6774\n",
      "epoch 85, batch 3500, loss:0.9991, acc:0.6771\n",
      "epoch 85, loss:0.9990, acc:0.6772\n",
      "time in 1 epoch:345.06483483314514 secs\n",
      "\n",
      "epoch 86, batch 0, loss:0.9654, acc:0.6835\n",
      "epoch 86, batch 500, loss:0.9902, acc:0.6731\n",
      "epoch 86, batch 1000, loss:0.9896, acc:0.6777\n",
      "epoch 86, batch 1500, loss:0.9914, acc:0.6781\n",
      "epoch 86, batch 2000, loss:0.9935, acc:0.6783\n",
      "epoch 86, batch 2500, loss:0.9968, acc:0.6777\n",
      "epoch 86, batch 3000, loss:0.9967, acc:0.6777\n",
      "epoch 86, batch 3500, loss:0.9985, acc:0.6773\n",
      "epoch 86, save model at ./checkpoint/train\\ckpt-43\n",
      "epoch 86, loss:0.9985, acc:0.6774\n",
      "time in 1 epoch:353.7864410877228 secs\n",
      "\n",
      "epoch 87, batch 0, loss:0.9423, acc:0.6783\n",
      "epoch 87, batch 500, loss:0.9898, acc:0.6728\n",
      "epoch 87, batch 1000, loss:0.9893, acc:0.6777\n",
      "epoch 87, batch 1500, loss:0.9908, acc:0.6782\n",
      "epoch 87, batch 2000, loss:0.9929, acc:0.6784\n",
      "epoch 87, batch 2500, loss:0.9963, acc:0.6777\n",
      "epoch 87, batch 3000, loss:0.9964, acc:0.6777\n",
      "epoch 87, batch 3500, loss:0.9981, acc:0.6773\n",
      "epoch 87, loss:0.9980, acc:0.6774\n",
      "time in 1 epoch:353.0953197479248 secs\n",
      "\n",
      "epoch 88, batch 0, loss:0.9640, acc:0.6815\n",
      "epoch 88, batch 500, loss:0.9891, acc:0.6728\n",
      "epoch 88, batch 1000, loss:0.9887, acc:0.6776\n",
      "epoch 88, batch 1500, loss:0.9905, acc:0.6781\n",
      "epoch 88, batch 2000, loss:0.9925, acc:0.6783\n",
      "epoch 88, batch 2500, loss:0.9959, acc:0.6778\n",
      "epoch 88, batch 3000, loss:0.9959, acc:0.6777\n",
      "epoch 88, batch 3500, loss:0.9976, acc:0.6774\n",
      "epoch 88, save model at ./checkpoint/train\\ckpt-44\n",
      "epoch 88, loss:0.9975, acc:0.6775\n",
      "time in 1 epoch:352.91480326652527 secs\n",
      "\n",
      "epoch 89, batch 0, loss:0.9468, acc:0.6923\n",
      "epoch 89, batch 500, loss:0.9893, acc:0.6729\n",
      "epoch 89, batch 1000, loss:0.9885, acc:0.6779\n",
      "epoch 89, batch 1500, loss:0.9902, acc:0.6783\n",
      "epoch 89, batch 2000, loss:0.9924, acc:0.6785\n",
      "epoch 89, batch 2500, loss:0.9957, acc:0.6779\n",
      "epoch 89, batch 3000, loss:0.9956, acc:0.6779\n",
      "epoch 89, batch 3500, loss:0.9972, acc:0.6776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 89, loss:0.9972, acc:0.6777\n",
      "time in 1 epoch:343.69150376319885 secs\n",
      "\n",
      "epoch 90, batch 0, loss:0.9552, acc:0.6783\n",
      "epoch 90, batch 500, loss:0.9889, acc:0.6729\n",
      "epoch 90, batch 1000, loss:0.9884, acc:0.6776\n",
      "epoch 90, batch 1500, loss:0.9898, acc:0.6783\n",
      "epoch 90, batch 2000, loss:0.9917, acc:0.6786\n",
      "epoch 90, batch 2500, loss:0.9952, acc:0.6780\n",
      "epoch 90, batch 3000, loss:0.9951, acc:0.6780\n",
      "epoch 90, batch 3500, loss:0.9967, acc:0.6777\n",
      "epoch 90, save model at ./checkpoint/train\\ckpt-45\n",
      "epoch 90, loss:0.9967, acc:0.6778\n",
      "time in 1 epoch:344.70873284339905 secs\n",
      "\n",
      "epoch 91, batch 0, loss:0.9552, acc:0.6839\n",
      "epoch 91, batch 500, loss:0.9885, acc:0.6729\n",
      "epoch 91, batch 1000, loss:0.9878, acc:0.6780\n",
      "epoch 91, batch 1500, loss:0.9895, acc:0.6786\n",
      "epoch 91, batch 2000, loss:0.9914, acc:0.6789\n",
      "epoch 91, batch 2500, loss:0.9948, acc:0.6783\n",
      "epoch 91, batch 3000, loss:0.9946, acc:0.6782\n",
      "epoch 91, batch 3500, loss:0.9964, acc:0.6779\n",
      "epoch 91, loss:0.9963, acc:0.6780\n",
      "time in 1 epoch:345.81878757476807 secs\n",
      "\n",
      "epoch 92, batch 0, loss:0.9539, acc:0.6891\n",
      "epoch 92, batch 500, loss:0.9879, acc:0.6734\n",
      "epoch 92, batch 1000, loss:0.9875, acc:0.6783\n",
      "epoch 92, batch 1500, loss:0.9893, acc:0.6787\n",
      "epoch 92, batch 2000, loss:0.9912, acc:0.6790\n",
      "epoch 92, batch 2500, loss:0.9946, acc:0.6783\n",
      "epoch 92, batch 3000, loss:0.9945, acc:0.6782\n",
      "epoch 92, batch 3500, loss:0.9961, acc:0.6779\n",
      "epoch 92, save model at ./checkpoint/train\\ckpt-46\n",
      "epoch 92, loss:0.9961, acc:0.6780\n",
      "time in 1 epoch:346.9401578903198 secs\n",
      "\n",
      "epoch 93, batch 0, loss:0.9482, acc:0.6835\n",
      "epoch 93, batch 500, loss:0.9876, acc:0.6737\n",
      "epoch 93, batch 1000, loss:0.9871, acc:0.6784\n",
      "epoch 93, batch 1500, loss:0.9887, acc:0.6789\n",
      "epoch 93, batch 2000, loss:0.9908, acc:0.6790\n",
      "epoch 93, batch 2500, loss:0.9942, acc:0.6784\n",
      "epoch 93, batch 3000, loss:0.9941, acc:0.6784\n",
      "epoch 93, batch 3500, loss:0.9958, acc:0.6781\n",
      "epoch 93, loss:0.9957, acc:0.6781\n",
      "time in 1 epoch:346.2127342224121 secs\n",
      "\n",
      "epoch 94, batch 0, loss:0.9454, acc:0.6839\n",
      "epoch 94, batch 500, loss:0.9872, acc:0.6738\n",
      "epoch 94, batch 1000, loss:0.9867, acc:0.6785\n",
      "epoch 94, batch 1500, loss:0.9884, acc:0.6789\n",
      "epoch 94, batch 2000, loss:0.9905, acc:0.6791\n",
      "epoch 94, batch 2500, loss:0.9938, acc:0.6785\n",
      "epoch 94, batch 3000, loss:0.9936, acc:0.6784\n",
      "epoch 94, batch 3500, loss:0.9953, acc:0.6781\n",
      "epoch 94, save model at ./checkpoint/train\\ckpt-47\n",
      "epoch 94, loss:0.9952, acc:0.6782\n",
      "time in 1 epoch:347.5790777206421 secs\n",
      "\n",
      "epoch 95, batch 0, loss:0.9473, acc:0.6851\n",
      "epoch 95, batch 500, loss:0.9867, acc:0.6738\n",
      "epoch 95, batch 1000, loss:0.9862, acc:0.6786\n",
      "epoch 95, batch 1500, loss:0.9881, acc:0.6790\n",
      "epoch 95, batch 2000, loss:0.9900, acc:0.6792\n",
      "epoch 95, batch 2500, loss:0.9934, acc:0.6786\n",
      "epoch 95, batch 3000, loss:0.9934, acc:0.6786\n",
      "epoch 95, batch 3500, loss:0.9950, acc:0.6782\n",
      "epoch 95, loss:0.9950, acc:0.6783\n",
      "time in 1 epoch:346.87396478652954 secs\n",
      "\n",
      "epoch 96, batch 0, loss:0.9583, acc:0.6839\n",
      "epoch 96, batch 500, loss:0.9863, acc:0.6734\n",
      "epoch 96, batch 1000, loss:0.9859, acc:0.6783\n",
      "epoch 96, batch 1500, loss:0.9878, acc:0.6789\n",
      "epoch 96, batch 2000, loss:0.9899, acc:0.6791\n",
      "epoch 96, batch 2500, loss:0.9931, acc:0.6786\n",
      "epoch 96, batch 3000, loss:0.9930, acc:0.6786\n",
      "epoch 96, batch 3500, loss:0.9946, acc:0.6784\n",
      "epoch 96, save model at ./checkpoint/train\\ckpt-48\n",
      "epoch 96, loss:0.9946, acc:0.6784\n",
      "time in 1 epoch:346.9128608703613 secs\n",
      "\n",
      "epoch 97, batch 0, loss:0.9387, acc:0.6875\n",
      "epoch 97, batch 500, loss:0.9855, acc:0.6743\n",
      "epoch 97, batch 1000, loss:0.9851, acc:0.6790\n",
      "epoch 97, batch 1500, loss:0.9870, acc:0.6794\n",
      "epoch 97, batch 2000, loss:0.9890, acc:0.6795\n",
      "epoch 97, batch 2500, loss:0.9926, acc:0.6789\n",
      "epoch 97, batch 3000, loss:0.9925, acc:0.6788\n",
      "epoch 97, batch 3500, loss:0.9941, acc:0.6785\n",
      "epoch 97, loss:0.9941, acc:0.6786\n",
      "time in 1 epoch:347.49929213523865 secs\n",
      "\n",
      "epoch 98, batch 0, loss:0.9498, acc:0.6831\n",
      "epoch 98, batch 500, loss:0.9854, acc:0.6741\n",
      "epoch 98, batch 1000, loss:0.9851, acc:0.6787\n",
      "epoch 98, batch 1500, loss:0.9868, acc:0.6793\n",
      "epoch 98, batch 2000, loss:0.9888, acc:0.6796\n",
      "epoch 98, batch 2500, loss:0.9922, acc:0.6790\n",
      "epoch 98, batch 3000, loss:0.9922, acc:0.6789\n",
      "epoch 98, batch 3500, loss:0.9939, acc:0.6786\n",
      "epoch 98, save model at ./checkpoint/train\\ckpt-49\n",
      "epoch 98, loss:0.9939, acc:0.6787\n",
      "time in 1 epoch:346.2167224884033 secs\n",
      "\n",
      "epoch 99, batch 0, loss:0.9510, acc:0.6819\n",
      "epoch 99, batch 500, loss:0.9851, acc:0.6740\n",
      "epoch 99, batch 1000, loss:0.9851, acc:0.6789\n",
      "epoch 99, batch 1500, loss:0.9867, acc:0.6795\n",
      "epoch 99, batch 2000, loss:0.9886, acc:0.6796\n",
      "epoch 99, batch 2500, loss:0.9920, acc:0.6790\n",
      "epoch 99, batch 3000, loss:0.9919, acc:0.6790\n",
      "epoch 99, batch 3500, loss:0.9935, acc:0.6787\n",
      "epoch 99, loss:0.9934, acc:0.6788\n",
      "time in 1 epoch:346.0092782974243 secs\n",
      "\n",
      "epoch 100, batch 0, loss:0.9401, acc:0.6879\n",
      "epoch 100, batch 500, loss:0.9854, acc:0.6741\n",
      "epoch 100, batch 1000, loss:0.9847, acc:0.6790\n",
      "epoch 100, batch 1500, loss:0.9862, acc:0.6796\n",
      "epoch 100, batch 2000, loss:0.9883, acc:0.6797\n",
      "epoch 100, batch 2500, loss:0.9916, acc:0.6791\n",
      "epoch 100, batch 3000, loss:0.9916, acc:0.6790\n",
      "epoch 100, batch 3500, loss:0.9932, acc:0.6787\n",
      "epoch 100, save model at ./checkpoint/train\\ckpt-50\n",
      "epoch 100, loss:0.9931, acc:0.6788\n",
      "time in 1 epoch:346.6326103210449 secs\n",
      "\n",
      "epoch 101, batch 0, loss:0.9399, acc:0.6767\n",
      "epoch 101, batch 500, loss:0.9846, acc:0.6745\n",
      "epoch 101, batch 1000, loss:0.9844, acc:0.6790\n",
      "epoch 101, batch 1500, loss:0.9861, acc:0.6796\n",
      "epoch 101, batch 2000, loss:0.9879, acc:0.6798\n",
      "epoch 101, batch 2500, loss:0.9913, acc:0.6792\n",
      "epoch 101, batch 3000, loss:0.9912, acc:0.6792\n",
      "epoch 101, batch 3500, loss:0.9928, acc:0.6789\n",
      "epoch 101, loss:0.9927, acc:0.6790\n",
      "time in 1 epoch:346.1020300388336 secs\n",
      "\n",
      "epoch 102, batch 0, loss:0.9493, acc:0.6851\n",
      "epoch 102, batch 500, loss:0.9845, acc:0.6745\n",
      "epoch 102, batch 1000, loss:0.9838, acc:0.6793\n",
      "epoch 102, batch 1500, loss:0.9854, acc:0.6799\n",
      "epoch 102, batch 2000, loss:0.9874, acc:0.6800\n",
      "epoch 102, batch 2500, loss:0.9907, acc:0.6794\n",
      "epoch 102, batch 3000, loss:0.9907, acc:0.6793\n",
      "epoch 102, batch 3500, loss:0.9924, acc:0.6790\n",
      "epoch 102, save model at ./checkpoint/train\\ckpt-51\n",
      "epoch 102, loss:0.9924, acc:0.6791\n",
      "time in 1 epoch:347.91916823387146 secs\n",
      "\n",
      "epoch 103, batch 0, loss:0.9434, acc:0.6867\n",
      "epoch 103, batch 500, loss:0.9843, acc:0.6745\n",
      "epoch 103, batch 1000, loss:0.9835, acc:0.6793\n",
      "epoch 103, batch 1500, loss:0.9851, acc:0.6799\n",
      "epoch 103, batch 2000, loss:0.9872, acc:0.6800\n",
      "epoch 103, batch 2500, loss:0.9906, acc:0.6794\n",
      "epoch 103, batch 3000, loss:0.9905, acc:0.6793\n",
      "epoch 103, batch 3500, loss:0.9922, acc:0.6791\n",
      "epoch 103, loss:0.9921, acc:0.6791\n",
      "time in 1 epoch:346.78619933128357 secs\n",
      "\n",
      "epoch 104, batch 0, loss:0.9449, acc:0.6879\n",
      "epoch 104, batch 500, loss:0.9837, acc:0.6744\n",
      "epoch 104, batch 1000, loss:0.9833, acc:0.6794\n",
      "epoch 104, batch 1500, loss:0.9851, acc:0.6799\n",
      "epoch 104, batch 2000, loss:0.9870, acc:0.6800\n",
      "epoch 104, batch 2500, loss:0.9903, acc:0.6795\n",
      "epoch 104, batch 3000, loss:0.9902, acc:0.6795\n",
      "epoch 104, batch 3500, loss:0.9919, acc:0.6792\n",
      "epoch 104, save model at ./checkpoint/train\\ckpt-52\n",
      "epoch 104, loss:0.9919, acc:0.6793\n",
      "time in 1 epoch:348.15054965019226 secs\n",
      "\n",
      "epoch 105, batch 0, loss:0.9400, acc:0.6799\n",
      "epoch 105, batch 500, loss:0.9833, acc:0.6745\n",
      "epoch 105, batch 1000, loss:0.9827, acc:0.6794\n",
      "epoch 105, batch 1500, loss:0.9843, acc:0.6801\n",
      "epoch 105, batch 2000, loss:0.9865, acc:0.6801\n",
      "epoch 105, batch 2500, loss:0.9898, acc:0.6796\n",
      "epoch 105, batch 3000, loss:0.9898, acc:0.6795\n",
      "epoch 105, batch 3500, loss:0.9915, acc:0.6793\n",
      "epoch 105, loss:0.9914, acc:0.6794\n",
      "time in 1 epoch:347.0076389312744 secs\n",
      "\n",
      "epoch 106, batch 0, loss:0.9411, acc:0.6819\n",
      "epoch 106, batch 500, loss:0.9832, acc:0.6748\n",
      "epoch 106, batch 1000, loss:0.9824, acc:0.6798\n",
      "epoch 106, batch 1500, loss:0.9842, acc:0.6802\n",
      "epoch 106, batch 2000, loss:0.9861, acc:0.6803\n",
      "epoch 106, batch 2500, loss:0.9894, acc:0.6797\n",
      "epoch 106, batch 3000, loss:0.9893, acc:0.6797\n",
      "epoch 106, batch 3500, loss:0.9911, acc:0.6794\n",
      "epoch 106, save model at ./checkpoint/train\\ckpt-53\n",
      "epoch 106, loss:0.9911, acc:0.6794\n",
      "time in 1 epoch:347.046471118927 secs\n",
      "\n",
      "epoch 107, batch 0, loss:0.9353, acc:0.6827\n",
      "epoch 107, batch 500, loss:0.9825, acc:0.6748\n",
      "epoch 107, batch 1000, loss:0.9822, acc:0.6795\n",
      "epoch 107, batch 1500, loss:0.9836, acc:0.6801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 107, batch 2000, loss:0.9857, acc:0.6804\n",
      "epoch 107, batch 2500, loss:0.9890, acc:0.6798\n",
      "epoch 107, batch 3000, loss:0.9890, acc:0.6798\n",
      "epoch 107, batch 3500, loss:0.9906, acc:0.6795\n",
      "epoch 107, loss:0.9906, acc:0.6796\n",
      "time in 1 epoch:347.2010896205902 secs\n",
      "\n",
      "epoch 108, batch 0, loss:0.9475, acc:0.6855\n",
      "epoch 108, batch 500, loss:0.9828, acc:0.6749\n",
      "epoch 108, batch 1000, loss:0.9823, acc:0.6795\n",
      "epoch 108, batch 1500, loss:0.9838, acc:0.6800\n",
      "epoch 108, batch 2000, loss:0.9857, acc:0.6803\n",
      "epoch 108, batch 2500, loss:0.9889, acc:0.6798\n",
      "epoch 108, batch 3000, loss:0.9889, acc:0.6798\n",
      "epoch 108, batch 3500, loss:0.9906, acc:0.6794\n",
      "epoch 108, save model at ./checkpoint/train\\ckpt-54\n",
      "epoch 108, loss:0.9905, acc:0.6795\n",
      "time in 1 epoch:346.6625306606293 secs\n",
      "\n",
      "epoch 109, batch 0, loss:0.9439, acc:0.6875\n",
      "epoch 109, batch 500, loss:0.9814, acc:0.6752\n",
      "epoch 109, batch 1000, loss:0.9810, acc:0.6800\n",
      "epoch 109, batch 1500, loss:0.9829, acc:0.6805\n",
      "epoch 109, batch 2000, loss:0.9851, acc:0.6806\n",
      "epoch 109, batch 2500, loss:0.9884, acc:0.6800\n",
      "epoch 109, batch 3000, loss:0.9884, acc:0.6800\n",
      "epoch 109, batch 3500, loss:0.9900, acc:0.6796\n",
      "epoch 109, loss:0.9900, acc:0.6797\n",
      "time in 1 epoch:345.67816400527954 secs\n",
      "\n",
      "epoch 110, batch 0, loss:0.9461, acc:0.6747\n",
      "epoch 110, batch 500, loss:0.9817, acc:0.6753\n",
      "epoch 110, batch 1000, loss:0.9809, acc:0.6801\n",
      "epoch 110, batch 1500, loss:0.9829, acc:0.6806\n",
      "epoch 110, batch 2000, loss:0.9848, acc:0.6808\n",
      "epoch 110, batch 2500, loss:0.9882, acc:0.6802\n",
      "epoch 110, batch 3000, loss:0.9881, acc:0.6801\n",
      "epoch 110, batch 3500, loss:0.9897, acc:0.6799\n",
      "epoch 110, save model at ./checkpoint/train\\ckpt-55\n",
      "epoch 110, loss:0.9896, acc:0.6799\n",
      "time in 1 epoch:346.18780040740967 secs\n",
      "\n",
      "epoch 111, batch 0, loss:0.9480, acc:0.6803\n",
      "epoch 111, batch 500, loss:0.9819, acc:0.6752\n",
      "epoch 111, batch 1000, loss:0.9812, acc:0.6801\n",
      "epoch 111, batch 1500, loss:0.9827, acc:0.6807\n",
      "epoch 111, batch 2000, loss:0.9847, acc:0.6809\n",
      "epoch 111, batch 2500, loss:0.9879, acc:0.6803\n",
      "epoch 111, batch 3000, loss:0.9879, acc:0.6803\n",
      "epoch 111, batch 3500, loss:0.9896, acc:0.6799\n",
      "epoch 111, loss:0.9896, acc:0.6800\n",
      "time in 1 epoch:346.8270902633667 secs\n",
      "\n",
      "epoch 112, batch 0, loss:0.9532, acc:0.6815\n",
      "epoch 112, batch 500, loss:0.9810, acc:0.6754\n",
      "epoch 112, batch 1000, loss:0.9806, acc:0.6802\n",
      "epoch 112, batch 1500, loss:0.9823, acc:0.6808\n",
      "epoch 112, batch 2000, loss:0.9844, acc:0.6809\n",
      "epoch 112, batch 2500, loss:0.9877, acc:0.6803\n",
      "epoch 112, batch 3000, loss:0.9876, acc:0.6802\n",
      "epoch 112, batch 3500, loss:0.9893, acc:0.6799\n",
      "epoch 112, save model at ./checkpoint/train\\ckpt-56\n",
      "epoch 112, loss:0.9892, acc:0.6800\n",
      "time in 1 epoch:350.52619338035583 secs\n",
      "\n",
      "epoch 113, batch 0, loss:0.9346, acc:0.6899\n",
      "epoch 113, batch 500, loss:0.9804, acc:0.6754\n",
      "epoch 113, batch 1000, loss:0.9797, acc:0.6806\n",
      "epoch 113, batch 1500, loss:0.9817, acc:0.6810\n",
      "epoch 113, batch 2000, loss:0.9839, acc:0.6811\n",
      "epoch 113, batch 2500, loss:0.9873, acc:0.6805\n",
      "epoch 113, batch 3000, loss:0.9872, acc:0.6804\n",
      "epoch 113, batch 3500, loss:0.9889, acc:0.6801\n",
      "epoch 113, loss:0.9889, acc:0.6802\n",
      "time in 1 epoch:346.02423787117004 secs\n",
      "\n",
      "epoch 114, batch 0, loss:0.9352, acc:0.6875\n",
      "epoch 114, batch 500, loss:0.9806, acc:0.6753\n",
      "epoch 114, batch 1000, loss:0.9804, acc:0.6801\n",
      "epoch 114, batch 1500, loss:0.9819, acc:0.6808\n",
      "epoch 114, batch 2000, loss:0.9840, acc:0.6809\n",
      "epoch 114, batch 2500, loss:0.9872, acc:0.6804\n",
      "epoch 114, batch 3000, loss:0.9872, acc:0.6804\n",
      "epoch 114, batch 3500, loss:0.9889, acc:0.6800\n",
      "epoch 114, save model at ./checkpoint/train\\ckpt-57\n",
      "epoch 114, loss:0.9889, acc:0.6801\n",
      "time in 1 epoch:346.79673433303833 secs\n",
      "\n",
      "epoch 115, batch 0, loss:0.9347, acc:0.6863\n",
      "epoch 115, batch 500, loss:0.9802, acc:0.6758\n",
      "epoch 115, batch 1000, loss:0.9796, acc:0.6806\n",
      "epoch 115, batch 1500, loss:0.9814, acc:0.6810\n",
      "epoch 115, batch 2000, loss:0.9834, acc:0.6812\n",
      "epoch 115, batch 2500, loss:0.9867, acc:0.6806\n",
      "epoch 115, batch 3000, loss:0.9866, acc:0.6806\n",
      "epoch 115, batch 3500, loss:0.9883, acc:0.6802\n",
      "epoch 115, loss:0.9882, acc:0.6803\n",
      "time in 1 epoch:346.89889788627625 secs\n",
      "\n",
      "epoch 116, batch 0, loss:0.9486, acc:0.6795\n",
      "epoch 116, batch 500, loss:0.9797, acc:0.6758\n",
      "epoch 116, batch 1000, loss:0.9793, acc:0.6805\n",
      "epoch 116, batch 1500, loss:0.9812, acc:0.6809\n",
      "epoch 116, batch 2000, loss:0.9832, acc:0.6811\n",
      "epoch 116, batch 2500, loss:0.9865, acc:0.6805\n",
      "epoch 116, batch 3000, loss:0.9865, acc:0.6805\n",
      "epoch 116, batch 3500, loss:0.9882, acc:0.6801\n",
      "epoch 116, save model at ./checkpoint/train\\ckpt-58\n",
      "epoch 116, loss:0.9882, acc:0.6802\n",
      "time in 1 epoch:346.8500282764435 secs\n",
      "\n",
      "epoch 117, batch 0, loss:0.9398, acc:0.6855\n",
      "epoch 117, batch 500, loss:0.9792, acc:0.6759\n",
      "epoch 117, batch 1000, loss:0.9790, acc:0.6807\n",
      "epoch 117, batch 1500, loss:0.9808, acc:0.6812\n",
      "epoch 117, batch 2000, loss:0.9828, acc:0.6813\n",
      "epoch 117, batch 2500, loss:0.9862, acc:0.6808\n",
      "epoch 117, batch 3000, loss:0.9862, acc:0.6807\n",
      "epoch 117, batch 3500, loss:0.9878, acc:0.6804\n",
      "epoch 117, loss:0.9878, acc:0.6804\n",
      "time in 1 epoch:345.6282982826233 secs\n",
      "\n",
      "epoch 118, batch 0, loss:0.9359, acc:0.6831\n",
      "epoch 118, batch 500, loss:0.9793, acc:0.6757\n",
      "epoch 118, batch 1000, loss:0.9787, acc:0.6807\n",
      "epoch 118, batch 1500, loss:0.9803, acc:0.6813\n",
      "epoch 118, batch 2000, loss:0.9824, acc:0.6814\n",
      "epoch 118, batch 2500, loss:0.9857, acc:0.6808\n",
      "epoch 118, batch 3000, loss:0.9858, acc:0.6807\n",
      "epoch 118, batch 3500, loss:0.9874, acc:0.6804\n",
      "epoch 118, save model at ./checkpoint/train\\ckpt-59\n",
      "epoch 118, loss:0.9873, acc:0.6805\n",
      "time in 1 epoch:346.426162481308 secs\n",
      "\n",
      "epoch 119, batch 0, loss:0.9436, acc:0.6811\n",
      "epoch 119, batch 500, loss:0.9792, acc:0.6758\n",
      "epoch 119, batch 1000, loss:0.9786, acc:0.6807\n",
      "epoch 119, batch 1500, loss:0.9802, acc:0.6812\n",
      "epoch 119, batch 2000, loss:0.9824, acc:0.6813\n",
      "epoch 119, batch 2500, loss:0.9857, acc:0.6808\n",
      "epoch 119, batch 3000, loss:0.9856, acc:0.6808\n",
      "epoch 119, batch 3500, loss:0.9873, acc:0.6804\n",
      "epoch 119, loss:0.9872, acc:0.6805\n",
      "time in 1 epoch:346.2436511516571 secs\n",
      "\n",
      "epoch 120, batch 0, loss:0.9367, acc:0.6871\n",
      "epoch 120, batch 500, loss:0.9790, acc:0.6761\n",
      "epoch 120, batch 1000, loss:0.9786, acc:0.6810\n",
      "epoch 120, batch 1500, loss:0.9802, acc:0.6814\n",
      "epoch 120, batch 2000, loss:0.9821, acc:0.6816\n",
      "epoch 120, batch 2500, loss:0.9855, acc:0.6809\n",
      "epoch 120, batch 3000, loss:0.9854, acc:0.6809\n",
      "epoch 120, batch 3500, loss:0.9870, acc:0.6806\n",
      "epoch 120, save model at ./checkpoint/train\\ckpt-60\n",
      "epoch 120, loss:0.9869, acc:0.6806\n",
      "time in 1 epoch:346.78619956970215 secs\n",
      "\n",
      "epoch 121, batch 0, loss:0.9393, acc:0.6843\n",
      "epoch 121, batch 500, loss:0.9787, acc:0.6756\n",
      "epoch 121, batch 1000, loss:0.9780, acc:0.6807\n",
      "epoch 121, batch 1500, loss:0.9797, acc:0.6812\n",
      "epoch 121, batch 2000, loss:0.9818, acc:0.6815\n",
      "epoch 121, batch 2500, loss:0.9850, acc:0.6809\n",
      "epoch 121, batch 3000, loss:0.9849, acc:0.6809\n",
      "epoch 121, batch 3500, loss:0.9866, acc:0.6806\n",
      "epoch 121, loss:0.9866, acc:0.6807\n",
      "time in 1 epoch:346.31944847106934 secs\n",
      "\n",
      "epoch 122, batch 0, loss:0.9359, acc:0.6815\n",
      "epoch 122, batch 500, loss:0.9785, acc:0.6760\n",
      "epoch 122, batch 1000, loss:0.9778, acc:0.6809\n",
      "epoch 122, batch 1500, loss:0.9792, acc:0.6816\n",
      "epoch 122, batch 2000, loss:0.9813, acc:0.6818\n",
      "epoch 122, batch 2500, loss:0.9845, acc:0.6813\n",
      "epoch 122, batch 3000, loss:0.9846, acc:0.6812\n",
      "epoch 122, batch 3500, loss:0.9862, acc:0.6808\n",
      "epoch 122, save model at ./checkpoint/train\\ckpt-61\n",
      "epoch 122, loss:0.9861, acc:0.6809\n",
      "time in 1 epoch:347.1292815208435 secs\n",
      "\n",
      "epoch 123, batch 0, loss:0.9446, acc:0.6819\n",
      "epoch 123, batch 500, loss:0.9783, acc:0.6762\n",
      "epoch 123, batch 1000, loss:0.9777, acc:0.6810\n",
      "epoch 123, batch 1500, loss:0.9793, acc:0.6816\n",
      "epoch 123, batch 2000, loss:0.9813, acc:0.6817\n",
      "epoch 123, batch 2500, loss:0.9845, acc:0.6812\n",
      "epoch 123, batch 3000, loss:0.9846, acc:0.6811\n",
      "epoch 123, batch 3500, loss:0.9862, acc:0.6809\n",
      "epoch 123, loss:0.9861, acc:0.6809\n",
      "time in 1 epoch:346.2047550678253 secs\n",
      "\n",
      "epoch 124, batch 0, loss:0.9330, acc:0.6879\n",
      "epoch 124, batch 500, loss:0.9780, acc:0.6762\n",
      "epoch 124, batch 1000, loss:0.9770, acc:0.6811\n",
      "epoch 124, batch 1500, loss:0.9787, acc:0.6818\n",
      "epoch 124, batch 2000, loss:0.9809, acc:0.6819\n",
      "epoch 124, batch 2500, loss:0.9844, acc:0.6813\n",
      "epoch 124, batch 3000, loss:0.9844, acc:0.6813\n",
      "epoch 124, batch 3500, loss:0.9860, acc:0.6810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 124, save model at ./checkpoint/train\\ckpt-62\n",
      "epoch 124, loss:0.9859, acc:0.6810\n",
      "time in 1 epoch:354.08451795578003 secs\n",
      "\n",
      "epoch 125, batch 0, loss:0.9468, acc:0.6775\n",
      "epoch 125, batch 500, loss:0.9772, acc:0.6766\n",
      "epoch 125, batch 1000, loss:0.9766, acc:0.6814\n",
      "epoch 125, batch 1500, loss:0.9785, acc:0.6818\n",
      "epoch 125, batch 2000, loss:0.9805, acc:0.6819\n",
      "epoch 125, batch 2500, loss:0.9839, acc:0.6813\n",
      "epoch 125, batch 3000, loss:0.9839, acc:0.6813\n",
      "epoch 125, batch 3500, loss:0.9854, acc:0.6810\n",
      "epoch 125, loss:0.9854, acc:0.6811\n",
      "time in 1 epoch:345.7490017414093 secs\n",
      "\n",
      "epoch 126, batch 0, loss:0.9453, acc:0.6859\n",
      "epoch 126, batch 500, loss:0.9773, acc:0.6764\n",
      "epoch 126, batch 1000, loss:0.9768, acc:0.6812\n",
      "epoch 126, batch 1500, loss:0.9785, acc:0.6817\n",
      "epoch 126, batch 2000, loss:0.9805, acc:0.6819\n",
      "epoch 126, batch 2500, loss:0.9838, acc:0.6813\n",
      "epoch 126, batch 3000, loss:0.9837, acc:0.6814\n",
      "epoch 126, batch 3500, loss:0.9854, acc:0.6811\n",
      "epoch 126, save model at ./checkpoint/train\\ckpt-63\n",
      "epoch 126, loss:0.9853, acc:0.6811\n",
      "time in 1 epoch:345.2951612472534 secs\n",
      "\n",
      "epoch 127, batch 0, loss:0.9344, acc:0.6851\n",
      "epoch 127, batch 500, loss:0.9768, acc:0.6767\n",
      "epoch 127, batch 1000, loss:0.9764, acc:0.6814\n",
      "epoch 127, batch 1500, loss:0.9779, acc:0.6820\n",
      "epoch 127, batch 2000, loss:0.9800, acc:0.6821\n",
      "epoch 127, batch 2500, loss:0.9834, acc:0.6815\n",
      "epoch 127, batch 3000, loss:0.9833, acc:0.6815\n",
      "epoch 127, batch 3500, loss:0.9849, acc:0.6812\n",
      "epoch 127, loss:0.9849, acc:0.6812\n",
      "time in 1 epoch:346.09804034233093 secs\n",
      "\n",
      "epoch 128, batch 0, loss:0.9415, acc:0.6911\n",
      "epoch 128, batch 500, loss:0.9765, acc:0.6766\n",
      "epoch 128, batch 1000, loss:0.9760, acc:0.6816\n",
      "epoch 128, batch 1500, loss:0.9778, acc:0.6820\n",
      "epoch 128, batch 2000, loss:0.9799, acc:0.6821\n",
      "epoch 128, batch 2500, loss:0.9832, acc:0.6816\n",
      "epoch 128, batch 3000, loss:0.9831, acc:0.6816\n",
      "epoch 128, batch 3500, loss:0.9847, acc:0.6813\n",
      "epoch 128, save model at ./checkpoint/train\\ckpt-64\n",
      "epoch 128, loss:0.9846, acc:0.6814\n",
      "time in 1 epoch:348.3938982486725 secs\n",
      "\n",
      "epoch 129, batch 0, loss:0.9373, acc:0.6847\n",
      "epoch 129, batch 500, loss:0.9761, acc:0.6766\n",
      "epoch 129, batch 1000, loss:0.9756, acc:0.6816\n",
      "epoch 129, batch 1500, loss:0.9774, acc:0.6822\n",
      "epoch 129, batch 2000, loss:0.9795, acc:0.6823\n",
      "epoch 129, batch 2500, loss:0.9827, acc:0.6817\n",
      "epoch 129, batch 3000, loss:0.9827, acc:0.6817\n",
      "epoch 129, batch 3500, loss:0.9843, acc:0.6813\n",
      "epoch 129, loss:0.9843, acc:0.6814\n",
      "time in 1 epoch:346.0930542945862 secs\n",
      "\n",
      "epoch 130, batch 0, loss:0.9308, acc:0.6883\n",
      "epoch 130, batch 500, loss:0.9760, acc:0.6769\n",
      "epoch 130, batch 1000, loss:0.9754, acc:0.6816\n",
      "epoch 130, batch 1500, loss:0.9773, acc:0.6820\n",
      "epoch 130, batch 2000, loss:0.9793, acc:0.6823\n",
      "epoch 130, batch 2500, loss:0.9825, acc:0.6817\n",
      "epoch 130, batch 3000, loss:0.9825, acc:0.6817\n",
      "epoch 130, batch 3500, loss:0.9841, acc:0.6814\n",
      "epoch 130, save model at ./checkpoint/train\\ckpt-65\n",
      "epoch 130, loss:0.9841, acc:0.6815\n",
      "time in 1 epoch:346.31347703933716 secs\n",
      "\n",
      "epoch 131, batch 0, loss:0.9528, acc:0.6803\n",
      "epoch 131, batch 500, loss:0.9761, acc:0.6767\n",
      "epoch 131, batch 1000, loss:0.9754, acc:0.6816\n",
      "epoch 131, batch 1500, loss:0.9772, acc:0.6821\n",
      "epoch 131, batch 2000, loss:0.9792, acc:0.6823\n",
      "epoch 131, batch 2500, loss:0.9825, acc:0.6817\n",
      "epoch 131, batch 3000, loss:0.9825, acc:0.6817\n",
      "epoch 131, batch 3500, loss:0.9840, acc:0.6814\n",
      "epoch 131, loss:0.9840, acc:0.6815\n",
      "time in 1 epoch:346.7802152633667 secs\n",
      "\n",
      "epoch 132, batch 0, loss:0.9474, acc:0.6767\n",
      "epoch 132, batch 500, loss:0.9756, acc:0.6768\n",
      "epoch 132, batch 1000, loss:0.9753, acc:0.6817\n",
      "epoch 132, batch 1500, loss:0.9770, acc:0.6822\n",
      "epoch 132, batch 2000, loss:0.9791, acc:0.6823\n",
      "epoch 132, batch 2500, loss:0.9824, acc:0.6818\n",
      "epoch 132, batch 3000, loss:0.9824, acc:0.6818\n",
      "epoch 132, batch 3500, loss:0.9840, acc:0.6814\n",
      "epoch 132, save model at ./checkpoint/train\\ckpt-66\n",
      "epoch 132, loss:0.9839, acc:0.6815\n",
      "time in 1 epoch:346.8470368385315 secs\n",
      "\n",
      "epoch 133, batch 0, loss:0.9312, acc:0.6879\n",
      "epoch 133, batch 500, loss:0.9753, acc:0.6772\n",
      "epoch 133, batch 1000, loss:0.9745, acc:0.6820\n",
      "epoch 133, batch 1500, loss:0.9762, acc:0.6825\n",
      "epoch 133, batch 2000, loss:0.9783, acc:0.6826\n",
      "epoch 133, batch 2500, loss:0.9816, acc:0.6820\n",
      "epoch 133, batch 3000, loss:0.9817, acc:0.6820\n",
      "epoch 133, batch 3500, loss:0.9833, acc:0.6817\n",
      "epoch 133, loss:0.9833, acc:0.6818\n",
      "time in 1 epoch:345.59538555145264 secs\n",
      "\n",
      "epoch 134, batch 0, loss:0.9314, acc:0.6871\n",
      "epoch 134, batch 500, loss:0.9755, acc:0.6770\n",
      "epoch 134, batch 1000, loss:0.9748, acc:0.6819\n",
      "epoch 134, batch 1500, loss:0.9764, acc:0.6825\n",
      "epoch 134, batch 2000, loss:0.9784, acc:0.6826\n",
      "epoch 134, batch 2500, loss:0.9817, acc:0.6820\n",
      "epoch 134, batch 3000, loss:0.9817, acc:0.6820\n",
      "epoch 134, batch 3500, loss:0.9833, acc:0.6817\n",
      "epoch 134, save model at ./checkpoint/train\\ckpt-67\n",
      "epoch 134, loss:0.9833, acc:0.6817\n",
      "time in 1 epoch:345.3340845108032 secs\n",
      "\n",
      "epoch 135, batch 0, loss:0.9425, acc:0.6871\n",
      "epoch 135, batch 500, loss:0.9748, acc:0.6772\n",
      "epoch 135, batch 1000, loss:0.9745, acc:0.6820\n",
      "epoch 135, batch 1500, loss:0.9764, acc:0.6825\n",
      "epoch 135, batch 2000, loss:0.9783, acc:0.6826\n",
      "epoch 135, batch 2500, loss:0.9817, acc:0.6820\n",
      "epoch 135, batch 3000, loss:0.9817, acc:0.6819\n",
      "epoch 135, batch 3500, loss:0.9832, acc:0.6816\n",
      "epoch 135, loss:0.9832, acc:0.6817\n",
      "time in 1 epoch:345.1017060279846 secs\n",
      "\n",
      "epoch 136, batch 0, loss:0.9218, acc:0.6891\n",
      "epoch 136, batch 500, loss:0.9748, acc:0.6771\n",
      "epoch 136, batch 1000, loss:0.9742, acc:0.6821\n",
      "epoch 136, batch 1500, loss:0.9758, acc:0.6826\n",
      "epoch 136, batch 2000, loss:0.9779, acc:0.6827\n",
      "epoch 136, batch 2500, loss:0.9811, acc:0.6822\n",
      "epoch 136, batch 3000, loss:0.9811, acc:0.6821\n",
      "epoch 136, batch 3500, loss:0.9828, acc:0.6818\n",
      "epoch 136, save model at ./checkpoint/train\\ckpt-68\n",
      "epoch 136, loss:0.9827, acc:0.6819\n",
      "time in 1 epoch:344.61002135276794 secs\n",
      "\n",
      "epoch 137, batch 0, loss:0.9256, acc:0.6927\n",
      "epoch 137, batch 500, loss:0.9738, acc:0.6773\n",
      "epoch 137, batch 1000, loss:0.9736, acc:0.6820\n",
      "epoch 137, batch 1500, loss:0.9754, acc:0.6825\n",
      "epoch 137, batch 2000, loss:0.9774, acc:0.6828\n",
      "epoch 137, batch 2500, loss:0.9808, acc:0.6821\n",
      "epoch 137, batch 3000, loss:0.9807, acc:0.6821\n",
      "epoch 137, batch 3500, loss:0.9824, acc:0.6817\n",
      "epoch 137, loss:0.9823, acc:0.6818\n",
      "time in 1 epoch:345.19149804115295 secs\n",
      "\n",
      "epoch 138, batch 0, loss:0.9346, acc:0.6887\n",
      "epoch 138, batch 500, loss:0.9736, acc:0.6777\n",
      "epoch 138, batch 1000, loss:0.9733, acc:0.6825\n",
      "epoch 138, batch 1500, loss:0.9751, acc:0.6829\n",
      "epoch 138, batch 2000, loss:0.9772, acc:0.6831\n",
      "epoch 138, batch 2500, loss:0.9806, acc:0.6824\n",
      "epoch 138, batch 3000, loss:0.9806, acc:0.6824\n",
      "epoch 138, batch 3500, loss:0.9822, acc:0.6820\n",
      "epoch 138, save model at ./checkpoint/train\\ckpt-69\n",
      "epoch 138, loss:0.9822, acc:0.6821\n",
      "time in 1 epoch:345.35300183296204 secs\n",
      "\n",
      "epoch 139, batch 0, loss:0.9345, acc:0.6875\n",
      "epoch 139, batch 500, loss:0.9742, acc:0.6774\n",
      "epoch 139, batch 1000, loss:0.9735, acc:0.6822\n",
      "epoch 139, batch 1500, loss:0.9752, acc:0.6827\n",
      "epoch 139, batch 2000, loss:0.9771, acc:0.6829\n",
      "epoch 139, batch 2500, loss:0.9804, acc:0.6823\n",
      "epoch 139, batch 3000, loss:0.9804, acc:0.6823\n",
      "epoch 139, batch 3500, loss:0.9820, acc:0.6820\n",
      "epoch 139, loss:0.9819, acc:0.6820\n",
      "time in 1 epoch:344.9072265625 secs\n",
      "\n",
      "epoch 140, batch 0, loss:0.9372, acc:0.6815\n",
      "epoch 140, batch 500, loss:0.9739, acc:0.6772\n",
      "epoch 140, batch 1000, loss:0.9732, acc:0.6823\n",
      "epoch 140, batch 1500, loss:0.9749, acc:0.6827\n",
      "epoch 140, batch 2000, loss:0.9770, acc:0.6830\n",
      "epoch 140, batch 2500, loss:0.9803, acc:0.6824\n",
      "epoch 140, batch 3000, loss:0.9803, acc:0.6823\n",
      "epoch 140, batch 3500, loss:0.9819, acc:0.6821\n",
      "epoch 140, save model at ./checkpoint/train\\ckpt-70\n",
      "epoch 140, loss:0.9818, acc:0.6821\n",
      "time in 1 epoch:345.49864435195923 secs\n",
      "\n",
      "epoch 141, batch 0, loss:0.9350, acc:0.6883\n",
      "epoch 141, batch 500, loss:0.9738, acc:0.6771\n",
      "epoch 141, batch 1000, loss:0.9732, acc:0.6821\n",
      "epoch 141, batch 1500, loss:0.9748, acc:0.6826\n",
      "epoch 141, batch 2000, loss:0.9768, acc:0.6827\n",
      "epoch 141, batch 2500, loss:0.9802, acc:0.6822\n",
      "epoch 141, batch 3000, loss:0.9801, acc:0.6822\n",
      "epoch 141, batch 3500, loss:0.9818, acc:0.6820\n",
      "epoch 141, loss:0.9817, acc:0.6820\n",
      "time in 1 epoch:352.69962763786316 secs\n",
      "\n",
      "epoch 142, batch 0, loss:0.9431, acc:0.6863\n",
      "epoch 142, batch 500, loss:0.9729, acc:0.6778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 142, batch 1000, loss:0.9724, acc:0.6825\n",
      "epoch 142, batch 1500, loss:0.9742, acc:0.6830\n",
      "epoch 142, batch 2000, loss:0.9763, acc:0.6832\n",
      "epoch 142, batch 2500, loss:0.9797, acc:0.6825\n",
      "epoch 142, batch 3000, loss:0.9797, acc:0.6824\n",
      "epoch 142, batch 3500, loss:0.9814, acc:0.6821\n",
      "epoch 142, save model at ./checkpoint/train\\ckpt-71\n",
      "epoch 142, loss:0.9814, acc:0.6821\n",
      "time in 1 epoch:351.7465012073517 secs\n",
      "\n",
      "epoch 143, batch 0, loss:0.9242, acc:0.6911\n",
      "epoch 143, batch 500, loss:0.9731, acc:0.6777\n",
      "epoch 143, batch 1000, loss:0.9727, acc:0.6825\n",
      "epoch 143, batch 1500, loss:0.9745, acc:0.6830\n",
      "epoch 143, batch 2000, loss:0.9765, acc:0.6832\n",
      "epoch 143, batch 2500, loss:0.9797, acc:0.6826\n",
      "epoch 143, batch 3000, loss:0.9797, acc:0.6826\n",
      "epoch 143, batch 3500, loss:0.9812, acc:0.6823\n",
      "epoch 143, loss:0.9812, acc:0.6824\n",
      "time in 1 epoch:345.34006810188293 secs\n",
      "\n",
      "epoch 144, batch 0, loss:0.9270, acc:0.6919\n",
      "epoch 144, batch 500, loss:0.9728, acc:0.6777\n",
      "epoch 144, batch 1000, loss:0.9724, acc:0.6825\n",
      "epoch 144, batch 1500, loss:0.9741, acc:0.6829\n",
      "epoch 144, batch 2000, loss:0.9762, acc:0.6831\n",
      "epoch 144, batch 2500, loss:0.9796, acc:0.6825\n",
      "epoch 144, batch 3000, loss:0.9796, acc:0.6825\n",
      "epoch 144, batch 3500, loss:0.9811, acc:0.6822\n",
      "epoch 144, save model at ./checkpoint/train\\ckpt-72\n",
      "epoch 144, loss:0.9811, acc:0.6823\n",
      "time in 1 epoch:345.82676672935486 secs\n",
      "\n",
      "epoch 145, batch 0, loss:0.9186, acc:0.6943\n",
      "epoch 145, batch 500, loss:0.9723, acc:0.6778\n",
      "epoch 145, batch 1000, loss:0.9722, acc:0.6826\n",
      "epoch 145, batch 1500, loss:0.9737, acc:0.6831\n",
      "epoch 145, batch 2000, loss:0.9759, acc:0.6832\n",
      "epoch 145, batch 2500, loss:0.9792, acc:0.6826\n",
      "epoch 145, batch 3000, loss:0.9791, acc:0.6826\n",
      "epoch 145, batch 3500, loss:0.9807, acc:0.6823\n",
      "epoch 145, loss:0.9807, acc:0.6823\n",
      "time in 1 epoch:344.2550003528595 secs\n",
      "\n",
      "epoch 146, batch 0, loss:0.9308, acc:0.6851\n",
      "epoch 146, batch 500, loss:0.9722, acc:0.6782\n",
      "epoch 146, batch 1000, loss:0.9720, acc:0.6828\n",
      "epoch 146, batch 1500, loss:0.9736, acc:0.6833\n",
      "epoch 146, batch 2000, loss:0.9757, acc:0.6834\n",
      "epoch 146, batch 2500, loss:0.9788, acc:0.6828\n",
      "epoch 146, batch 3000, loss:0.9787, acc:0.6829\n",
      "epoch 146, batch 3500, loss:0.9805, acc:0.6825\n",
      "epoch 146, save model at ./checkpoint/train\\ckpt-73\n",
      "epoch 146, loss:0.9804, acc:0.6826\n",
      "time in 1 epoch:345.44176745414734 secs\n",
      "\n",
      "epoch 147, batch 0, loss:0.9373, acc:0.6847\n",
      "epoch 147, batch 500, loss:0.9725, acc:0.6781\n",
      "epoch 147, batch 1000, loss:0.9719, acc:0.6827\n",
      "epoch 147, batch 1500, loss:0.9735, acc:0.6831\n",
      "epoch 147, batch 2000, loss:0.9757, acc:0.6833\n",
      "epoch 147, batch 2500, loss:0.9790, acc:0.6827\n",
      "epoch 147, batch 3000, loss:0.9790, acc:0.6827\n",
      "epoch 147, batch 3500, loss:0.9806, acc:0.6823\n",
      "epoch 147, loss:0.9806, acc:0.6824\n",
      "time in 1 epoch:343.7652826309204 secs\n",
      "\n",
      "epoch 148, batch 0, loss:0.9303, acc:0.6867\n",
      "epoch 148, batch 500, loss:0.9723, acc:0.6779\n",
      "epoch 148, batch 1000, loss:0.9716, acc:0.6828\n",
      "epoch 148, batch 1500, loss:0.9733, acc:0.6833\n",
      "epoch 148, batch 2000, loss:0.9754, acc:0.6835\n",
      "epoch 148, batch 2500, loss:0.9786, acc:0.6829\n",
      "epoch 148, batch 3000, loss:0.9786, acc:0.6828\n",
      "epoch 148, batch 3500, loss:0.9801, acc:0.6826\n",
      "epoch 148, save model at ./checkpoint/train\\ckpt-74\n",
      "epoch 148, loss:0.9801, acc:0.6826\n",
      "time in 1 epoch:346.9138569831848 secs\n",
      "\n",
      "epoch 149, batch 0, loss:0.9328, acc:0.6903\n",
      "epoch 149, batch 500, loss:0.9715, acc:0.6779\n",
      "epoch 149, batch 1000, loss:0.9713, acc:0.6827\n",
      "epoch 149, batch 1500, loss:0.9732, acc:0.6832\n",
      "epoch 149, batch 2000, loss:0.9751, acc:0.6834\n",
      "epoch 149, batch 2500, loss:0.9784, acc:0.6828\n",
      "epoch 149, batch 3000, loss:0.9783, acc:0.6828\n",
      "epoch 149, batch 3500, loss:0.9800, acc:0.6825\n",
      "epoch 149, loss:0.9799, acc:0.6826\n",
      "time in 1 epoch:346.14491605758667 secs\n",
      "\n",
      "epoch 150, batch 0, loss:0.9244, acc:0.6907\n",
      "epoch 150, batch 500, loss:0.9717, acc:0.6779\n",
      "epoch 150, batch 1000, loss:0.9711, acc:0.6829\n",
      "epoch 150, batch 1500, loss:0.9729, acc:0.6834\n",
      "epoch 150, batch 2000, loss:0.9749, acc:0.6835\n",
      "epoch 150, batch 2500, loss:0.9781, acc:0.6830\n",
      "epoch 150, batch 3000, loss:0.9780, acc:0.6829\n",
      "epoch 150, batch 3500, loss:0.9797, acc:0.6826\n",
      "epoch 150, save model at ./checkpoint/train\\ckpt-75\n",
      "epoch 150, loss:0.9797, acc:0.6827\n",
      "time in 1 epoch:356.6007285118103 secs\n",
      "\n",
      "epoch 151, batch 0, loss:0.9345, acc:0.6859\n",
      "epoch 151, batch 500, loss:0.9716, acc:0.6777\n",
      "epoch 151, batch 1000, loss:0.9709, acc:0.6827\n",
      "epoch 151, batch 1500, loss:0.9726, acc:0.6835\n",
      "epoch 151, batch 2000, loss:0.9748, acc:0.6836\n",
      "epoch 151, batch 2500, loss:0.9779, acc:0.6831\n",
      "epoch 151, batch 3000, loss:0.9779, acc:0.6831\n",
      "epoch 151, batch 3500, loss:0.9796, acc:0.6827\n",
      "epoch 151, loss:0.9795, acc:0.6828\n",
      "time in 1 epoch:360.17399740219116 secs\n",
      "\n",
      "epoch 152, batch 0, loss:0.9216, acc:0.6899\n",
      "epoch 152, batch 500, loss:0.9712, acc:0.6781\n",
      "epoch 152, batch 1000, loss:0.9708, acc:0.6829\n",
      "epoch 152, batch 1500, loss:0.9726, acc:0.6834\n",
      "epoch 152, batch 2000, loss:0.9748, acc:0.6835\n",
      "epoch 152, batch 2500, loss:0.9779, acc:0.6830\n",
      "epoch 152, batch 3000, loss:0.9778, acc:0.6830\n",
      "epoch 152, batch 3500, loss:0.9794, acc:0.6827\n",
      "epoch 152, save model at ./checkpoint/train\\ckpt-76\n",
      "epoch 152, loss:0.9794, acc:0.6827\n",
      "time in 1 epoch:351.5305061340332 secs\n",
      "\n",
      "epoch 153, batch 0, loss:0.9295, acc:0.6839\n",
      "epoch 153, batch 500, loss:0.9708, acc:0.6784\n",
      "epoch 153, batch 1000, loss:0.9708, acc:0.6829\n",
      "epoch 153, batch 1500, loss:0.9724, acc:0.6834\n",
      "epoch 153, batch 2000, loss:0.9744, acc:0.6837\n",
      "epoch 153, batch 2500, loss:0.9775, acc:0.6831\n",
      "epoch 153, batch 3000, loss:0.9775, acc:0.6831\n",
      "epoch 153, batch 3500, loss:0.9791, acc:0.6828\n",
      "epoch 153, loss:0.9791, acc:0.6829\n",
      "time in 1 epoch:391.84670090675354 secs\n",
      "\n",
      "epoch 154, batch 0, loss:0.9369, acc:0.6831\n",
      "epoch 154, batch 500, loss:0.9710, acc:0.6783\n",
      "epoch 154, batch 1000, loss:0.9703, acc:0.6831\n",
      "epoch 154, batch 1500, loss:0.9722, acc:0.6836\n",
      "epoch 154, batch 2000, loss:0.9742, acc:0.6838\n",
      "epoch 154, batch 2500, loss:0.9774, acc:0.6832\n",
      "epoch 154, batch 3000, loss:0.9773, acc:0.6832\n",
      "epoch 154, batch 3500, loss:0.9789, acc:0.6829\n",
      "epoch 154, save model at ./checkpoint/train\\ckpt-77\n",
      "epoch 154, loss:0.9789, acc:0.6829\n",
      "time in 1 epoch:426.80415749549866 secs\n",
      "\n",
      "epoch 155, batch 0, loss:0.9347, acc:0.6871\n",
      "epoch 155, batch 500, loss:0.9710, acc:0.6781\n",
      "epoch 155, batch 1000, loss:0.9705, acc:0.6829\n",
      "epoch 155, batch 1500, loss:0.9718, acc:0.6835\n",
      "epoch 155, batch 2000, loss:0.9738, acc:0.6837\n",
      "epoch 155, batch 2500, loss:0.9771, acc:0.6831\n",
      "epoch 155, batch 3000, loss:0.9771, acc:0.6830\n",
      "epoch 155, batch 3500, loss:0.9787, acc:0.6827\n",
      "epoch 155, loss:0.9787, acc:0.6828\n",
      "time in 1 epoch:417.88499689102173 secs\n",
      "\n",
      "epoch 156, batch 0, loss:0.9306, acc:0.6891\n",
      "epoch 156, batch 500, loss:0.9703, acc:0.6783\n",
      "epoch 156, batch 1000, loss:0.9701, acc:0.6831\n",
      "epoch 156, batch 1500, loss:0.9717, acc:0.6836\n",
      "epoch 156, batch 2000, loss:0.9736, acc:0.6839\n",
      "epoch 156, batch 2500, loss:0.9770, acc:0.6832\n",
      "epoch 156, batch 3000, loss:0.9770, acc:0.6832\n",
      "epoch 156, batch 3500, loss:0.9786, acc:0.6829\n",
      "epoch 156, save model at ./checkpoint/train\\ckpt-78\n",
      "epoch 156, loss:0.9785, acc:0.6830\n",
      "time in 1 epoch:405.05928564071655 secs\n",
      "\n",
      "epoch 157, batch 0, loss:0.9238, acc:0.6927\n",
      "epoch 157, batch 500, loss:0.9703, acc:0.6783\n",
      "epoch 157, batch 1000, loss:0.9695, acc:0.6832\n",
      "epoch 157, batch 1500, loss:0.9712, acc:0.6838\n",
      "epoch 157, batch 2000, loss:0.9733, acc:0.6841\n",
      "epoch 157, batch 2500, loss:0.9767, acc:0.6835\n",
      "epoch 157, batch 3000, loss:0.9767, acc:0.6834\n",
      "epoch 157, batch 3500, loss:0.9783, acc:0.6831\n",
      "epoch 157, loss:0.9783, acc:0.6832\n",
      "time in 1 epoch:381.8216061592102 secs\n",
      "\n",
      "epoch 158, batch 0, loss:0.9436, acc:0.6835\n",
      "epoch 158, batch 500, loss:0.9697, acc:0.6786\n",
      "epoch 158, batch 1000, loss:0.9694, acc:0.6832\n",
      "epoch 158, batch 1500, loss:0.9712, acc:0.6837\n",
      "epoch 158, batch 2000, loss:0.9732, acc:0.6840\n",
      "epoch 158, batch 2500, loss:0.9765, acc:0.6834\n",
      "epoch 158, batch 3000, loss:0.9766, acc:0.6834\n",
      "epoch 158, batch 3500, loss:0.9782, acc:0.6831\n",
      "epoch 158, save model at ./checkpoint/train\\ckpt-79\n",
      "epoch 158, loss:0.9782, acc:0.6831\n",
      "time in 1 epoch:382.5043158531189 secs\n",
      "\n",
      "epoch 159, batch 0, loss:0.9265, acc:0.6919\n",
      "epoch 159, batch 500, loss:0.9697, acc:0.6784\n",
      "epoch 159, batch 1000, loss:0.9688, acc:0.6834\n",
      "epoch 159, batch 1500, loss:0.9707, acc:0.6839\n",
      "epoch 159, batch 2000, loss:0.9730, acc:0.6841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 159, batch 2500, loss:0.9763, acc:0.6834\n",
      "epoch 159, batch 3000, loss:0.9763, acc:0.6834\n",
      "epoch 159, batch 3500, loss:0.9779, acc:0.6831\n",
      "epoch 159, loss:0.9779, acc:0.6832\n",
      "time in 1 epoch:381.25474858283997 secs\n",
      "\n",
      "epoch 160, batch 0, loss:0.9292, acc:0.6899\n",
      "epoch 160, batch 500, loss:0.9697, acc:0.6786\n",
      "epoch 160, batch 1000, loss:0.9693, acc:0.6833\n",
      "epoch 160, batch 1500, loss:0.9709, acc:0.6839\n",
      "epoch 160, batch 2000, loss:0.9730, acc:0.6841\n",
      "epoch 160, batch 2500, loss:0.9763, acc:0.6835\n",
      "epoch 160, batch 3000, loss:0.9763, acc:0.6834\n",
      "epoch 160, batch 3500, loss:0.9779, acc:0.6831\n",
      "epoch 160, save model at ./checkpoint/train\\ckpt-80\n",
      "epoch 160, loss:0.9779, acc:0.6832\n",
      "time in 1 epoch:382.90543937683105 secs\n",
      "\n",
      "epoch 161, batch 0, loss:0.9434, acc:0.6827\n",
      "epoch 161, batch 500, loss:0.9695, acc:0.6786\n",
      "epoch 161, batch 1000, loss:0.9692, acc:0.6834\n",
      "epoch 161, batch 1500, loss:0.9710, acc:0.6838\n",
      "epoch 161, batch 2000, loss:0.9730, acc:0.6840\n",
      "epoch 161, batch 2500, loss:0.9761, acc:0.6834\n",
      "epoch 161, batch 3000, loss:0.9761, acc:0.6835\n",
      "epoch 161, batch 3500, loss:0.9777, acc:0.6832\n",
      "epoch 161, loss:0.9776, acc:0.6833\n",
      "time in 1 epoch:367.04128670692444 secs\n",
      "\n",
      "epoch 162, batch 0, loss:0.9401, acc:0.6795\n",
      "epoch 162, batch 500, loss:0.9697, acc:0.6787\n",
      "epoch 162, batch 1000, loss:0.9694, acc:0.6832\n",
      "epoch 162, batch 1500, loss:0.9710, acc:0.6839\n",
      "epoch 162, batch 2000, loss:0.9730, acc:0.6841\n",
      "epoch 162, batch 2500, loss:0.9763, acc:0.6835\n",
      "epoch 162, batch 3000, loss:0.9761, acc:0.6835\n",
      "epoch 162, batch 3500, loss:0.9777, acc:0.6832\n",
      "epoch 162, save model at ./checkpoint/train\\ckpt-81\n",
      "epoch 162, loss:0.9777, acc:0.6833\n",
      "time in 1 epoch:346.8031542301178 secs\n",
      "\n",
      "epoch 163, batch 0, loss:0.9208, acc:0.6919\n",
      "epoch 163, batch 500, loss:0.9695, acc:0.6788\n",
      "epoch 163, batch 1000, loss:0.9689, acc:0.6836\n",
      "epoch 163, batch 1500, loss:0.9704, acc:0.6841\n",
      "epoch 163, batch 2000, loss:0.9725, acc:0.6842\n",
      "epoch 163, batch 2500, loss:0.9758, acc:0.6836\n",
      "epoch 163, batch 3000, loss:0.9758, acc:0.6837\n",
      "epoch 163, batch 3500, loss:0.9774, acc:0.6833\n",
      "epoch 163, loss:0.9774, acc:0.6833\n",
      "time in 1 epoch:344.60104608535767 secs\n",
      "\n",
      "epoch 164, batch 0, loss:0.9312, acc:0.6899\n",
      "epoch 164, batch 500, loss:0.9685, acc:0.6791\n",
      "epoch 164, batch 1000, loss:0.9680, acc:0.6836\n",
      "epoch 164, batch 1500, loss:0.9699, acc:0.6841\n",
      "epoch 164, batch 2000, loss:0.9720, acc:0.6842\n",
      "epoch 164, batch 2500, loss:0.9755, acc:0.6836\n",
      "epoch 164, batch 3000, loss:0.9754, acc:0.6837\n",
      "epoch 164, batch 3500, loss:0.9770, acc:0.6834\n",
      "epoch 164, save model at ./checkpoint/train\\ckpt-82\n",
      "epoch 164, loss:0.9770, acc:0.6835\n",
      "time in 1 epoch:345.86267018318176 secs\n",
      "\n",
      "epoch 165, batch 0, loss:0.9321, acc:0.6899\n",
      "epoch 165, batch 500, loss:0.9690, acc:0.6790\n",
      "epoch 165, batch 1000, loss:0.9684, acc:0.6837\n",
      "epoch 165, batch 1500, loss:0.9702, acc:0.6841\n",
      "epoch 165, batch 2000, loss:0.9721, acc:0.6843\n",
      "epoch 165, batch 2500, loss:0.9753, acc:0.6837\n",
      "epoch 165, batch 3000, loss:0.9753, acc:0.6837\n",
      "epoch 165, batch 3500, loss:0.9769, acc:0.6833\n",
      "epoch 165, loss:0.9769, acc:0.6834\n",
      "time in 1 epoch:346.09408044815063 secs\n",
      "\n",
      "epoch 166, batch 0, loss:0.9233, acc:0.6867\n",
      "epoch 166, batch 500, loss:0.9680, acc:0.6790\n",
      "epoch 166, batch 1000, loss:0.9681, acc:0.6836\n",
      "epoch 166, batch 1500, loss:0.9697, acc:0.6842\n",
      "epoch 166, batch 2000, loss:0.9718, acc:0.6844\n",
      "epoch 166, batch 2500, loss:0.9751, acc:0.6838\n",
      "epoch 166, batch 3000, loss:0.9751, acc:0.6838\n",
      "epoch 166, batch 3500, loss:0.9767, acc:0.6835\n",
      "epoch 166, save model at ./checkpoint/train\\ckpt-83\n",
      "epoch 166, loss:0.9766, acc:0.6836\n",
      "time in 1 epoch:345.8815903663635 secs\n",
      "\n",
      "epoch 167, batch 0, loss:0.9308, acc:0.6831\n",
      "epoch 167, batch 500, loss:0.9683, acc:0.6791\n",
      "epoch 167, batch 1000, loss:0.9677, acc:0.6838\n",
      "epoch 167, batch 1500, loss:0.9695, acc:0.6843\n",
      "epoch 167, batch 2000, loss:0.9715, acc:0.6845\n",
      "epoch 167, batch 2500, loss:0.9746, acc:0.6839\n",
      "epoch 167, batch 3000, loss:0.9747, acc:0.6839\n",
      "epoch 167, batch 3500, loss:0.9763, acc:0.6836\n",
      "epoch 167, loss:0.9763, acc:0.6836\n",
      "time in 1 epoch:346.31944823265076 secs\n",
      "\n",
      "epoch 168, batch 0, loss:0.9214, acc:0.6919\n",
      "epoch 168, batch 500, loss:0.9680, acc:0.6791\n",
      "epoch 168, batch 1000, loss:0.9676, acc:0.6838\n",
      "epoch 168, batch 1500, loss:0.9693, acc:0.6843\n",
      "epoch 168, batch 2000, loss:0.9714, acc:0.6845\n",
      "epoch 168, batch 2500, loss:0.9746, acc:0.6839\n",
      "epoch 168, batch 3000, loss:0.9746, acc:0.6839\n",
      "epoch 168, batch 3500, loss:0.9762, acc:0.6835\n",
      "epoch 168, save model at ./checkpoint/train\\ckpt-84\n",
      "epoch 168, loss:0.9762, acc:0.6836\n",
      "time in 1 epoch:346.0411927700043 secs\n",
      "\n",
      "epoch 169, batch 0, loss:0.9240, acc:0.6911\n",
      "epoch 169, batch 500, loss:0.9680, acc:0.6793\n",
      "epoch 169, batch 1000, loss:0.9675, acc:0.6840\n",
      "epoch 169, batch 1500, loss:0.9693, acc:0.6844\n",
      "epoch 169, batch 2000, loss:0.9715, acc:0.6845\n",
      "epoch 169, batch 2500, loss:0.9746, acc:0.6840\n",
      "epoch 169, batch 3000, loss:0.9744, acc:0.6840\n",
      "epoch 169, batch 3500, loss:0.9760, acc:0.6838\n",
      "epoch 169, loss:0.9760, acc:0.6838\n",
      "time in 1 epoch:346.35734724998474 secs\n",
      "\n",
      "epoch 170, batch 0, loss:0.9284, acc:0.6903\n",
      "epoch 170, batch 500, loss:0.9677, acc:0.6791\n",
      "epoch 170, batch 1000, loss:0.9673, acc:0.6839\n",
      "epoch 170, batch 1500, loss:0.9690, acc:0.6843\n",
      "epoch 170, batch 2000, loss:0.9711, acc:0.6846\n",
      "epoch 170, batch 2500, loss:0.9744, acc:0.6839\n",
      "epoch 170, batch 3000, loss:0.9743, acc:0.6840\n",
      "epoch 170, batch 3500, loss:0.9759, acc:0.6837\n",
      "epoch 170, save model at ./checkpoint/train\\ckpt-85\n",
      "epoch 170, loss:0.9758, acc:0.6838\n",
      "time in 1 epoch:352.84948348999023 secs\n",
      "\n",
      "epoch 171, batch 0, loss:0.9334, acc:0.6847\n",
      "epoch 171, batch 500, loss:0.9673, acc:0.6795\n",
      "epoch 171, batch 1000, loss:0.9669, acc:0.6842\n",
      "epoch 171, batch 1500, loss:0.9686, acc:0.6847\n",
      "epoch 171, batch 2000, loss:0.9707, acc:0.6848\n",
      "epoch 171, batch 2500, loss:0.9740, acc:0.6841\n",
      "epoch 171, batch 3000, loss:0.9739, acc:0.6841\n",
      "epoch 171, batch 3500, loss:0.9755, acc:0.6838\n",
      "epoch 171, loss:0.9755, acc:0.6839\n",
      "time in 1 epoch:344.6100218296051 secs\n",
      "\n",
      "epoch 172, batch 0, loss:0.9205, acc:0.6891\n",
      "epoch 172, batch 500, loss:0.9672, acc:0.6791\n",
      "epoch 172, batch 1000, loss:0.9667, acc:0.6840\n",
      "epoch 172, batch 1500, loss:0.9685, acc:0.6845\n",
      "epoch 172, batch 2000, loss:0.9707, acc:0.6846\n",
      "epoch 172, batch 2500, loss:0.9740, acc:0.6840\n",
      "epoch 172, batch 3000, loss:0.9741, acc:0.6840\n",
      "epoch 172, batch 3500, loss:0.9756, acc:0.6836\n",
      "epoch 172, save model at ./checkpoint/train\\ckpt-86\n",
      "epoch 172, loss:0.9756, acc:0.6837\n",
      "time in 1 epoch:346.5657892227173 secs\n",
      "\n",
      "epoch 173, batch 0, loss:0.9337, acc:0.6851\n",
      "epoch 173, batch 500, loss:0.9672, acc:0.6794\n",
      "epoch 173, batch 1000, loss:0.9671, acc:0.6841\n",
      "epoch 173, batch 1500, loss:0.9689, acc:0.6845\n",
      "epoch 173, batch 2000, loss:0.9709, acc:0.6846\n",
      "epoch 173, batch 2500, loss:0.9742, acc:0.6840\n",
      "epoch 173, batch 3000, loss:0.9742, acc:0.6839\n",
      "epoch 173, batch 3500, loss:0.9757, acc:0.6837\n",
      "epoch 173, loss:0.9757, acc:0.6838\n",
      "time in 1 epoch:344.7436943054199 secs\n",
      "\n",
      "epoch 174, batch 0, loss:0.9201, acc:0.6887\n",
      "epoch 174, batch 500, loss:0.9675, acc:0.6791\n",
      "epoch 174, batch 1000, loss:0.9669, acc:0.6840\n",
      "epoch 174, batch 1500, loss:0.9684, acc:0.6845\n",
      "epoch 174, batch 2000, loss:0.9705, acc:0.6847\n",
      "epoch 174, batch 2500, loss:0.9737, acc:0.6841\n",
      "epoch 174, batch 3000, loss:0.9738, acc:0.6841\n",
      "epoch 174, batch 3500, loss:0.9754, acc:0.6838\n",
      "epoch 174, save model at ./checkpoint/train\\ckpt-87\n",
      "epoch 174, loss:0.9754, acc:0.6839\n",
      "time in 1 epoch:344.98898220062256 secs\n",
      "\n",
      "epoch 175, batch 0, loss:0.9184, acc:0.6947\n",
      "epoch 175, batch 500, loss:0.9669, acc:0.6793\n",
      "epoch 175, batch 1000, loss:0.9664, acc:0.6841\n",
      "epoch 175, batch 1500, loss:0.9682, acc:0.6845\n",
      "epoch 175, batch 2000, loss:0.9702, acc:0.6847\n",
      "epoch 175, batch 2500, loss:0.9735, acc:0.6841\n",
      "epoch 175, batch 3000, loss:0.9736, acc:0.6841\n",
      "epoch 175, batch 3500, loss:0.9752, acc:0.6838\n",
      "epoch 175, loss:0.9751, acc:0.6839\n",
      "time in 1 epoch:346.45607829093933 secs\n",
      "\n",
      "epoch 176, batch 0, loss:0.9243, acc:0.6811\n",
      "epoch 176, batch 500, loss:0.9669, acc:0.6795\n",
      "epoch 176, batch 1000, loss:0.9664, acc:0.6842\n",
      "epoch 176, batch 1500, loss:0.9681, acc:0.6847\n",
      "epoch 176, batch 2000, loss:0.9701, acc:0.6849\n",
      "epoch 176, batch 2500, loss:0.9735, acc:0.6843\n",
      "epoch 176, batch 3000, loss:0.9735, acc:0.6842\n",
      "epoch 176, batch 3500, loss:0.9751, acc:0.6839\n",
      "epoch 176, save model at ./checkpoint/train\\ckpt-88\n",
      "epoch 176, loss:0.9751, acc:0.6840\n",
      "time in 1 epoch:345.28820753097534 secs\n",
      "\n",
      "epoch 177, batch 0, loss:0.9364, acc:0.6855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 177, batch 500, loss:0.9668, acc:0.6796\n",
      "epoch 177, batch 1000, loss:0.9663, acc:0.6845\n",
      "epoch 177, batch 1500, loss:0.9679, acc:0.6849\n",
      "epoch 177, batch 2000, loss:0.9700, acc:0.6852\n",
      "epoch 177, batch 2500, loss:0.9732, acc:0.6845\n",
      "epoch 177, batch 3000, loss:0.9732, acc:0.6845\n",
      "epoch 177, batch 3500, loss:0.9748, acc:0.6841\n",
      "epoch 177, loss:0.9748, acc:0.6842\n",
      "time in 1 epoch:345.1526041030884 secs\n",
      "\n",
      "epoch 178, batch 0, loss:0.9306, acc:0.6919\n",
      "epoch 178, batch 500, loss:0.9664, acc:0.6797\n",
      "epoch 178, batch 1000, loss:0.9661, acc:0.6843\n",
      "epoch 178, batch 1500, loss:0.9678, acc:0.6847\n",
      "epoch 178, batch 2000, loss:0.9698, acc:0.6849\n",
      "epoch 178, batch 2500, loss:0.9731, acc:0.6843\n",
      "epoch 178, batch 3000, loss:0.9731, acc:0.6843\n",
      "epoch 178, batch 3500, loss:0.9748, acc:0.6839\n",
      "epoch 178, save model at ./checkpoint/train\\ckpt-89\n",
      "epoch 178, loss:0.9747, acc:0.6840\n",
      "time in 1 epoch:347.2708685398102 secs\n",
      "\n",
      "epoch 179, batch 0, loss:0.9274, acc:0.6931\n",
      "epoch 179, batch 500, loss:0.9658, acc:0.6798\n",
      "epoch 179, batch 1000, loss:0.9657, acc:0.6843\n",
      "epoch 179, batch 1500, loss:0.9675, acc:0.6848\n",
      "epoch 179, batch 2000, loss:0.9696, acc:0.6850\n",
      "epoch 179, batch 2500, loss:0.9727, acc:0.6844\n",
      "epoch 179, batch 3000, loss:0.9728, acc:0.6844\n",
      "epoch 179, batch 3500, loss:0.9744, acc:0.6840\n",
      "epoch 179, loss:0.9744, acc:0.6841\n",
      "time in 1 epoch:345.6492838859558 secs\n",
      "\n",
      "epoch 180, batch 0, loss:0.9353, acc:0.6851\n",
      "epoch 180, batch 500, loss:0.9661, acc:0.6794\n",
      "epoch 180, batch 1000, loss:0.9659, acc:0.6842\n",
      "epoch 180, batch 1500, loss:0.9675, acc:0.6847\n",
      "epoch 180, batch 2000, loss:0.9697, acc:0.6849\n",
      "epoch 180, batch 2500, loss:0.9729, acc:0.6843\n",
      "epoch 180, batch 3000, loss:0.9729, acc:0.6843\n",
      "epoch 180, batch 3500, loss:0.9745, acc:0.6840\n",
      "epoch 180, save model at ./checkpoint/train\\ckpt-90\n",
      "epoch 180, loss:0.9744, acc:0.6841\n",
      "time in 1 epoch:345.87359833717346 secs\n",
      "\n",
      "epoch 181, batch 0, loss:0.9194, acc:0.6867\n",
      "epoch 181, batch 500, loss:0.9660, acc:0.6797\n",
      "epoch 181, batch 1000, loss:0.9654, acc:0.6845\n",
      "epoch 181, batch 1500, loss:0.9672, acc:0.6849\n",
      "epoch 181, batch 2000, loss:0.9694, acc:0.6850\n",
      "epoch 181, batch 2500, loss:0.9726, acc:0.6844\n",
      "epoch 181, batch 3000, loss:0.9726, acc:0.6844\n",
      "epoch 181, batch 3500, loss:0.9742, acc:0.6841\n",
      "epoch 181, loss:0.9741, acc:0.6842\n",
      "time in 1 epoch:343.5997247695923 secs\n",
      "\n",
      "epoch 182, batch 0, loss:0.9115, acc:0.6915\n",
      "epoch 182, batch 500, loss:0.9660, acc:0.6793\n",
      "epoch 182, batch 1000, loss:0.9653, acc:0.6843\n",
      "epoch 182, batch 1500, loss:0.9671, acc:0.6849\n",
      "epoch 182, batch 2000, loss:0.9691, acc:0.6851\n",
      "epoch 182, batch 2500, loss:0.9724, acc:0.6845\n",
      "epoch 182, batch 3000, loss:0.9724, acc:0.6846\n",
      "epoch 182, batch 3500, loss:0.9740, acc:0.6842\n",
      "epoch 182, save model at ./checkpoint/train\\ckpt-91\n",
      "epoch 182, loss:0.9740, acc:0.6843\n",
      "time in 1 epoch:346.59570932388306 secs\n",
      "\n",
      "epoch 183, batch 0, loss:0.9210, acc:0.6899\n",
      "epoch 183, batch 500, loss:0.9654, acc:0.6801\n",
      "epoch 183, batch 1000, loss:0.9653, acc:0.6847\n",
      "epoch 183, batch 1500, loss:0.9669, acc:0.6851\n",
      "epoch 183, batch 2000, loss:0.9690, acc:0.6854\n",
      "epoch 183, batch 2500, loss:0.9723, acc:0.6848\n",
      "epoch 183, batch 3000, loss:0.9724, acc:0.6848\n",
      "epoch 183, batch 3500, loss:0.9739, acc:0.6844\n",
      "epoch 183, loss:0.9738, acc:0.6845\n",
      "time in 1 epoch:345.98235034942627 secs\n",
      "\n",
      "epoch 184, batch 0, loss:0.9179, acc:0.6895\n",
      "epoch 184, batch 500, loss:0.9650, acc:0.6799\n",
      "epoch 184, batch 1000, loss:0.9649, acc:0.6845\n",
      "epoch 184, batch 1500, loss:0.9668, acc:0.6851\n",
      "epoch 184, batch 2000, loss:0.9689, acc:0.6852\n",
      "epoch 184, batch 2500, loss:0.9721, acc:0.6846\n",
      "epoch 184, batch 3000, loss:0.9722, acc:0.6845\n",
      "epoch 184, batch 3500, loss:0.9737, acc:0.6843\n",
      "epoch 184, save model at ./checkpoint/train\\ckpt-92\n",
      "epoch 184, loss:0.9737, acc:0.6844\n",
      "time in 1 epoch:346.2741324901581 secs\n",
      "\n",
      "epoch 185, batch 0, loss:0.9266, acc:0.6887\n",
      "epoch 185, batch 500, loss:0.9654, acc:0.6797\n",
      "epoch 185, batch 1000, loss:0.9648, acc:0.6845\n",
      "epoch 185, batch 1500, loss:0.9666, acc:0.6850\n",
      "epoch 185, batch 2000, loss:0.9686, acc:0.6853\n",
      "epoch 185, batch 2500, loss:0.9720, acc:0.6847\n",
      "epoch 185, batch 3000, loss:0.9719, acc:0.6847\n",
      "epoch 185, batch 3500, loss:0.9735, acc:0.6844\n",
      "epoch 185, loss:0.9735, acc:0.6845\n",
      "time in 1 epoch:345.8816194534302 secs\n",
      "\n",
      "epoch 186, batch 0, loss:0.9217, acc:0.6863\n",
      "epoch 186, batch 500, loss:0.9654, acc:0.6796\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10000\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # 重置记录项\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    # inputs 葡萄牙语， targets英语\n",
    "    \n",
    "    for batch, (inputs, targets) in enumerate(zip(cn_batch,en_batch)):\n",
    "        # 训练\n",
    "        train_step(inputs, targets)\n",
    "        \n",
    "        if batch % 500 == 0:\n",
    "            print('epoch {}, batch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "            epoch+1, batch, train_loss.result(), train_accuracy.result()\n",
    "            ))\n",
    "            \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('epoch {}, save model at {}'.format(\n",
    "        epoch+1, ckpt_save_path\n",
    "        ))\n",
    "    \n",
    "    \n",
    "    print('epoch {}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "    epoch+1, train_loss.result(), train_accuracy.result()\n",
    "    ))\n",
    "    \n",
    "    print('time in 1 epoch:{} secs\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1315c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
